{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuroidss/silent_speech/blob/main/EMG_Silent_Speech_with_WaveNet%26DeepSpeech_via_BrainFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SD4RO2F-xOT"
      },
      "outputs": [],
      "source": [
        "gen_tpu = 1 << 0\n",
        "gen_gpu = 1 << 1\n",
        "gen_pytorch = 1 << 2\n",
        "gen_tf1 = 1 << 3\n",
        "gen_tf2 = 1 << 4\n",
        "gen_stylegan2 = 1 << 5\n",
        "gen_sg2_nagolinc_pt = 1 << 6\n",
        "gen_sg2_nvlabs_ada_pt = 1 << 7\n",
        "gen_sg2_tf1 = 1 << 8\n",
        "gen_sg2_tf2 = 1 << 9\n",
        "gen_sg2_rosasalberto_tf2 = 1 << 10\n",
        "gen_anime_tf2_npy = 1 << 11\n",
        "gen_tadne_tf2_npy = 1 << 12\n",
        "gen_anime_protraits_tf2_npy = 1 << 13\n",
        "gen_abctract_art_tf2_npy = 1 << 14\n",
        "gen_tf2_npy = 1 << 15\n",
        "gen_sg2_moono_tf2 = 1 << 16\n",
        "gen_anime_tf2 = 1 << 17\n",
        "gen_sg2_shawwn_tpu = 1 << 18\n",
        "gen_sg2_cyrilzakka_tpu = 1 << 19\n",
        "gen_sg2_nvlabs = 1 << 20\n",
        "gen_anime = 1 << 21\n",
        "gen_sg2_shawwn = 1 << 22\n",
        "gen_tadne = 1 << 23\n",
        "gen_sg2_nvlabs_ada = 1 << 24\n",
        "gen_anime_protraits = 1 << 25\n",
        "gen_abctract_art = 1 << 26\n",
        "gen_wavegan = 1 << 27\n",
        "gen_drums = 1 << 28\n",
        "gen_mp3 = 1 << 29\n",
        "gen_wav = 1 << 30\n",
        "gen_png = 1 << 31\n",
        "gen_jpeg = 1 << 32\n",
        "gen_heatmap = 1 << 33\n",
        "gen_thdne = 1 << 34\n",
        "gen_wg_stereo = 1 << 35\n",
        "gen_wg_st_swap = 1 << 36\n",
        "gen_webm = 1 << 37\n",
        "gen_mp4 = 1 << 38\n",
        "gen_mp4_pyav = 1 << 39\n",
        "gen_mp4_imageio = 1 << 40\n",
        "gen_mp4_moviepy = 1 << 41\n",
        "gen_mp4_h264_nvenc = 1 << 42\n",
        "gen_sg2_aydao_surgery_model_release = 1 << 43\n",
        "gen_game = 1 << 44\n",
        "gen_game_mode1 = 1 << 45\n",
        "gen_game_mode3 = 1 << 46\n",
        "gen_parallel = 1 << 47\n",
        "gen_silent_speech = 1 << 48\n",
        "gen_ss_dgaddy_pt = 1 << 49\n",
        "gen_ss_wm50_tm07_dm070 = 1 << 50\n",
        "gen_gpu_cuda = 1 << 51\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q08NMGPUAfmh"
      },
      "outputs": [],
      "source": [
        "#generate = gen_gpu | gen_pytorch | gen_stylegan2 | gen_sg2_nvlabs_ada_pt | gen_anime_protraits\n",
        "#generate = gen_gpu | gen_pytorch | gen_stylegan2 | gen_sg2_nvlabs_ada_pt | gen_abctract_art\n",
        "\n",
        "#generate = gen_gpu | gen_pytorch | gen_stylegan2 | gen_sg2_nvlabs_ada_pt | gen_abctract_art | gen_tf1 | gen_sg2_nvlabs_ada | gen_anime_protraits\n",
        "#generate = gen_gpu | gen_pytorch | gen_stylegan2 | gen_sg2_nvlabs_ada_pt | gen_abctract_art | gen_tf1 | gen_sg2_nvlabs_ada | gen_anime_protraits | gen_wavegan | gen_drums\n",
        "\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_nvlabs_ada | gen_anime_protraits\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_nvlabs_ada | gen_abctract_art\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_shawwn | gen_tadne\n",
        "#generate = gen_gpu | gen_pytorch | gen_sg2_nagolinc_pt | gen_tf1 | gen_stylegan2 | gen_sg2_shawwn | gen_tadne\n",
        "#generate = gen_gpu | gen_tf1 | gen_wavegan | gen_drums\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_nvlabs_ada | gen_anime_protraits | gen_wavegan | gen_drums\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_nvlabs_ada | gen_abctract_art | gen_wavegan | gen_drums\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_shawwn | gen_tadne | gen_wavegan | gen_drums\n",
        "#generate = gen_gpu | gen_pytorch | gen_sg2_nagolinc_pt | gen_tf1 | gen_stylegan2 | gen_sg2_shawwn | gen_tadne | gen_wavegan | gen_drums\n",
        "\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_nvlabs | gen_anime_protraits | gen_tf2_npy\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_nvlabs_ada | gen_anime_protraits | gen_tf2_npy\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_nvlabs_ada | gen_abctract_art | gen_tf2_npy\n",
        "\n",
        "#generate = gen_gpu | gen_tf2 | gen_stylegan2 | gen_sg2_rosasalberto_tf2 | gen_anime_protraits_tf2_npy\n",
        "#generate = gen_gpu | gen_tf2 | gen_stylegan2 | gen_sg2_rosasalberto_tf2 | gen_abctract_art_tf2_npy\n",
        "#generate = gen_tpu | gen_tf2 | gen_stylegan2 | gen_sg2_rosasalberto_tf2 | gen_anime_protraits_tf2_npy\n",
        "#generate = gen_tpu | gen_tf2 | gen_stylegan2 | gen_sg2_rosasalberto_tf2 | gen_abctract_art_tf2_npy\n",
        "\n",
        "#generate = gen_tpu | gen_tf1 | gen_wavegan | gen_drums\n",
        "\n",
        "#generate = gen_tf2 | gen_stylegan2 | gen_sg2_rosasalberto_tf2 | gen_anime_protraits_tf2_npy\n",
        "#generate = gen_tf2 | gen_stylegan2 | gen_sg2_rosasalberto_tf2 | gen_abctract_art_tf2_npy\n",
        "#generate = gen_tf1 | gen_wavegan | gen_drums\n",
        "\n",
        "#generate = gen_tpu | gen_tf1 | gen_stylegan2 | gen_sg2_aydao_surgery_model_release | gen_thdne\n",
        "#generate = gen_tpu | gen_tf1 | gen_stylegan2 | gen_sg2_shawwn_tpu | gen_thdne\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_shawwn | gen_thdne\n",
        "#generate = gen_gpu | gen_pytorch | gen_stylegan2 | gen_sg2_nvlabs_ada_pt | gen_thdne\n",
        "\n",
        "#generate = gen_gpu | gen_tf1 | gen_wavegan | gen_wg_stereo | gen_drums\n",
        "#generate = gen_tf1 | gen_wavegan | gen_wg_stereo | gen_drums\n",
        "#generate = gen_tf1 | gen_wavegan | gen_wg_stereo | gen_wg_st_swap | gen_drums\n",
        "\n",
        "generate = gen_gpu | gen_pytorch | gen_tf2 | gen_silent_speech | gen_ss_dgaddy_pt | gen_ss_wm50_tm07_dm070\n",
        "\n",
        "generate = generate | gen_wg_stereo | gen_wg_st_swap\n",
        "\n",
        "#generate = generate | gen_png | gen_wav\n",
        "generate = generate | gen_jpeg | gen_mp3\n",
        "#generate = generate | gen_mp4\n",
        "#generate = generate | gen_webm\n",
        "\n",
        "#generate = generate | gen_mp4_pyav\n",
        "#generate = generate | gen_mp4_imageio\n",
        "#generate = generate | gen_mp4_moviepy\n",
        "#generate = generate | gen_mp4_h264_nvenc\n",
        "\n",
        "#generate = generate | gen_game\n",
        "#generate = generate | gen_game_mode1\n",
        "#generate = generate | gen_game_mode3\n",
        "\n",
        "generate = generate | gen_parallel\n",
        "\n",
        "#generate = generate | gen_gpu_cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwpQzGQCZVrM"
      },
      "outputs": [],
      "source": [
        "device_ad7771 = 1 << 0\n",
        "device_ads131m08 = 1 << 1\n",
        "\n",
        "device = device_ad7771\n",
        "#device = device_ads131m08\n",
        "\n",
        "if device&device_ad7771:\n",
        "  sfreq=512\n",
        "  vref = 2.50 #2.5V voltage ref +/- 250nV\n",
        "  gain = 8\n",
        "  data_channels = 32\n",
        "if device&device_ads131m08:\n",
        "  sfreq=250\n",
        "  #sfreq=83.3333333333\n",
        "  #sfreq=83\n",
        "  vref = 1.25 #2.5V voltage ref +/- 250nV\n",
        "  gain = 32\n",
        "#  data_channels = 32\n",
        "  data_channels = 128\n",
        "\n",
        "stepSize = 1/pow(2,24)\n",
        "\n",
        "vscale = (vref/gain)*stepSize #volts per step.\n",
        "uVperStep = 1000000 * ((vref/gain)*stepSize) #uV per step.\n",
        "scalar = 1/(1000000 / ((vref/gain)*stepSize)) #steps per uV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mkq7jIXnJm1W"
      },
      "outputs": [],
      "source": [
        "##biosemi16-2:\n",
        "ch_names_wg = ['FP1','F3','T7','C3','P3','Pz','O1','O2','P4','C4','T8','F4','FP2','Fz']\n",
        "ch_locations_wg=[0,3,6,7,11,12,14,16,18,22,23,26,29,30]\n",
        "\n",
        "#biosemi32_l14\n",
        "ch_names_wg_l = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','PO3','O1']\n",
        "ch_locations_wg_l=[0,1,2,3,4,5,6,7,8,9,10,11,13,14]\n",
        "#biosemi32_r14\n",
        "ch_names_wg_r_ = ['O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','FP2']\n",
        "ch_locations_wg_r_=[15,16,17,18,19,20,21,22,23,24,25,26,27,28,29]\n",
        "#biosemi32_r14_\n",
        "ch_names_wg_r = ['FP2','AF4','F8','F4','FC2','FC6','T8','C4','CP2','CP6','P8','P4','PO4','O2']\n",
        "ch_locations_wg_r=[29,28,27,26,25,24,23,22,21,20,19,18,17,16,15]\n",
        "\n",
        "#biosemi128_45\n",
        "#ch_names_sg2 = ['A1','A2','A3','A4','A5','A6','A7','A8','A9','A10','A11','A12','A13','A14','A15','A16','A17','A18','A19','A20','A21','A22','A23','A24','A25','A26','A27','A28','A29','A30','A31','A32',\n",
        "#                'B1','B2','B3','B4','B5','B6','B7','B8','B9','B10','B11','B12','B13']\n",
        "#ch_locations_sg2=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,\n",
        "#                  32,33,34,35,36,37,38,39,40,41,42,43,44,45]\n",
        "#biosemi128_32\n",
        "#ch_names_sg2 = ['A1','A2','A3','A4','A5','A6','A7','A8','A9','A10','A11','A12','A13','A14','A15','A16','A17','A18','A19','A20','A21','A22','A23','A24','A25','A26','A27','A28','A29','A30','A31','A32']\n",
        "#ch_locations_sg2=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]\n",
        "#biosemi128\n",
        "#ch_names_sg2 = ['A1','A2','A3','A4','A5','A6','A7','A8','A9','A10','A11','A12','A13','A14','A15','A16','A17','A18','A19','A20','A21','A22','A23','A24','A25','A26','A27','A28','A29','A30','A31','A32',\n",
        "#                'B1','B2','B3','B4','B5','B6','B7','B8','B9','B10','B11','B12','B13','B14','B15','B16','B17','B18','B19','B20','B21','B22','B23','B24','B25','B26','B27','B28','B29','B30','B31','B32',\n",
        "#                'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14','C15','C16','C17','C18','C19','C20','C21','C22','C23','C24','C25','C26','C27','C28','C29','C30','C31','C32',\n",
        "#                'D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15','D16','D17','D18','D19','D20','D21','D22','D23','D24','D25','D26','D27','D28','D29','D30','D31','D32']\n",
        "#ch_locations_sg2=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,\n",
        "#                  32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,\n",
        "#                  64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,\n",
        "#                  96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127]\n",
        "#biosemi32-1\n",
        "#ch_names_sg2 = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','FP2','Cz']\n",
        "#ch_locations_sg2=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,26,27,28,29,30,31]\n",
        "#silent_speech_8\n",
        "ch_names_sg2 = ['1','2','3','4','5','6','7','8']\n",
        "ch_locations_sg2=[0,1,2,3,4,5,6,7]\n",
        "#biosemi32\n",
        "#ch_names_sg2 = ['FP1','AF3','F7','F3','FC1','FC5','T7','C3','CP1','CP5','P7','P3','Pz','PO3','O1','Oz','O2','PO4','P4','P8','CP6','CP2','C4','T8','FC6','FC2','F4','F8','AF4','FP2','Fz','Cz']\n",
        "#ch_locations_sg2=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]\n",
        "##Bernard's 19ch:\n",
        "#ch_names = [\"FP2\",\"FP1\",\"O2\",\"T6\",\"T4\",\"F8\",\"F4\",\"C4\",\"P4\",\"F3\",\"C3\",\"P3\",\"O1\",\"T5\",\"T3\",\"F7\",\"FZ\",\"PZ\"]#,\"other\"]\n",
        "#ch_locations=[4,24,0,1,2,3,5,6,7,25,26,27,28,29,30,31,16,12]#,8]\n",
        "##Bernard's 2ch:\n",
        "#ch_names = [\"FP2\",\"FP1\"]#,\"other\"]\n",
        "#ch_locations=[4,24]#,8]\n",
        "\n",
        "bands = [[8.,12.]]\n",
        "#bands = [[4.,7.],[8.,12.]]\n",
        "#bands = [[8.,12.],[8.,12.],[8.,12.]]\n",
        "methods = ['coh']\n",
        "#methods = ['plv']\n",
        "#methods = ['ciplv']\n",
        "#methods = ['ppc']\n",
        "#methods = ['pli']\n",
        "#methods = ['wpli']\n",
        "#methods = ['coh', 'plv', 'ciplv', 'ppc', 'pli', 'wpli']\n",
        "\n",
        "vol=1\n",
        "#vol=6\n",
        "#vol=0.1\n",
        "\n",
        "duration=5*1/8\n",
        "overlap=0\n",
        "#overlap=duration-0.1\n",
        "if generate&gen_game:\n",
        "  xsize=128\n",
        "  ysize=128\n",
        "else:\n",
        "#xsize=256\n",
        "#ysize=256\n",
        "#  xsize=128\n",
        "#  ysize=128\n",
        "  xsize=512\n",
        "  ysize=512\n",
        "#xsize=512/2\n",
        "#ysize=512/2\n",
        "hz=44100\n",
        "#fps=hz/(32768)\n",
        "\n",
        "#if generate_stylegan2:\n",
        "fps_sg2=1\n",
        "#if generate_wavegan:\n",
        "#fps_wg=((hz/(32768*2))/1)*0.25\n",
        "#fps_wg=((hz/(32768*2))/1)*0.5\n",
        "fps_wg=((hz/(32768*2))/1)*1\n",
        "#fps_wg=((hz/(32768*2))/1)*2\n",
        "#fps_wg=((hz/(32768*2))/1)*3\n",
        "#fps_sg2=fps_wg\n",
        "fps_sg2=fps_wg*1\n",
        "#fps_sg2=2\n",
        "#fps_sg2=6\n",
        "#fps_sg2=1/5\n",
        "#fps_sg2=fps_wg/3\n",
        "#fps_sg2=fps_wg*4\n",
        "fps_hm=fps_wg\n",
        "\n",
        "fps2_sg2=((fps_sg2*24/8)/3)*1\n",
        "#fps2_sg2=((fps_sg2*24/8)/3)*2\n",
        "\n",
        "#if 1/fps_wg-0.2>duration:\n",
        "#  duration=1/fps_wg-0.2\n",
        "#  overlap=duration-0.1\n",
        "\n",
        "if 2*1/fps_wg>duration:\n",
        "  duration=2*1/fps_wg\n",
        "#  overlap=0\n",
        "  overlap=(duration/2)-(duration/2)/(fps2_sg2/fps_sg2)\n",
        "\n",
        "#fps2_sg2=1\n",
        "#fps2_sg2=1\n",
        "#duration=2*1/fps_wg\n",
        "#overlap=duration-(fps_wg/fps_sg2)\n",
        "\n",
        "if generate&gen_wavegan:\n",
        "  dim_wg = 100\n",
        "if generate&gen_stylegan2:\n",
        "  dim_sg2 = 512\n",
        "if generate&gen_sg2_shawwn:\n",
        "  dim_sg2 = 1024\n",
        "if generate&gen_sg2_shawwn_tpu:\n",
        "  dim_sg2 = 1024\n",
        "if generate&gen_sg2_aydao_surgery_model_release:\n",
        "  dim_sg2 = 1024\n",
        "\n",
        "\n",
        "debug=False\n",
        "#debug=True\n",
        "\n",
        "#mp4_codec = 'h264_cuvid'\n",
        "mp4_codec = 'libx264'\n",
        "\n",
        "device=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exdhoEe151Ja"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WErNtyaMBkg",
        "outputId": "92fab256-1b74-45b6-bcea-1c2b6a80eb04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm running on Colab\n",
            "Tensorflow version: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "if generate&gen_tf1:\n",
        "  import os\n",
        "  if 'COLAB_GPU' in os.environ:\n",
        "    print(\"I'm running on Colab\")\n",
        "    %tensorflow_version 1.x\n",
        "  else:\n",
        "    !pip install testresources\n",
        "    !pip install tensorflow==1.15\n",
        "if generate&gen_tf2:\n",
        "  import os\n",
        "  if 'COLAB_GPU' in os.environ:\n",
        "    print(\"I'm running on Colab\")\n",
        "    %tensorflow_version 2.x\n",
        "  else:\n",
        "    !pip install tensorflow==2.6\n",
        "import tensorflow as tf\n",
        "print('Tensorflow version: {}'.format(tf.__version__) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLsvM-V7Oh9S",
        "outputId": "60d056d3-b505-41f2-9131-5b03067cd8aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of accelerators:  1\n"
          ]
        }
      ],
      "source": [
        "if generate&gen_tf1:\n",
        " if generate&gen_gpu:\n",
        "  from tensorflow.python.client import device_lib\n",
        "  print(device_lib.list_local_devices())\n",
        "\n",
        " if generate&gen_tpu:\n",
        "  import os\n",
        "  import tensorflow.compat.v1 as tf\n",
        "  tf.disable_eager_execution()\n",
        "  import pprint\n",
        "  assert 'COLAB_TPU_ADDR' in os.environ, 'Did you forget to switch to TPU?'\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(tpu_address) as sess:\n",
        "    devices = sess.list_devices()\n",
        "  pprint.pprint(devices)\n",
        "  device_is_tpu = [True if 'TPU' in str(x) else False for x in devices]\n",
        "  assert True in device_is_tpu, 'Did you forget to switch to TPU?'\n",
        "\n",
        "if generate&gen_tf2:\n",
        " try: # detect TPUs\n",
        "  tpu = None\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.TPUStrategy(tpu)\n",
        " except ValueError: # detect GPUs\n",
        "  strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n",
        "  #strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n",
        "\n",
        " print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q265NLt3Unk",
        "outputId": "5d7cbf7b-2116-4e5e-d700-d5385bf7d6a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 1715049651535509968\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11320098816\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 8500986183119431772\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "xla_global_id: 416903419\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "  from tensorflow.python.client import device_lib\n",
        "  print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FasUDeJx6L9R"
      },
      "outputs": [],
      "source": [
        "if generate&gen_sg2_shawwn_tpu:\n",
        "  %env TPU_NAME={tpu_address}\n",
        "if generate&gen_sg2_aydao_surgery_model_release:\n",
        "  %env TPU_NAME={tpu_address}\n",
        "  %env DEBUG=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nVIVsZOe_B4"
      },
      "outputs": [],
      "source": [
        "if generate&gen_sg2_aydao_surgery_model_release:\n",
        "     \n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  import os\n",
        "  if os.path.isdir(\"/content/drive/MyDrive/colab-sg2-aydao\"):\n",
        "    %cd \"/content/drive/MyDrive/colab-sg2-aydao/stylegan2-surgery\"\n",
        "  else:\n",
        "    #install script\n",
        "    %cd \"/content/drive/MyDrive/\"\n",
        "    !mkdir colab-sg2-aydao\n",
        "    %cd colab-sg2-aydao\n",
        "    !git clone --branch model-release https://github.com/aydao/stylegan2-surgery.git\n",
        "#    !git clone --branch tpu https://github.com/shawwn/stylegan2.git\n",
        "#    !git clone https://github.com/dvschultz/stylegan2-ada\n",
        "    %cd stylegan2-surgery\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "  !gcloud auth login\n",
        "\n",
        "#  project_id = 'encoded-phalanx-326615'\n",
        "  project_id = 'local-abbey-335821'\n",
        "  \n",
        "  !gcloud config set project {project_id}\n",
        "\n",
        "#  GCP_PROJECT_ID = 'encoded-phalanx-326615'\n",
        "  GCP_PROJECT_ID = 'local-abbey-335821'\n",
        "  PROJECT_NUMBER = '0'\n",
        "\n",
        "  !gcloud services --project $GCP_PROJECT_ID enable ml.googleapis.com cloudbuild.googleapis.com\n",
        "\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaHracqcMbGG",
        "outputId": "e8c28341-23a8-4b2c-cdd9-31335463e759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 15 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (4.2.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1\n",
            "/content\n",
            "Cloning into '/content/silent_speech-dgaddy-pytorch'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 43 (delta 6), reused 4 (delta 4), pack-reused 32\u001b[K\n",
            "Unpacking objects: 100% (43/43), done.\n",
            "/content/silent_speech-dgaddy-pytorch\n",
            "Cloning into 'nv_wavenet'...\n",
            "remote: Enumerating objects: 241, done.\u001b[K\n",
            "remote: Total 241 (delta 0), reused 0 (delta 0), pack-reused 241\u001b[K\n",
            "Receiving objects: 100% (241/241), 6.68 MiB | 16.25 MiB/s, done.\n",
            "Resolving deltas: 100% (151/151), done.\n",
            "Cloning into 'voice-conversion'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 69 (delta 28), reused 54 (delta 20), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (69/69), done.\n",
            "'voice-conversion/nv_wavenet/LICENSE.TXT' -> 'nv_wavenet/LICENSE.TXT'\n",
            "'voice-conversion/nv_wavenet/README.md' -> 'nv_wavenet/README.md'\n",
            "'voice-conversion/nv_wavenet/math_test.cu' -> 'nv_wavenet/math_test.cu'\n",
            "'voice-conversion/nv_wavenet/matrix.cpp' -> 'nv_wavenet/matrix.cpp'\n",
            "'voice-conversion/nv_wavenet/matrix.h' -> 'nv_wavenet/matrix.h'\n",
            "'voice-conversion/nv_wavenet/matrix_math.cuh' -> 'nv_wavenet/matrix_math.cuh'\n",
            "'voice-conversion/nv_wavenet/nv_wavenet.cuh' -> 'nv_wavenet/nv_wavenet.cuh'\n",
            "'voice-conversion/nv_wavenet/nv_wavenet_conversions.cuh' -> 'nv_wavenet/nv_wavenet_conversions.cuh'\n",
            "'voice-conversion/nv_wavenet/nv_wavenet_dualblock.cuh' -> 'nv_wavenet/nv_wavenet_dualblock.cuh'\n",
            "'voice-conversion/nv_wavenet/nv_wavenet_perf.cu' -> 'nv_wavenet/nv_wavenet_perf.cu'\n",
            "'voice-conversion/nv_wavenet/nv_wavenet_persistent.cuh' -> 'nv_wavenet/nv_wavenet_persistent.cuh'\n",
            "'voice-conversion/nv_wavenet/nv_wavenet_reference.cpp' -> 'nv_wavenet/nv_wavenet_reference.cpp'\n",
            "'voice-conversion/nv_wavenet/nv_wavenet_reference.h' -> 'nv_wavenet/nv_wavenet_reference.h'\n",
            "'voice-conversion/nv_wavenet/nv_wavenet_singleblock.cuh' -> 'nv_wavenet/nv_wavenet_singleblock.cuh'\n",
            "'voice-conversion/nv_wavenet/nv_wavenet_test.cu' -> 'nv_wavenet/nv_wavenet_test.cu'\n",
            "'voice-conversion/nv_wavenet/nv_wavenet_util.cuh' -> 'nv_wavenet/nv_wavenet_util.cuh'\n",
            "'voice-conversion/nv_wavenet/softmax.cuh' -> 'nv_wavenet/softmax.cuh'\n",
            "/content/silent_speech-dgaddy-pytorch/nv_wavenet/pytorch\n",
            "nvcc -arch=sm_37 -std=c++11  --use_fast_math -lineinfo -maxrregcount 128 -I .. wavenet_infer.cu ../matrix.cpp -lz -Xcompiler -fPIC -shared -o libwavenet_infer.so\n",
            "\u001b[01m\u001b[K../nv_wavenet_conversions.cuh:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool isDevicePtr(const void*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../nv_wavenet_conversions.cuh:41:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KcudaPointerAttributes::memoryType\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     return (result == cudaSuccess) && (attributes\u001b[01;35m\u001b[K.memoryTyp\u001b[m\u001b[Ke == cudaMemoryTypeDevice);\n",
            "                                                  \u001b[01;35m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/builtin_types.h:59:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_runtime.h:58\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/7/include/stddef.h:220\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda-10.1/targets/x86_64-linux/include/driver_types.h:1302:43:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "     __CUDA_DEPRECATED enum cudaMemoryType \u001b[01;36m\u001b[KmemoryType\u001b[m\u001b[K;\n",
            "                                           \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../nv_wavenet_conversions.cuh:41:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KcudaPointerAttributes::memoryType\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     return (result == cudaSuccess) && (attributes\u001b[01;35m\u001b[K.memoryTyp\u001b[m\u001b[Ke == cudaMemoryTypeDevice);\n",
            "                                                  \u001b[01;35m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/builtin_types.h:59:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_runtime.h:58\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/7/include/stddef.h:220\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda-10.1/targets/x86_64-linux/include/driver_types.h:1302:43:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "     __CUDA_DEPRECATED enum cudaMemoryType \u001b[01;36m\u001b[KmemoryType\u001b[m\u001b[K;\n",
            "                                           \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../nv_wavenet_conversions.cuh:41:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KcudaPointerAttributes::memoryType\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     return (result == cudaSuccess) && (attributes\u001b[01;35m\u001b[K.memoryTyp\u001b[m\u001b[Ke == cudaMemoryTypeDevice);\n",
            "                                                  \u001b[01;35m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/builtin_types.h:59:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/crt/host_runtime.h:58\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/7/include/stddef.h:220\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda-10.1/targets/x86_64-linux/include/driver_types.h:1302:43:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "     __CUDA_DEPRECATED enum cudaMemoryType \u001b[01;36m\u001b[KmemoryType\u001b[m\u001b[K;\n",
            "                                           \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating nv_wavenet_ext.egg-info\n",
            "writing nv_wavenet_ext.egg-info/PKG-INFO\n",
            "writing dependency_links to nv_wavenet_ext.egg-info/dependency_links.txt\n",
            "writing top-level names to nv_wavenet_ext.egg-info/top_level.txt\n",
            "writing manifest file 'nv_wavenet_ext.egg-info/SOURCES.txt'\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/cpp_extension.py:352: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "writing manifest file 'nv_wavenet_ext.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "building 'nv_wavenet_ext' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/content/silent_speech-dgaddy-pytorch/nv_wavenet/pytorch -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c wavenet_infer_wrapper.cpp -o build/temp.linux-x86_64-3.7/wavenet_infer_wrapper.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=nv_wavenet_ext -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:149:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensorApply.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensor.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THCTensor.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THC.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            " #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            " \n",
            "\u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint infer(at::Tensor, int, int, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, std::vector<at::Tensor>&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:45:45:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     int* samples = samples_tensor.data<int>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensorApply.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensor.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THCTensor.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THC.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:47:59:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     float* embedding_prev = embed_prev_tensor.data<float>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensorApply.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensor.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THCTensor.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THC.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:48:59:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     float* embedding_curr = embed_curr_tensor.data<float>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensorApply.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensor.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THCTensor.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THC.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:49:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     float* conv_out = conv_out_tensor.data<float>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensorApply.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensor.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THCTensor.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THC.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:50:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     float* conv_end = conv_end_tensor.data<float>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensorApply.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensor.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THCTensor.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THC.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:51:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     float* cond_input = cond_input_tensor.data<float>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensorApply.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensor.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THCTensor.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THC.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:62:53:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  in_layer_weights_prev[i] = layers[idx].data<float>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensorApply.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensor.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THCTensor.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THC.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:63:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  in_layer_weights_curr[i] = layers[idx+1].data<float>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensorApply.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensor.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THCTensor.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THC.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:64:49:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  in_layer_biases[i] = layers[idx+2].data<float>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensorApply.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensor.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THCTensor.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THC.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:65:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  res_layer_weights[i] = layers[idx+3].data<float>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensorApply.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensor.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THCTensor.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THC.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:66:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  res_layer_biases[i] = layers[idx+4].data<float>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensorApply.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensor.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THCTensor.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THC.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:67:52:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  skip_layer_weights[i] = layers[idx+5].data<float>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensorApply.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensor.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THCTensor.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THC.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:68:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  skip_layer_biases[i] = layers[idx+6].data<float>(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensorApply.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/TH/THTensor.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THCTensor.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/THC/THC.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kwavenet_infer_wrapper.cpp:28\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:363:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   T * \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K() const {\n",
            "       \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/wavenet_infer_wrapper.o -L/content/silent_speech-dgaddy-pytorch/nv_wavenet/pytorch -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -Wl,--enable-new-dtags,-R/content/silent_speech-dgaddy-pytorch/nv_wavenet/pytorch -Wl,--enable-new-dtags,-R/usr/local/lib/python3.7/dist-packages/torch/lib -Wl,--enable-new-dtags,-R/usr/local/cuda/lib64 -lwavenet_infer -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.7/nv_wavenet_ext.cpython-37m-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.7/nv_wavenet_ext.cpython-37m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for nv_wavenet_ext.cpython-37m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/nv_wavenet_ext.py to nv_wavenet_ext.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying nv_wavenet_ext.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying nv_wavenet_ext.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying nv_wavenet_ext.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying nv_wavenet_ext.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.nv_wavenet_ext.cpython-37: module references __file__\n",
            "creating dist\n",
            "creating 'dist/nv_wavenet_ext-0.0.0-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing nv_wavenet_ext-0.0.0-py3.7-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.7/dist-packages/nv_wavenet_ext-0.0.0-py3.7-linux-x86_64.egg\n",
            "Extracting nv_wavenet_ext-0.0.0-py3.7-linux-x86_64.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding nv-wavenet-ext 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/nv_wavenet_ext-0.0.0-py3.7-linux-x86_64.egg\n",
            "Processing dependencies for nv-wavenet-ext==0.0.0\n",
            "Finished processing dependencies for nv-wavenet-ext==0.0.0\n",
            "/content/silent_speech-dgaddy-pytorch\n"
          ]
        }
      ],
      "source": [
        "if generate&gen_ss_dgaddy_pt:\n",
        "  import os\n",
        "  if os.path.isdir(\"/content/silent_speech-dgaddy-pytorch\"):\n",
        "    %cd \"/content/silent_speech-dgaddy-pytorch\"\n",
        "  else:\n",
        "    !pip install torch==1.7.1\n",
        "    %cd \"/content/\"\n",
        "    !git clone https://github.com/dgaddy/silent_speech.git /content/silent_speech-dgaddy-pytorch\n",
        "    %cd \"/content/silent_speech-dgaddy-pytorch\"\n",
        "    !git clone https://github.com/NVIDIA/nv-wavenet.git nv_wavenet\n",
        "    !git clone https://github.com/hubertsiuzdak/voice-conversion.git voice-conversion\n",
        "    %cp -TRv voice-conversion/nv_wavenet nv_wavenet/\n",
        "    %cd /content/silent_speech-dgaddy-pytorch/nv_wavenet/pytorch\n",
        "    !sed -i 's/ARCH=sm_70/ARCH=sm_37/' ./Makefile\n",
        "    !rm -rf /usr/local/cuda\n",
        "    !ln -s /usr/local/cuda-10.1 /usr/local/cuda\n",
        "    #!ln -s /usr/local/cuda-11.2 /usr/local/cuda\n",
        "    !make\n",
        "    !python build.py install\n",
        "    %cd /content/silent_speech-dgaddy-pytorch\n",
        "\n",
        "if generate&gen_sg2_nvlabs_ada_pt:\n",
        "  %cd /content\n",
        "  !git clone https://github.com/NVlabs/stylegan2-ada-pytorch /content/stylegan2-nvlabs-ada-pytorch\n",
        "  %cd /content/stylegan2-nvlabs-ada-pytorch\n",
        "\n",
        "if generate&gen_sg2_nvlabs:\n",
        "  %cd /content\n",
        "  !git clone https://github.com/NVlabs/stylegan2.git /content/stylegan2-nvlabs\n",
        "  %cd /content/stylegan2-nvlabs\n",
        "if generate&gen_sg2_nvlabs_ada:\n",
        "  %cd /content\n",
        "  !git clone https://github.com/NVlabs/stylegan2-ada.git /content/stylegan2-nvlabs-ada\n",
        "  %cd /content/stylegan2-nvlabs-ada\n",
        "if generate&gen_sg2_shawwn:\n",
        "  %cd /content\n",
        "  !git clone https://github.com/shawwn/stylegan2.git /content/stylegan2-shawwn\n",
        "  %cd /content/stylegan2-shawwn\n",
        "if generate&gen_sg2_cyrilzakka_tpu:\n",
        "  %cd /content\n",
        "  !git clone https://github.com/cyrilzakka/stylegan2-tpu.git /content/stylegan2-cyrilzakka-tpu\n",
        "  %cd /content/stylegan2-cyrilzakka-tpu\n",
        "if generate&gen_sg2_shawwn_tpu:\n",
        "  %cd /content\n",
        "  !git clone --branch tpu https://github.com/shawwn/stylegan2.git /content/stylegan2-shawwn-tpu\n",
        "  %cd /content/stylegan2-shawwn-tpu\n",
        "if generate&gen_sg2_moono_tf2:\n",
        "  %cd /content\n",
        "  !git clone https://github.com/moono/stylegan2-tf-2.x.git /content/stylegan2-moono-tf2\n",
        "  %cd /content/stylegan2-moono-tf2\n",
        "if generate&gen_sg2_rosasalberto_tf2:\n",
        "  %cd /content\n",
        "  !git clone https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x.git /content/stylegan2-rosasalberto-tf2\n",
        "  %cd /content/stylegan2-rosasalberto-tf2\n",
        "if generate&gen_sg2_nagolinc_pt:\n",
        "  %cd /content\n",
        "  !git clone https://github.com/nagolinc/stylegan2-pytorch.git /content/stylegan2-nagolinc-pytorch\n",
        "  %cd /content/stylegan2-nagolinc-pytorch\n",
        "#if generate&gen_sg2_aydao_surgery_model_release:\n",
        "#  %cd /content\n",
        "#  !git clone --branch model-release https://github.com/aydao/stylegan2-surgery.git /content/stylegan2-aydao-surgery-model-release\n",
        "#  %cd stylegan2-aydao-surgery-model-release"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmDJ996ZBg4V"
      },
      "outputs": [],
      "source": [
        "def download_file_from_google_drive(file_id,dest_path):\n",
        " import os.path\n",
        " while not os.path.exists(dest_path):\n",
        "  !mkdir -p $(dirname {dest_path})\n",
        "  !wget --save-cookies cookies.txt 'https://docs.google.com/uc?export=download&id='{file_id} -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1/p' > confirm.txt\n",
        "  import os\n",
        "  if os.path.getsize(\"confirm.txt\")==0:\n",
        "    !wget -O {dest_path} 'https://docs.google.com/uc?export=download&id='{file_id}\n",
        "  else:\n",
        "    !wget --load-cookies cookies.txt -O {dest_path} 'https://docs.google.com/uc?export=download&id='{file_id}'&confirm='$(<confirm.txt)\n",
        "  if os.path.getsize(dest_path)==0:\n",
        "    !rm {dest_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MON1WhlMATAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e170a060-a0ec-4116-c355-dfbc77821d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-04 20:41:16--  https://docs.google.com/uc?export=download&id=1_x5Ath-6CRtjoiGXrkTqz1jhhYrAISX_\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.138, 142.250.152.101, 142.250.152.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                       [ <=>                ]   2.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-04 20:41:16 (12.4 MB/s) - written to stdout [2222]\n",
            "\n",
            "--2022-05-04 20:41:16--  https://docs.google.com/uc?export=download&id=1_x5Ath-6CRtjoiGXrkTqz1jhhYrAISX_&confirm=t\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.113, 142.250.152.139, 142.250.152.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-10-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qk74225p3p64tu3g02fihelrq9pnaiff/1651696875000/04306564571908411547/*/1_x5Ath-6CRtjoiGXrkTqz1jhhYrAISX_?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:41:16--  https://doc-10-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qk74225p3p64tu3g02fihelrq9pnaiff/1651696875000/04306564571908411547/*/1_x5Ath-6CRtjoiGXrkTqz1jhhYrAISX_?e=download\n",
            "Resolving doc-10-9g-docs.googleusercontent.com (doc-10-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-10-9g-docs.googleusercontent.com (doc-10-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2022-05-04 20:41:16 ERROR 403: Forbidden.\n",
            "\n",
            "--2022-05-04 20:41:17--  https://docs.google.com/uc?export=download&id=1_x5Ath-6CRtjoiGXrkTqz1jhhYrAISX_\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.138, 142.250.152.100, 142.250.152.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                       [ <=>                ]   2.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-04 20:41:17 (39.9 MB/s) - written to stdout [2222]\n",
            "\n",
            "--2022-05-04 20:41:17--  https://docs.google.com/uc?export=download&id=1_x5Ath-6CRtjoiGXrkTqz1jhhYrAISX_&confirm=t\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.121.100, 108.177.121.102, 108.177.121.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.121.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-10-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qk74225p3p64tu3g02fihelrq9pnaiff/1651696875000/04306564571908411547/*/1_x5Ath-6CRtjoiGXrkTqz1jhhYrAISX_?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:41:17--  https://doc-10-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qk74225p3p64tu3g02fihelrq9pnaiff/1651696875000/04306564571908411547/*/1_x5Ath-6CRtjoiGXrkTqz1jhhYrAISX_?e=download\n",
            "Resolving doc-10-9g-docs.googleusercontent.com (doc-10-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-10-9g-docs.googleusercontent.com (doc-10-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2022-05-04 20:41:17 ERROR 403: Forbidden.\n",
            "\n",
            "--2022-05-04 20:41:18--  https://docs.google.com/uc?export=download&id=1_x5Ath-6CRtjoiGXrkTqz1jhhYrAISX_\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.138, 142.250.152.113, 142.250.152.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                       [ <=>                ]   2.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-04 20:41:18 (33.6 MB/s) - written to stdout [2222]\n",
            "\n",
            "--2022-05-04 20:41:18--  https://docs.google.com/uc?export=download&id=1_x5Ath-6CRtjoiGXrkTqz1jhhYrAISX_&confirm=t\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.113, 142.250.152.139, 142.250.152.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-10-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qk74225p3p64tu3g02fihelrq9pnaiff/1651696875000/04306564571908411547/*/1_x5Ath-6CRtjoiGXrkTqz1jhhYrAISX_?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:41:18--  https://doc-10-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qk74225p3p64tu3g02fihelrq9pnaiff/1651696875000/04306564571908411547/*/1_x5Ath-6CRtjoiGXrkTqz1jhhYrAISX_?e=download\n",
            "Resolving doc-10-9g-docs.googleusercontent.com (doc-10-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-10-9g-docs.googleusercontent.com (doc-10-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41740657 (40M) [application/x-zip]\n",
            "Saving to: ‘/content/silent_speech-dgaddy-pytorch/models/wavenet_model/wavenet_model_50.pt’\n",
            "\n",
            "/content/silent_spe 100%[===================>]  39.81M   110MB/s    in 0.4s    \n",
            "\n",
            "2022-05-04 20:41:18 (110 MB/s) - ‘/content/silent_speech-dgaddy-pytorch/models/wavenet_model/wavenet_model_50.pt’ saved [41740657/41740657]\n",
            "\n",
            "--2022-05-04 20:41:19--  https://docs.google.com/uc?export=download&id=1cHkkUC8xbwbCnV76ewwxU2t_GPr5r-jj\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.125.102, 142.250.125.138, 142.250.125.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.125.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                       [ <=>                ]   2.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-04 20:41:19 (53.2 MB/s) - written to stdout [2215]\n",
            "\n",
            "--2022-05-04 20:41:19--  https://docs.google.com/uc?export=download&id=1cHkkUC8xbwbCnV76ewwxU2t_GPr5r-jj&confirm=t\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.139, 142.250.152.101, 142.250.152.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0o-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/blmakgpkg4tab7denf13raoui13n8e30/1651696875000/04306564571908411547/*/1cHkkUC8xbwbCnV76ewwxU2t_GPr5r-jj?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:41:19--  https://doc-0o-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/blmakgpkg4tab7denf13raoui13n8e30/1651696875000/04306564571908411547/*/1cHkkUC8xbwbCnV76ewwxU2t_GPr5r-jj?e=download\n",
            "Resolving doc-0o-9g-docs.googleusercontent.com (doc-0o-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-0o-9g-docs.googleusercontent.com (doc-0o-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 216794709 (207M) [application/x-zip]\n",
            "Saving to: ‘/content/silent_speech-dgaddy-pytorch/models/transduction_model/model_07.pt’\n",
            "\n",
            "/content/silent_spe 100%[===================>] 206.75M  96.6MB/s    in 2.1s    \n",
            "\n",
            "2022-05-04 20:41:21 (96.6 MB/s) - ‘/content/silent_speech-dgaddy-pytorch/models/transduction_model/model_07.pt’ saved [216794709/216794709]\n",
            "\n",
            "--2022-05-04 20:41:22--  https://docs.google.com/uc?export=download&id=16UhHp3FLiDl1wwgEvOsz9hsFDxYAYnK0\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.121.139, 108.177.121.113, 108.177.121.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.121.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                       [ <=>                ]   2.18K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-04 20:41:22 (8.37 MB/s) - written to stdout [2232]\n",
            "\n",
            "--2022-05-04 20:41:22--  https://docs.google.com/uc?export=download&id=16UhHp3FLiDl1wwgEvOsz9hsFDxYAYnK0&confirm=t\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.101, 142.250.152.113, 142.250.152.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0o-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/c40k8uvnbhb7bs0vu0c721v5055dle59/1651696875000/04306564571908411547/*/16UhHp3FLiDl1wwgEvOsz9hsFDxYAYnK0?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:41:22--  https://doc-0o-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/c40k8uvnbhb7bs0vu0c721v5055dle59/1651696875000/04306564571908411547/*/16UhHp3FLiDl1wwgEvOsz9hsFDxYAYnK0?e=download\n",
            "Resolving doc-0o-9g-docs.googleusercontent.com (doc-0o-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-0o-9g-docs.googleusercontent.com (doc-0o-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 188916323 (180M) [application/octet-stream]\n",
            "Saving to: ‘/content/silent_speech-dgaddy-pytorch/deepspeech-0.7.0-models.pbmm’\n",
            "\n",
            "/content/silent_spe 100%[===================>] 180.16M   124MB/s    in 1.5s    \n",
            "\n",
            "2022-05-04 20:41:24 (124 MB/s) - ‘/content/silent_speech-dgaddy-pytorch/deepspeech-0.7.0-models.pbmm’ saved [188916323/188916323]\n",
            "\n",
            "--2022-05-04 20:41:24--  https://docs.google.com/uc?export=download&id=1q34LabqWGIOKwf5DfJYOLZksnNuc2rpv\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.121.100, 108.177.121.102, 108.177.121.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.121.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                       [ <=>                ]   2.18K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-04 20:41:24 (38.1 MB/s) - written to stdout [2234]\n",
            "\n",
            "--2022-05-04 20:41:24--  https://docs.google.com/uc?export=download&id=1q34LabqWGIOKwf5DfJYOLZksnNuc2rpv&confirm=t\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.139, 142.250.152.113, 142.250.152.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-10-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/04r4qd778amm06s0hl0drgl45i4bhd5f/1651696875000/04306564571908411547/*/1q34LabqWGIOKwf5DfJYOLZksnNuc2rpv?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:41:24--  https://doc-10-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/04r4qd778amm06s0hl0drgl45i4bhd5f/1651696875000/04306564571908411547/*/1q34LabqWGIOKwf5DfJYOLZksnNuc2rpv?e=download\n",
            "Resolving doc-10-9g-docs.googleusercontent.com (doc-10-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-10-9g-docs.googleusercontent.com (doc-10-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 953363776 (909M) [application/octet-stream]\n",
            "Saving to: ‘/content/silent_speech-dgaddy-pytorch/deepspeech-0.7.0-models.scorer’\n",
            "\n",
            "/content/silent_spe 100%[===================>] 909.20M   124MB/s    in 7.9s    \n",
            "\n",
            "2022-05-04 20:41:33 (115 MB/s) - ‘/content/silent_speech-dgaddy-pytorch/deepspeech-0.7.0-models.scorer’ saved [953363776/953363776]\n",
            "\n",
            "--2022-05-04 20:41:33--  https://docs.google.com/uc?export=download&id=1p97-FG984_OQhk0X2okpyUbByG3DJhdb\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.138, 142.250.152.113, 142.250.152.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                       [ <=>                ]   2.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-04 20:41:33 (48.6 MB/s) - written to stdout [2216]\n",
            "\n",
            "--2022-05-04 20:41:33--  https://docs.google.com/uc?export=download&id=1p97-FG984_OQhk0X2okpyUbByG3DJhdb&confirm=t\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.113, 142.250.152.139, 142.250.152.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-00-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2sf83osqft4en2rkb7f96dpl6a73h3kp/1651696875000/04306564571908411547/*/1p97-FG984_OQhk0X2okpyUbByG3DJhdb?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:41:33--  https://doc-00-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2sf83osqft4en2rkb7f96dpl6a73h3kp/1651696875000/04306564571908411547/*/1p97-FG984_OQhk0X2okpyUbByG3DJhdb?e=download\n",
            "Resolving doc-00-9g-docs.googleusercontent.com (doc-00-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-00-9g-docs.googleusercontent.com (doc-00-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 204547213 (195M) [application/zip]\n",
            "Saving to: ‘/content/silent_speech-dgaddy-pytorch/emg_data.zip’\n",
            "\n",
            "/content/silent_spe 100%[===================>] 195.07M  82.9MB/s    in 2.4s    \n",
            "\n",
            "2022-05-04 20:41:36 (82.9 MB/s) - ‘/content/silent_speech-dgaddy-pytorch/emg_data.zip’ saved [204547213/204547213]\n",
            "\n",
            "--2022-05-04 20:41:36--  https://docs.google.com/uc?export=download&id=1RNYqqutEeSpFny_yYarHLeZvgxrvE_dm\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.101, 142.250.152.113, 142.250.152.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0s-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hnb2ivmtrmojhun30ince7mctnhi04ul/1651696875000/04306564571908411547/*/1RNYqqutEeSpFny_yYarHLeZvgxrvE_dm?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:41:47--  https://doc-0s-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hnb2ivmtrmojhun30ince7mctnhi04ul/1651696875000/04306564571908411547/*/1RNYqqutEeSpFny_yYarHLeZvgxrvE_dm?e=download\n",
            "Resolving doc-0s-9g-docs.googleusercontent.com (doc-0s-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-0s-9g-docs.googleusercontent.com (doc-0s-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12467913 (12M) [application/gzip]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]  11.89M  51.4MB/s    in 0.2s    \n",
            "\n",
            "2022-05-04 20:41:48 (51.4 MB/s) - written to stdout [12467913/12467913]\n",
            "\n",
            "--2022-05-04 20:41:48--  https://docs.google.com/uc?export=download&id=1RNYqqutEeSpFny_yYarHLeZvgxrvE_dm\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.125.139, 142.250.125.138, 142.250.125.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.125.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0s-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hnb2ivmtrmojhun30ince7mctnhi04ul/1651696875000/04306564571908411547/*/1RNYqqutEeSpFny_yYarHLeZvgxrvE_dm?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:41:59--  https://doc-0s-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hnb2ivmtrmojhun30ince7mctnhi04ul/1651696875000/04306564571908411547/*/1RNYqqutEeSpFny_yYarHLeZvgxrvE_dm?e=download\n",
            "Resolving doc-0s-9g-docs.googleusercontent.com (doc-0s-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-0s-9g-docs.googleusercontent.com (doc-0s-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2022-05-04 20:41:59 ERROR 403: Forbidden.\n",
            "\n",
            "--2022-05-04 20:42:00--  https://docs.google.com/uc?export=download&id=1RNYqqutEeSpFny_yYarHLeZvgxrvE_dm\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.102, 142.250.152.101, 142.250.152.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0s-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hnb2ivmtrmojhun30ince7mctnhi04ul/1651696875000/04306564571908411547/*/1RNYqqutEeSpFny_yYarHLeZvgxrvE_dm?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:42:00--  https://doc-0s-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hnb2ivmtrmojhun30ince7mctnhi04ul/1651696875000/04306564571908411547/*/1RNYqqutEeSpFny_yYarHLeZvgxrvE_dm?e=download\n",
            "Resolving doc-0s-9g-docs.googleusercontent.com (doc-0s-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-0s-9g-docs.googleusercontent.com (doc-0s-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12467913 (12M) [application/gzip]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]  11.89M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-05-04 20:42:01 (111 MB/s) - written to stdout [12467913/12467913]\n",
            "\n",
            "--2022-05-04 20:42:01--  https://docs.google.com/uc?export=download&id=1RNYqqutEeSpFny_yYarHLeZvgxrvE_dm\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.121.138, 108.177.121.113, 108.177.121.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.121.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0s-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hnb2ivmtrmojhun30ince7mctnhi04ul/1651696875000/04306564571908411547/*/1RNYqqutEeSpFny_yYarHLeZvgxrvE_dm?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:42:01--  https://doc-0s-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hnb2ivmtrmojhun30ince7mctnhi04ul/1651696875000/04306564571908411547/*/1RNYqqutEeSpFny_yYarHLeZvgxrvE_dm?e=download\n",
            "Resolving doc-0s-9g-docs.googleusercontent.com (doc-0s-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-0s-9g-docs.googleusercontent.com (doc-0s-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12467913 (12M) [application/gzip]\n",
            "Saving to: ‘/content/silent_speech-dgaddy-pytorch/out.tar.gz’\n",
            "\n",
            "/content/silent_spe 100%[===================>]  11.89M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-05-04 20:42:02 (187 MB/s) - ‘/content/silent_speech-dgaddy-pytorch/out.tar.gz’ saved [12467913/12467913]\n",
            "\n",
            "--2022-05-04 20:42:02--  https://docs.google.com/uc?export=download&id=1Adhn8Y4qplXMtp44VqRm7esUdhxd3pQk\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.101, 142.250.152.102, 142.250.152.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-00-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8m593jfkmgvsnl52nq2i6cb3a9c34i83/1651696875000/04306564571908411547/*/1Adhn8Y4qplXMtp44VqRm7esUdhxd3pQk?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:42:02--  https://doc-00-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8m593jfkmgvsnl52nq2i6cb3a9c34i83/1651696875000/04306564571908411547/*/1Adhn8Y4qplXMtp44VqRm7esUdhxd3pQk?e=download\n",
            "Resolving doc-00-9g-docs.googleusercontent.com (doc-00-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-00-9g-docs.googleusercontent.com (doc-00-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 345718 (338K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>] 337.62K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2022-05-04 20:42:03 (99.5 MB/s) - written to stdout [345718/345718]\n",
            "\n",
            "--2022-05-04 20:42:03--  https://docs.google.com/uc?export=download&id=1Adhn8Y4qplXMtp44VqRm7esUdhxd3pQk\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.138, 142.250.152.101, 142.250.152.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-00-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8m593jfkmgvsnl52nq2i6cb3a9c34i83/1651696875000/04306564571908411547/*/1Adhn8Y4qplXMtp44VqRm7esUdhxd3pQk?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:42:03--  https://doc-00-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8m593jfkmgvsnl52nq2i6cb3a9c34i83/1651696875000/04306564571908411547/*/1Adhn8Y4qplXMtp44VqRm7esUdhxd3pQk?e=download\n",
            "Resolving doc-00-9g-docs.googleusercontent.com (doc-00-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-00-9g-docs.googleusercontent.com (doc-00-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 345718 (338K) [text/plain]\n",
            "Saving to: ‘/content/silent_speech-dgaddy-pytorch/books/War_of_the_Worlds.txt’\n",
            "\n",
            "/content/silent_spe 100%[===================>] 337.62K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2022-05-04 20:42:03 (60.7 MB/s) - ‘/content/silent_speech-dgaddy-pytorch/books/War_of_the_Worlds.txt’ saved [345718/345718]\n",
            "\n",
            "--2022-05-04 20:42:03--  https://docs.google.com/uc?export=download&id=1YoycqFrjtWnDM67vSc4yC3U-WxOUSvrN\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.138, 142.250.152.113, 142.250.152.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3ki246fjcpv8kmu2vlme6i8smh6kllbj/1651696875000/04306564571908411547/*/1YoycqFrjtWnDM67vSc4yC3U-WxOUSvrN?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:42:03--  https://doc-04-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3ki246fjcpv8kmu2vlme6i8smh6kllbj/1651696875000/04306564571908411547/*/1YoycqFrjtWnDM67vSc4yC3U-WxOUSvrN?e=download\n",
            "Resolving doc-04-9g-docs.googleusercontent.com (doc-04-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-04-9g-docs.googleusercontent.com (doc-04-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92 [application/json]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]      92  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-04 20:42:04 (4.03 MB/s) - written to stdout [92/92]\n",
            "\n",
            "--2022-05-04 20:42:04--  https://docs.google.com/uc?export=download&id=1YoycqFrjtWnDM67vSc4yC3U-WxOUSvrN\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.152.138, 142.250.152.113, 142.250.152.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.152.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3ki246fjcpv8kmu2vlme6i8smh6kllbj/1651696875000/04306564571908411547/*/1YoycqFrjtWnDM67vSc4yC3U-WxOUSvrN?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:42:04--  https://doc-04-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3ki246fjcpv8kmu2vlme6i8smh6kllbj/1651696875000/04306564571908411547/*/1YoycqFrjtWnDM67vSc4yC3U-WxOUSvrN?e=download\n",
            "Resolving doc-04-9g-docs.googleusercontent.com (doc-04-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-04-9g-docs.googleusercontent.com (doc-04-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2022-05-04 20:42:04 ERROR 403: Forbidden.\n",
            "\n",
            "--2022-05-04 20:42:04--  https://docs.google.com/uc?export=download&id=1YoycqFrjtWnDM67vSc4yC3U-WxOUSvrN\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.132.139, 74.125.132.113, 74.125.132.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.132.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3ki246fjcpv8kmu2vlme6i8smh6kllbj/1651696875000/04306564571908411547/*/1YoycqFrjtWnDM67vSc4yC3U-WxOUSvrN?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:42:05--  https://doc-04-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3ki246fjcpv8kmu2vlme6i8smh6kllbj/1651696875000/04306564571908411547/*/1YoycqFrjtWnDM67vSc4yC3U-WxOUSvrN?e=download\n",
            "Resolving doc-04-9g-docs.googleusercontent.com (doc-04-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-04-9g-docs.googleusercontent.com (doc-04-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92 [application/json]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]      92  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-04 20:42:05 (4.22 MB/s) - written to stdout [92/92]\n",
            "\n",
            "--2022-05-04 20:42:05--  https://docs.google.com/uc?export=download&id=1YoycqFrjtWnDM67vSc4yC3U-WxOUSvrN\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.125.139, 142.250.125.138, 142.250.125.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.125.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3ki246fjcpv8kmu2vlme6i8smh6kllbj/1651696875000/04306564571908411547/*/1YoycqFrjtWnDM67vSc4yC3U-WxOUSvrN?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-04 20:42:05--  https://doc-04-9g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3ki246fjcpv8kmu2vlme6i8smh6kllbj/1651696875000/04306564571908411547/*/1YoycqFrjtWnDM67vSc4yC3U-WxOUSvrN?e=download\n",
            "Resolving doc-04-9g-docs.googleusercontent.com (doc-04-9g-docs.googleusercontent.com)... 142.250.152.132, 2607:f8b0:4001:c56::84\n",
            "Connecting to doc-04-9g-docs.googleusercontent.com (doc-04-9g-docs.googleusercontent.com)|142.250.152.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92 [application/json]\n",
            "Saving to: ‘/content/silent_speech-dgaddy-pytorch/testset_onlinedev.json’\n",
            "\n",
            "/content/silent_spe 100%[===================>]      92  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-04 20:42:05 (4.06 MB/s) - ‘/content/silent_speech-dgaddy-pytorch/testset_onlinedev.json’ saved [92/92]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "files_path=[]\n",
        "if generate&gen_drums:\n",
        "  files_path = [['1ZJir-_ls92s56LFmw_HVuQ7ANqFFN5WG', '/content/model/model.ckpt-18637.data-00000-of-00001'],\n",
        "\n",
        "              ['1d5ayi4w-70AvKYPk-8sXzsSzpK1jMRgm', '/content/model/model.ckpt-18637.index'],  \n",
        "              ['15CWn0yK3FKsHbAOGNLQYVg4eZC1oNIrL', '/content/model/model.ckpt-18637.meta'],  \n",
        "              ['1x5QEFeoochk-rhvtvJc98kIB5_SAwn0u', '/content/model/args.txt'],  \n",
        "              ['1UgSZaBTCTDXaPbfv8l0wmpHbD5u051o5', '/content/model/graph.pbtxt'],  \n",
        "              ['1LGfAkuOFvA3NdFE_rOq9WyeXGGgEOf0F', '/content/model/checkpoint'],  \n",
        "              ['1bPD0bXCC_18oWbUjmjkacF-CShlA6yNd', '/content/model/infer/infer.pbtxt'],  \n",
        "              ['13OQuRx7Ku6KJ9o9FU-JN3yB0Njul9Vem', '/content/model/infer/infer.meta']]\n",
        "for i in range(len(files_path)):\n",
        "  download_file_from_google_drive(file_id=files_path[i][0], dest_path=files_path[i][1])\n",
        "files_path=[]\n",
        "\n",
        "if generate&gen_ss_wm50_tm07_dm070:\n",
        "    files_path = [['1_x5Ath-6CRtjoiGXrkTqz1jhhYrAISX_', '/content/silent_speech-dgaddy-pytorch/models/wavenet_model/wavenet_model_50.pt', \n",
        "                 'wavenet_model_50',''],\n",
        "                  ['1cHkkUC8xbwbCnV76ewwxU2t_GPr5r-jj', '/content/silent_speech-dgaddy-pytorch/models/transduction_model/model_07.pt',\n",
        "                 'model_07',''],\n",
        "                  ['16UhHp3FLiDl1wwgEvOsz9hsFDxYAYnK0', '/content/silent_speech-dgaddy-pytorch/deepspeech-0.7.0-models.pbmm', \n",
        "                 'deepspeech-0.7.0-models',''],\n",
        "                  ['1q34LabqWGIOKwf5DfJYOLZksnNuc2rpv', '/content/silent_speech-dgaddy-pytorch/deepspeech-0.7.0-models.scorer', \n",
        "                 'deepspeech-0.7.0-models',''],\n",
        "                  ['1p97-FG984_OQhk0X2okpyUbByG3DJhdb', '/content/silent_speech-dgaddy-pytorch/emg_data.zip', \n",
        "                 'emg_data',''],\n",
        "                  ['1RNYqqutEeSpFny_yYarHLeZvgxrvE_dm', '/content/silent_speech-dgaddy-pytorch/out.tar.gz', \n",
        "                 'out',''],\n",
        "                  ['1Adhn8Y4qplXMtp44VqRm7esUdhxd3pQk', '/content/silent_speech-dgaddy-pytorch/books/War_of_the_Worlds.txt', \n",
        "                 'book',''],\n",
        "                  ['1YoycqFrjtWnDM67vSc4yC3U-WxOUSvrN', '/content/silent_speech-dgaddy-pytorch/testset_onlinedev.json', \n",
        "                 'testset_onlinedev','']]\n",
        "if generate&gen_abctract_art:\n",
        "  files_path = [['1ie1vWw1JNsfrZWRtMvhteqzVz4mt4KGa', '/content/model/sg2-ada_abstract_network-snapshot-000188.pkl',\n",
        "                 'sg2-ada_abstract_network-snapshot-000188','stylegan2-ada']]\n",
        "if generate&gen_anime_protraits:\n",
        "  files_path = [['1aUrChOhq5jDEddZK1v_Dp1vYNlHSBL9o', '/content/model/sg2-ada_2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664.pkl', \n",
        "                 'sg2-ada_2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664','stylegan2-ada']]\n",
        "\n",
        "if generate&gen_abctract_art:\n",
        "  if generate&gen_anime_protraits:\n",
        "    files_path = [['1aUrChOhq5jDEddZK1v_Dp1vYNlHSBL9o', '/content/model/sg2-ada_2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664.pkl', \n",
        "                 'sg2-ada_2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664','stylegan2-ada'],\n",
        "                  ['1ie1vWw1JNsfrZWRtMvhteqzVz4mt4KGa', '/content/model/sg2-ada_abstract_network-snapshot-000188.pkl',\n",
        "                 'sg2-ada_abstract_network-snapshot-000188','stylegan2-ada']]\n",
        "\n",
        "if generate&gen_tadne:\n",
        "#  files_path = [['1sdnL-lIl2kYAnuleafK-5GPiLNHfxh4W', '/content/model/sg2-ext_aydao-anime-danbooru2019s-512-5268480.pkl', \n",
        "#                 'sg2-ext_aydao-anime-danbooru2019s-512-5268480','stylegan2-shawwn']]\n",
        "  files_path = [['1LCkyOPmcWBsPlQX_DxKAuPM1Ew_nh83I', '/content/model/sg2-ext_network-tadne.pkl', \n",
        "                 'sg2-ext_network-tadne','stylegan2-shawwn']]\n",
        "  #files_path = [['1l5zG0g_RMEAwFUK_veD1EZweVEoY9gUT', '/content/model/aydao-anime-danbooru2019s-512-5268480.pkl']]\n",
        "#  files_path = [['1BHeqOZ58WZ-vACR2MJkh1ZVbJK2B-Kle', '/content/model/network-snapshot-017325.pkl']]\n",
        "#  files_path = [['1WNQELgHnaqMTq3TlrnDaVkyrAH8Zrjez', '/content/model/network-snapshot-018528.pkl']]\n",
        "\n",
        "if generate&gen_anime:\n",
        "  files_path = [['1YckI8gwqPbZBI8X4eaQAJCgWx-CqCTdi', '/content/model/sg2_anime_network-snapshot-018528.pkl', \n",
        "                 'sg2_anime_network-snapshot-018528']]\n",
        "if generate&gen_anime_tf2:\n",
        "  files_path = [['1-1neAg_FUymzBvCStMe7CfV94-VD22kk', '/content/stylegan2-moono-tf2/official-converted/cuda/ckpt-0.data-00000-of-00001'],\n",
        "              ['1-4ih0wi68y4xH5tg0_kClpuWDSnvdmoE', '/content/stylegan2-moono-tf2/official-converted/cuda/ckpt-0.index'],\n",
        "              ['1-C6H58vmfZykqWpilR1u9puH8oPFQtcQ', '/content/stylegan2-moono-tf2/official-converted/cuda/checkpoint']]\n",
        "if generate&gen_abctract_art_tf2_npy:\n",
        "#  files_path = [['1cauGWIVGGiMJA0_OZftJU3-rVAVdFwZM', '/content/stylegan2-rosasalberto-tf2/weights/sg2-ada_abstract_network-snapshot-000188.npy',\n",
        "#                 'sg2-ada_abstract_network-snapshot-000188']]\n",
        "  files_path = [['1-CXjDfP_g5ZD5aC9AwOXEC5WNIf5dCEh', '/content/stylegan2-rosasalberto-tf2/weights/sg2-ada_abstract_network-snapshot-000188.npy',\n",
        "                 'sg2-ada_abstract_network-snapshot-000188']]\n",
        "if generate&gen_anime_protraits_tf2_npy:\n",
        "#  files_path = [['1-Cp-RRJnjvfCIrD0ylaUYxvLbxN4aj8K', '/content/stylegan2-rosasalberto-tf2/weights/sg2-ada_2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664.npy', \n",
        "#                 'sg2-ada_2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664']]\n",
        "  files_path = [['1-AiS_pdkssIz_nU9GYSLJRZiXgJpCrSo', '/content/stylegan2-rosasalberto-tf2/weights/sg2-ada_2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664.npy', \n",
        "                 'sg2-ada_2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664']]\n",
        "if generate&gen_tadne_tf2_npy:\n",
        "  files_path = [['1-36-rfueVBWvigBvCuwzrXKl1AeVtzu6', '/content/stylegan2-rosasalberto-tf2/weights/sg2-ext_aydao-anime-danbooru2019s-512-5268480.npy', \n",
        "                 'sg2-ext_aydao-anime-danbooru2019s-512-5268480']]\n",
        "if generate&gen_anime_tf2_npy:\n",
        "  files_path = [['1--ajK29hgTTAYNcZhQk9lLFhwUlXqxNA', '/content/stylegan2-rosasalberto-tf2/weights/sg2_anime_network-snapshot-018528.npy', \n",
        "                 'sg2_anime_network-snapshot-018528']]\n",
        "if generate&gen_thdne:\n",
        "  files_path = [['106KNd9oqMmslYKpGmjUkyjNZzpd_vKmM', '/content/model/sg2-ext_thdne-120320.pkl', \n",
        "                 'sg2-ext_thdne','stylegan2-shawwn']]          \n",
        "#  files_path = [['1-Fop9RImTgWh3-WtehUAKNfcPskmzx1O', '/content/model/sg2-ext_thdne-34048.pkl', \n",
        "#                 'sg2-ext_thdne','stylegan2-shawwn']]          \n",
        "#  files_path = [['1-KLjFV2mCQw7AiofYO-EKSV5XZutbDcn', '/content/model/sg2-ext_thdne-16384.pkl', \n",
        "#                 'sg2-ext_thdne','stylegan2-shawwn']]          \n",
        "#  files_path = [['1zBXH6O-6i3TPorFAVZ8HqgamjWq3WTeg', '/content/model/sg2-ext_thdne-113152.pkl', \n",
        "#                 'sg2-ext_thdne','stylegan2-shawwn']]          \n",
        "#  files_path = [['1-GdLwuYWdu3QLGQJWKCRsLLYjgQpxrgd', '/content/model/sg2-ext_thdne-95232.pkl', \n",
        "#                 'sg2-ext_thdne','stylegan2-shawwn']]\n",
        "#  files_path = [['1O1dCRbeMjD0EemjVHWmoO_x-58vOzNuK', '/content/model/sg2-ext_thdne-latest.pkl', \n",
        "#                 'sg2-ext_thdne','stylegan2-shawwn']]          \n",
        "#  files_path = [['1-Nhf4lcxSiUvCmQiHJNU1GfFTVNWobvY', '/content/model/sg2-ext_thdne-54784.pkl', \n",
        "#                 'sg2-ext_thdne','stylegan2-shawwn']]          \n",
        "#if generate&gen_thdne_256:\n",
        "#  files_path = [['1-J_b-nX0KnKK_fDkZ8KC0RgoP9CTmAxy', '/content/model/sg2-ext_network-thdne_256.pkl', \n",
        "#                 'sg2-ext_network-thdne_256','stylegan2-shawwn']]\n",
        "#if generate&gen_thdne_256_pt:\n",
        "#  files_path = [['1L3joymV2LartzOSMXyzzniGr57Gqf_1z', '/content/model/sg2-ada-pt_thdne-snapshot-256-latest.pkl',\n",
        "#                 'sg2-ada-pt_thdne-snapshot-256-latest','stylegan2-ada-pytorch']]\n",
        "\n",
        "#https://drive.google.com/file/d/1ie1vWw1JNsfrZWRtMvhteqzVz4mt4KGa/view?usp=sharing\n",
        "#network-snapshot-000188.pkl\n",
        "#https://drive.google.com/file/d/1YckI8gwqPbZBI8X4eaQAJCgWx-CqCTdi/view?usp=sharing\n",
        "#network-snapshot-018528.pkl\n",
        "\n",
        "\n",
        "for i in range(len(files_path)):\n",
        "  download_file_from_google_drive(file_id=files_path[i][0], dest_path=files_path[i][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1cqleJU9YdF"
      },
      "outputs": [],
      "source": [
        "#!python convert_ckpt_to_pkl.py --ckpt_model_dir gs://train_with_tpu/networks_aydao/sq-512-135680 --tpu_address={tpu_address} --output_pkl_dir gs://train_with_tpu/networks_aydao/ --reference_pkl gs://train_with_tpu/models/2020-11-27-aydao-stylegan2ext-danbooru2019s-512px-5268480.pkl\n",
        "#files_path = [['', 'gs://train_with_tpu/networks_aydao/model.ckpt-113152.pkl', 'sg2-ext_thdne','stylegan2-shawwn']]  \n",
        "#!gsutil cp -r gs://train_with_tpu/networks_aydao/model.ckpt-113152.pkl /content/drive/MyDrive/networks_aydao/model.ckpt-113152.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbqAzyggAXtL"
      },
      "outputs": [],
      "source": [
        "if generate&gen_stylegan2:\n",
        "  !pip install Pillow\n",
        "  import PIL.Image \n",
        "  !pip install tqdm\n",
        "  from tqdm import tqdm\n",
        "  !pip install imageio==2.4.1\n",
        "  !pip install imageio-ffmpeg==0.4.3 pyspng==0.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqYszSxLKjt1"
      },
      "outputs": [],
      "source": [
        "if generate&gen_sg2_rosasalberto_tf2:\n",
        "  import tensorflow as tf\n",
        "  import numpy as np\n",
        "  !pip install matplotlib\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  from utils.utils_stylegan2 import convert_images_to_uint8\n",
        "\n",
        "  def generate_and_plot_images(gen, seed, w_avg, truncation_psi=1):\n",
        "    \"\"\" plot images from generator output \"\"\"\n",
        "    \n",
        "    fig, ax = plt.subplots(1,3,figsize=(15,15))\n",
        "    for i in range(3):\n",
        "    \n",
        "        # creating random latent vector\n",
        "        rnd = np.random.RandomState(seed)\n",
        "        z = rnd.randn(1, 512).astype('float32')\n",
        "\n",
        "        # running mapping network\n",
        "        dlatents = gen.mapping_network(z)\n",
        "        # adjusting dlatents depending on truncation psi, if truncatio_psi = 1, no adjust\n",
        "        dlatents = w_avg + (dlatents - w_avg) * truncation_psi \n",
        "        # running synthesis network\n",
        "        out = gen.synthesis_network(dlatents)\n",
        "\n",
        "        #converting image/s to uint8\n",
        "        img = convert_images_to_uint8(out, nchw_to_nhwc=True, uint8_cast=True)\n",
        "\n",
        "        #plotting images\n",
        "        ax[i].axis('off')\n",
        "        img_plot = ax[i].imshow(img.numpy()[0])\n",
        "        \n",
        "        seed += 1\n",
        "\n",
        "  impl = 'ref' # 'ref' if cuda is not available in your machine\n",
        "  gpu = False # False if tensorflow cpu is used\n",
        "  if generate&gen_tpu:\n",
        "    impl = 'ref' # 'ref' if cuda is not available in your machine\n",
        "    gpu = False # False if tensorflow cpu is used\n",
        "  if generate&gen_gpu:\n",
        "    impl = 'cuda' # 'ref' if cuda is not available in your machine\n",
        "    gpu = True # False if tensorflow cpu is used\n",
        "\n",
        "  import tensorflow as tf\n",
        "  import numpy as np\n",
        "\n",
        "  from utils.weights_map import available_weights, synthesis_weights, mapping_weights, weights_stylegan2_dir\n",
        "  from utils.utils_stylegan2 import nf\n",
        "  from layers.dense_layer import DenseLayer\n",
        "  from layers.synthesis_main_layer import SynthesisMainLayer\n",
        "  from layers.to_rgb_layer import ToRgbLayer\n",
        "  from dnnlib.ops.upfirdn_2d import upsample_2d\n",
        "\n",
        "  class MappingNetwork(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    StyleGan2 generator mapping network, from z to dlatents for tensorflow 2.x\n",
        "    \"\"\"\n",
        "    def __init__(self, resolution=1024, **kwargs):\n",
        "        \n",
        "        super(MappingNetwork, self).__init__(**kwargs)\n",
        "        \n",
        "        self.dlatent_size = 512\n",
        "        self.dlatent_vector = (int(np.log2(resolution))-1)*2\n",
        "        self.mapping_layers = 8\n",
        "        self.lrmul = 0.01\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.weights_dict = {}\n",
        "        for i in range(self.mapping_layers):\n",
        "            setattr(self, 'Dense{}'.format(i), DenseLayer(fmaps=512, lrmul=self.lrmul, name='Dense{}'.format(i)))\n",
        "    \n",
        "        self.g_mapping_broadcast = tf.keras.layers.RepeatVector(self.dlatent_vector)\n",
        "            \n",
        "    def call(self, z):\n",
        "        \n",
        "        z = tf.cast(z, 'float32')\n",
        "        \n",
        "        # Normalize inputs\n",
        "        scale = tf.math.rsqrt(tf.reduce_mean(tf.square(z), axis=1, keepdims=True) + 1e-8)\n",
        "        x = tf.math.multiply(z, scale)\n",
        "        \n",
        "        # Mapping\n",
        "        for i in range(self.mapping_layers):\n",
        "        \n",
        "            x = getattr(self, 'Dense{}'.format(i))(x)\n",
        "            x = tf.math.multiply(tf.nn.leaky_relu(x, 0.2), tf.math.sqrt(2.))\n",
        "        \n",
        "        # Broadcasting\n",
        "        dlatents = self.g_mapping_broadcast(x)\n",
        "        \n",
        "        return dlatents\n",
        "\n",
        "  class SynthesisNetwork(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    StyleGan2 generator synthesis network from dlatents to img tensor for tensorflow 2.x\n",
        "    \"\"\"\n",
        "    def __init__(self, resolution=1024, impl='cuda', gpu=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        resolution : int, optional\n",
        "            Resolution output of the synthesis network, will be parsed to the floor integer power of 2. \n",
        "            The default is 1024.\n",
        "        impl : str, optional\n",
        "            Wether to run some convolutions in custom tensorflow operations or cuda operations. 'ref' and 'cuda' available.\n",
        "            The default is 'cuda'.\n",
        "        gpu : boolean, optional\n",
        "            Wether to use gpu. The default is True.\n",
        "        \"\"\"\n",
        "        super(SynthesisNetwork, self).__init__(**kwargs)\n",
        "        \n",
        "        self.impl = impl\n",
        "        self.gpu = gpu\n",
        "        self.resolution = resolution\n",
        "        \n",
        "        self.resolution_log2 = int(np.log2(self.resolution))\n",
        "        self.resample_kernel = [1, 3, 3, 1]\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        \n",
        "        #constant layer\n",
        "        self.const_4_4 = self.add_weight(name='4x4/Const/const', shape=(1, 512, 4, 4), \n",
        "                                        initializer=tf.random_normal_initializer(0, 1), trainable=True)\n",
        "        #early layer 4x4\n",
        "        self.layer_4_4 = SynthesisMainLayer(fmaps=nf(1), impl=self.impl, gpu=self.gpu, name='4x4')\n",
        "        self.torgb_4_4 = ToRgbLayer(impl=self.impl, gpu=self.gpu, name='4x4')\n",
        "        #main layers\n",
        "        for res in range(3, self.resolution_log2 + 1):\n",
        "            res_str = str(2**res)\n",
        "            setattr(self, 'layer_{}_{}_up'.format(res_str, res_str), \n",
        "                    SynthesisMainLayer(fmaps=nf(res-1), impl=self.impl, gpu=self.gpu, up=True, name='{}x{}'.format(res_str, res_str)))\n",
        "            setattr(self, 'layer_{}_{}'.format(res_str, res_str), \n",
        "                    SynthesisMainLayer(fmaps=nf(res-1), impl=self.impl, gpu=self.gpu, name='{}x{}'.format(res_str, res_str)))\n",
        "            setattr(self, 'torgb_{}_{}'.format(res_str, res_str), \n",
        "                    ToRgbLayer(impl=self.impl, gpu=self.gpu, name='{}x{}'.format(res_str, res_str)))\n",
        "        \n",
        "    def call(self, dlatents_in):\n",
        "        \n",
        "        dlatents_in = tf.cast(dlatents_in, 'float32')\n",
        "        y = None\n",
        "        \n",
        "        # Early layers\n",
        "        x = tf.tile(tf.cast(self.const_4_4, 'float32'), [tf.shape(dlatents_in)[0], 1, 1, 1])\n",
        "        x = self.layer_4_4(x, dlatents_in[:, 0])\n",
        "        y = self.torgb_4_4(x, dlatents_in[:, 1], y)\n",
        "                \n",
        "        # Main layers\n",
        "        for res in range(3, self.resolution_log2 + 1):\n",
        "            x = getattr(self, 'layer_{}_{}_up'.format(2**res, 2**res))(x, dlatents_in[:, res*2-5])\n",
        "            x = getattr(self, 'layer_{}_{}'.format(2**res, 2**res))(x, dlatents_in[:, res*2-4])\n",
        "            y = upsample_2d(y, k=self.resample_kernel, impl=self.impl, gpu=self.gpu)\n",
        "            y = getattr(self, 'torgb_{}_{}'.format(2**res, 2**res))(x, dlatents_in[:, res*2-3], y)\n",
        "                    \n",
        "        images_out = y\n",
        "        return tf.identity(images_out, name='images_out')\n",
        "    \n",
        "  class StyleGan2Generator(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    StyleGan2 generator config f for tensorflow 2.x\n",
        "    \"\"\"\n",
        "    def __init__(self, resolution=1024, weights=None, impl='cuda', gpu=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        resolution : int, optional\n",
        "            Resolution output of the synthesis network, will be parsed \n",
        "            to the floor integer power of 2. \n",
        "            The default is 1024.\n",
        "        weights : string, optional\n",
        "            weights name in weights dir to be loaded. The default is None.\n",
        "        impl : str, optional\n",
        "            Wether to run some convolutions in custom tensorflow operations \n",
        "            or cuda operations. 'ref' and 'cuda' available.\n",
        "            The default is 'cuda'.\n",
        "        gpu : boolean, optional\n",
        "            Wether to use gpu. The default is True.\n",
        "        \"\"\"\n",
        "        super(StyleGan2Generator, self).__init__(**kwargs)\n",
        "        \n",
        "        self.resolution = resolution\n",
        "        if weights is not None: self.__adjust_resolution(weights)\n",
        "\n",
        "        self.mapping_network = MappingNetwork(resolution=self.resolution,name='Mapping_network')\n",
        "        self.synthesis_network = SynthesisNetwork(resolution=self.resolution, impl=impl, \n",
        "                                                  gpu=gpu, name='Synthesis_network')\n",
        "        \n",
        "        # load weights\n",
        "        if weights is not None:\n",
        "            #we run the network to define it, not the most efficient thing to do...\n",
        "            _ = self(tf.zeros(shape=(1, 512)))\n",
        "            self.__load_weights(weights)\n",
        "        \n",
        "    def call(self, z):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        z : tensor, latent vector of shape [batch, 512]\n",
        "        Returns\n",
        "        -------\n",
        "        img : tensor, image generated by the generator of shape  [batch, channel, height, width]\n",
        "        \"\"\"\n",
        "        dlatents = self.mapping_network(z)\n",
        "        img = self.synthesis_network(dlatents)\n",
        "\n",
        "        return img\n",
        "    \n",
        "    def __adjust_resolution(self, weights_name):\n",
        "        \"\"\"\n",
        "        Adjust resolution of the synthesis network output. \n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        weights_name : name of the weights\n",
        "        Returns\n",
        "        -------\n",
        "        None.\n",
        "        \"\"\"\n",
        "        if  weights_name == 'ffhq': \n",
        "            self.resolution = 1024\n",
        "        elif weights_name == 'car': \n",
        "            self.resolution = 512\n",
        "        elif weights_name in ['cat', 'church', 'horse']: \n",
        "            self.resolution = 256\n",
        "        elif weights_name == 'sg2-ada_2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664': \n",
        "            self.resolution = 512\n",
        "        elif weights_name == 'sg2_anime_network-snapshot-018528': \n",
        "            self.resolution = 512\n",
        "        elif weights_name == 'sg2-ext_aydao-anime-danbooru2019s-512-5268480': \n",
        "            self.resolution = 1024\n",
        "        elif weights_name == 'sg2-ada_abstract_network-snapshot-000188': \n",
        "            self.resolution = 1024    \n",
        "    def __load_weights(self, weights_name):\n",
        "        \"\"\"\n",
        "        Load pretrained weights, stored as a dict with numpy arrays.\n",
        "        Parameters\n",
        "        ----------\n",
        "        weights_name : name of the weights\n",
        "        Returns\n",
        "        -------\n",
        "        None.\n",
        "        \"\"\"\n",
        "        \n",
        "        if (weights_name in available_weights) and type(weights_name) == str:\n",
        "            data = np.load(weights_stylegan2_dir + weights_name + '.npy', allow_pickle=True)[()]\n",
        "            #datatmp = np.load(weights_stylegan2_dir + weights_name + '.npy', allow_pickle=True)[()]\n",
        "            #data=datatmp.copy()\n",
        "            #for key in datatmp.keys(): \n",
        "            #  if not (key[:4]=='disc'):\n",
        "            #    del data[key]\n",
        "            for key in data.keys(): \n",
        "              print(key)\n",
        "            \n",
        "            weights_mapping = [data.get(key) for key in mapping_weights]\n",
        "            print(weights_mapping)\n",
        "            weights_synthesis = [data.get(key) for key in synthesis_weights[weights_name]]\n",
        "            #print(weights_synthesis)\n",
        "            \n",
        "            self.mapping_network.set_weights(weights_mapping)\n",
        "            self.synthesis_network.set_weights(weights_synthesis)\n",
        "            \n",
        "            print(\"Loaded {} generator weights!\".format(weights_name))\n",
        "        else:\n",
        "            raise Exception('Cannot load {} weights'.format(weights_name))\n",
        "\n",
        "  def generate_and_plot_images_notrunc(gen, seed):\n",
        "    \"\"\" plot images from generator output \"\"\"\n",
        "    \n",
        "    fig, ax = plt.subplots(1,3,figsize=(15,15))\n",
        "    for i in range(3):\n",
        "    \n",
        "        # creating random latent vector\n",
        "        rnd = np.random.RandomState(seed)\n",
        "        z = rnd.randn(1, 512).astype('float32')\n",
        "\n",
        "        # running mapping network\n",
        "        dlatents = gen.mapping_network(z)\n",
        "        # adjusting dlatents depending on truncation psi, if truncatio_psi = 1, no adjust\n",
        "        #dlatents = w_avg + (dlatents - w_avg) * truncation_psi \n",
        "        # running synthesis network\n",
        "        out = gen.synthesis_network(dlatents)\n",
        "\n",
        "        #converting image/s to uint8\n",
        "        img = convert_images_to_uint8(out, nchw_to_nhwc=True, uint8_cast=True)\n",
        "\n",
        "        #plotting images\n",
        "        ax[i].axis('off')\n",
        "        img_plot = ax[i].imshow(img.numpy()[0])\n",
        "        #plt.axis('off')\n",
        "        #plt.imshow(img.numpy()[0])\n",
        "        #plt.show()\n",
        "        seed += 1\n",
        "\n",
        "\n",
        "  weights_name = files_path[0][2]\n",
        "\n",
        "  from utils.weights_map import synthesis_weights_1024, synthesis_weights_512, synthesis_weights_256\n",
        "  from utils.weights_map import discriminator_weights_1024, discriminator_weights_512, discriminator_weights_256\n",
        "\n",
        "  available_weights = ['ffhq', 'car', 'cat', 'church', 'horse', \n",
        "    'sg2-ada_2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664',\n",
        "    'sg2_anime_network-snapshot-018528',\n",
        "    'sg2-ext_aydao-anime-danbooru2019s-512-5268480',\n",
        "    'sg2-ada_abstract_network-snapshot-000188']\n",
        "\n",
        "  mapping_weights = [ 'Dense0/weight', 'Dense0/bias',\n",
        "                    'Dense1/weight', 'Dense1/bias'\n",
        "                    ,\n",
        "                    'Dense2/weight', 'Dense2/bias',\n",
        "                    'Dense3/weight', 'Dense3/bias',\n",
        "                    'Dense4/weight', 'Dense4/bias',\n",
        "                    'Dense5/weight', 'Dense5/bias',\n",
        "                    'Dense6/weight', 'Dense6/bias',\n",
        "                    'Dense7/weight', 'Dense7/bias'\n",
        "                    ]\n",
        "\n",
        "  synthesis_weights = {\n",
        "    'ffhq' : synthesis_weights_1024,\n",
        "    'car' : synthesis_weights_512,\n",
        "    'cat' : synthesis_weights_256,\n",
        "    'horse' : synthesis_weights_256,\n",
        "    'church' : synthesis_weights_256,\n",
        "    'sg2-ada_2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664' : synthesis_weights_512,\n",
        "    'sg2_anime_network-snapshot-018528' : synthesis_weights_512,\n",
        "    'sg2-ext_aydao-anime-danbooru2019s-512-5268480' : synthesis_weights_1024,\n",
        "    'sg2-ada_abstract_network-snapshot-000188' : synthesis_weights_1024\n",
        "    }\n",
        "\n",
        "  discriminator_weights = {\n",
        "    'ffhq' : discriminator_weights_1024,\n",
        "    'car' : discriminator_weights_512,\n",
        "    'cat' : discriminator_weights_256,\n",
        "    'horse' : discriminator_weights_256,\n",
        "    'church' : discriminator_weights_256,\n",
        "    'sg2-ada_2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664' : discriminator_weights_512,\n",
        "    'sg2_anime_network-snapshot-018528' : discriminator_weights_512,\n",
        "    'sg2-ext_aydao-anime-danbooru2019s-512-5268480' : discriminator_weights_1024,\n",
        "    'sg2-ada_abstract_network-snapshot-000188' : discriminator_weights_1024\n",
        "    }\n",
        "\n",
        "  # instantiating generator network\n",
        "  generator = StyleGan2Generator(weights=weights_name, impl=impl, gpu=gpu)\n",
        "\n",
        "  # loading w average\n",
        "  #w_average = np.load('weights/{}_dlatent_avg.npy'.format(weights_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb6XC_M-AeoC"
      },
      "outputs": [],
      "source": [
        "if generate&gen_sg2_moono_tf2:\n",
        "  \n",
        "  import os\n",
        "  import numpy as np\n",
        "  import tensorflow as tf\n",
        "  from PIL import Image\n",
        "  from stylegan2.utils import postprocess_images\n",
        "  from load_models import load_generator\n",
        "  from copy_official_weights import convert_official_weights_together\n",
        "  \n",
        "  if True:\n",
        "    from tf_utils import allow_memory_growth\n",
        "\n",
        "    allow_memory_growth()\n",
        "\n",
        "    # common variables\n",
        "    ckpt_dir_base = './official-converted'\n",
        "\n",
        "    use_custom_cuda=True\n",
        "    # saving phase\n",
        "    #for use_custom_cuda in [True, False]:\n",
        "    #    ckpt_dir = os.path.join(ckpt_dir_base, 'cuda') if use_custom_cuda else os.path.join(ckpt_dir_base, 'ref')\n",
        "    #    convert_official_weights_together(ckpt_dir, use_custom_cuda)\n",
        "\n",
        "    # inference phase\n",
        "    ckpt_dir_cuda = os.path.join(ckpt_dir_base, 'cuda')\n",
        "    ckpt_dir_ref = os.path.join(ckpt_dir_base, 'ref')\n",
        "\n",
        "    g_clone = load_generator(g_params=None, is_g_clone=True, ckpt_dir=ckpt_dir_cuda, custom_cuda=use_custom_cuda)\n",
        "\n",
        "#if generate_stylegan2_tpu:\n",
        "#  tflib.init_tf()\n",
        "#  import pretrained_networks\n",
        "#  _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvoouJG1Pt-c"
      },
      "outputs": [],
      "source": [
        "if generate&gen_sg2_aydao_surgery_model_release:\n",
        "  import argparse\n",
        "  import numpy as np\n",
        "  import PIL.Image\n",
        "  import dnnlib\n",
        "  import dnnlib.tflib as tflib\n",
        "  import re\n",
        "  import sys\n",
        "\n",
        "  import pretrained_networks\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "  def generate_images(network_pkl, seeds, truncation_psi):\n",
        "    print('Loading networks from \"%s\"...' % network_pkl)\n",
        "    global _G, _D, Gs, Gs_kwargs\n",
        "    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "    noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n",
        "\n",
        "    Gs_kwargs = dnnlib.EasyDict()\n",
        "    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_kwargs.randomize_noise = False\n",
        "    if truncation_psi is not None:\n",
        "        Gs_kwargs.truncation_psi = truncation_psi\n",
        "\n",
        "    for seed_idx, seed in enumerate(seeds):\n",
        "        print('Generating image for seed %d (%d/%d) ...' % (seed, seed_idx, len(seeds)))\n",
        "        rnd = np.random.RandomState(seed)\n",
        "        z = rnd.randn(1, *Gs.input_shape[1:]) # [minibatch, component]\n",
        "        tflib.set_vars({var: rnd.randn(*var.shape.as_list()) for var in noise_vars}) # [height, width]\n",
        "        images = Gs.run(z, None, **Gs_kwargs) # [minibatch, height, width, channel]\n",
        "        PIL.Image.fromarray(images[0], 'RGB').save(dnnlib.make_run_dir_path('seed%04d.png' % seed))\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "  def style_mixing_example(network_pkl, row_seeds, col_seeds, truncation_psi, col_styles, minibatch_size=4):\n",
        "    print('Loading networks from \"%s\"...' % network_pkl)\n",
        "    global _G, _D, Gs, Gs_syn_kwargs\n",
        "    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "    w_avg = Gs.get_var('dlatent_avg') # [component]\n",
        "\n",
        "    Gs_syn_kwargs = dnnlib.EasyDict()\n",
        "    Gs_syn_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_syn_kwargs.randomize_noise = False\n",
        "    Gs_syn_kwargs.minibatch_size = minibatch_size\n",
        "\n",
        "    print('Generating W vectors...')\n",
        "    all_seeds = list(set(row_seeds + col_seeds))\n",
        "    all_z = np.stack([np.random.RandomState(seed).randn(*Gs.input_shape[1:]) for seed in all_seeds]) # [minibatch, component]\n",
        "    all_w = Gs.components.mapping.run(all_z, None) # [minibatch, layer, component]\n",
        "    all_w = w_avg + (all_w - w_avg) * truncation_psi # [minibatch, layer, component]\n",
        "    w_dict = {seed: w for seed, w in zip(all_seeds, list(all_w))} # [layer, component]\n",
        "\n",
        "    print('Generating images...')\n",
        "    all_images = Gs.components.synthesis.run(all_w, **Gs_syn_kwargs) # [minibatch, height, width, channel]\n",
        "    image_dict = {(seed, seed): image for seed, image in zip(all_seeds, list(all_images))}\n",
        "\n",
        "    print('Generating style-mixed images...')\n",
        "    for row_seed in row_seeds:\n",
        "        for col_seed in col_seeds:\n",
        "            w = w_dict[row_seed].copy()\n",
        "            w[col_styles] = w_dict[col_seed][col_styles]\n",
        "            image = Gs.components.synthesis.run(w[np.newaxis], **Gs_syn_kwargs)[0]\n",
        "            image_dict[(row_seed, col_seed)] = image\n",
        "\n",
        "    print('Saving images...')\n",
        "    for (row_seed, col_seed), image in image_dict.items():\n",
        "        PIL.Image.fromarray(image, 'RGB').save(dnnlib.make_run_dir_path('%d-%d.png' % (row_seed, col_seed)))\n",
        "\n",
        "    print('Saving image grid...')\n",
        "    _N, _C, H, W = Gs.output_shape\n",
        "    canvas = PIL.Image.new('RGB', (W * (len(col_seeds) + 1), H * (len(row_seeds) + 1)), 'black')\n",
        "    for row_idx, row_seed in enumerate([None] + row_seeds):\n",
        "        for col_idx, col_seed in enumerate([None] + col_seeds):\n",
        "            if row_seed is None and col_seed is None:\n",
        "                continue\n",
        "            key = (row_seed, col_seed)\n",
        "            if row_seed is None:\n",
        "                key = (col_seed, col_seed)\n",
        "            if col_seed is None:\n",
        "                key = (row_seed, row_seed)\n",
        "            canvas.paste(PIL.Image.fromarray(image_dict[key], 'RGB'), (W * col_idx, H * row_idx))\n",
        "    canvas.save(dnnlib.make_run_dir_path('grid.png'))\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "  def _parse_num_range(s):\n",
        "    '''Accept either a comma separated list of numbers 'a,b,c' or a range 'a-c' and return as a list of ints.'''\n",
        "\n",
        "    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n",
        "    m = range_re.match(s)\n",
        "    if m:\n",
        "        return range(int(m.group(1)), int(m.group(2))+1)\n",
        "    vals = s.split(',')\n",
        "    return [int(x) for x in vals]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2comFiMPa2H"
      },
      "outputs": [],
      "source": [
        "if generate&gen_sg2_aydao_surgery_model_release:\n",
        "  \n",
        "    #%env TF_XLA_FLAGS=\"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit\"\n",
        "    %env TF_XLA_FLAGS=\"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_clustering_debug\"\n",
        "    %env TF_DUMP_GRAPH_PREFIX=\"gs://train_with_tpu/generated\" \n",
        "    #%env TF_DUMP_GRAPH_PREFIX=\"/content/generated\" \n",
        "    #%env XLA_FLAGS=\"--xla_dump_to=/content/generated\"\n",
        "    %env XLA_FLAGS=\"--xla_dump_hlo_as_text --xla_dump_to=gs://train_with_tpu/generated\"\n",
        "    #%env XLA_FLAGS=\"--xla_dump_hlo_as_text --xla_dump_to=/content/generated\"\n",
        "\n",
        "    _G = None\n",
        "    _D = None\n",
        "    Gs = None\n",
        "    Gs_syn_kwargs = None\n",
        "    Gs_kwargs = None\n",
        "\n",
        "    class Namespace:\n",
        "      def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "    #from argparse import Namespace\n",
        "\n",
        "    network_pkl=files_path[0][1]\n",
        "    truncation_psi=0.5\n",
        "\n",
        "    args = Namespace(command='generate-images', network_pkl=network_pkl, \n",
        "                     result_dir='/content/result_dir', seeds=[66], \n",
        "                     truncation_psi=0.5)\n",
        "    kwargs = vars(args)\n",
        "    subcmd = kwargs.pop('command')\n",
        "\n",
        "    sc = dnnlib.SubmitConfig()\n",
        "    sc.num_gpus = 8\n",
        "#    sc.num_gpus = 1\n",
        "    sc.submit_target = dnnlib.SubmitTarget.LOCAL\n",
        "    sc.local.do_not_copy_source_files = True\n",
        "    sc.run_dir_root = kwargs.pop('result_dir')\n",
        "    sc.run_desc = subcmd\n",
        "\n",
        "    func_name_map = {\n",
        "        'generate-images': 'run_generator.generate_images',\n",
        "        'style-mixing-example': 'run_generator.style_mixing_example'\n",
        "    }\n",
        "    dnnlib.submit_run(sc, func_name_map[subcmd], **kwargs)\n",
        "\n",
        "    print('Loading networks from \"%s\"...' % network_pkl)\n",
        "\n",
        "    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "    noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n",
        "\n",
        "    Gs_kwargs = dnnlib.EasyDict()\n",
        "    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_kwargs.randomize_noise = False\n",
        "    if truncation_psi is not None:\n",
        "        Gs_kwargs.truncation_psi = truncation_psi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgmmI70rAiey"
      },
      "outputs": [],
      "source": [
        "if generate&gen_stylegan2:\n",
        " if generate&gen_tf1:\n",
        "  if generate&gen_gpu:\n",
        "   \n",
        "#if generate_stylegan2_ada or generate_stylegan2_ext:\n",
        "   import dnnlib\n",
        "   import dnnlib.tflib as tflib  \n",
        "   tflib.init_tf()\n",
        "  \n",
        "   import pickle\n",
        "   network_pkl=files_path[0][1]\n",
        "  #with dnnlib.util.open_url(network_pkl) as fp:\n",
        " #       _G, _D, Gs = pickle.load(fp)\n",
        "   _G, _D, Gs = pickle.load(open(network_pkl, \"rb\"))\n",
        "\n",
        "  if generate&gen_tf2_npy:\n",
        "    import numpy as np\n",
        "    data = {}\n",
        "\n",
        "#    import pretrained_networks\n",
        "#    g, d, Gs_network = pretrained_networks.load_networks('/content/model/2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664.pkl')\n",
        "#    for key in  d.trainables.keys():\n",
        "#        data['disc_'+ key] = d.get_var(key)\n",
        "    #print(_G)\n",
        "    #print(_D)\n",
        "    #print(Gs)\n",
        "    _G.print_layers()\n",
        "    _D.print_layers()\n",
        "    Gs.print_layers()\n",
        "\n",
        "    \n",
        "    for key in  _G.trainables.keys():\n",
        "        data[key[key.find('/')+1:]] = _G.get_var(key)\n",
        "    #for key in  Gs.trainables.keys():\n",
        "    #    data[key[key.find('/')+1:]] = Gs.get_var(key)\n",
        "    #for key in  _G.trainables.keys():\n",
        "    #    data[key] = _G.get_var(key)\n",
        "    #for key in  Gs.trainables.keys():\n",
        "    #    data['gens_'+ key] = Gs.get_var(key)\n",
        "\n",
        "    for key in  _D.trainables.keys():\n",
        "        data['disc_'+ key] = _D.get_var(key)\n",
        "    np.save('/content/model/{}.npy'.format(files_path[0][2]), data, allow_pickle=True)\n",
        "    #from google.colab import files\n",
        "    #files.download('/content/model/{}.npy'.format(files_path[0][2]))\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    !mkdir /content/gdrive/MyDrive/EEG-GAN-audio-video/models\n",
        "    !cp -r -v \"/content/model/{files_path[0][2]}.npy\" \"/content/gdrive/MyDrive/EEG-GAN-audio-video/models/{files_path[0][2]}.npy\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhYyUwxJ1oxu"
      },
      "outputs": [],
      "source": [
        "if generate&gen_sg2_nagolinc_pt:\n",
        "\n",
        "  import subprocess\n",
        "\n",
        "  CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "  print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "  if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "  elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "  elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "  else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "  !pip install ninja\n",
        "\n",
        "  !pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "\n",
        "  #%cd /content/stylegan2-pytorch\n",
        "  from convert_weight import convertStyleGan2\n",
        "\n",
        "#conver the model from tf to torch\n",
        "  ckpt, g, disc,g_train = convertStyleGan2(_G,_D,Gs)#,style_dim=dim_sg2,max_channel_size=dim_sg2)\n",
        "  latent_avg=ckpt[\"latent_avg\"]\n",
        "\n",
        "  import torch\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  def fmtImg(r):\n",
        "    img = ((r+1)/2*256).clip(0,255).astype(np.uint8).transpose(1,2,0)\n",
        "    return PIL.Image.fromarray(img, 'RGB')\n",
        "\n",
        "  device='cuda'\n",
        "  n_sample=1\n",
        "\n",
        "  g = g.to(device)\n",
        "\n",
        "  inputSize=1024#dim_sg2\n",
        "  import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrgaAwdJOkee"
      },
      "outputs": [],
      "source": [
        "if generate&gen_sg2_nvlabs_ada_pt:\n",
        "\n",
        "  !pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3\n",
        "  !pip install torch\n",
        "  #!pip install torch==1.7.1\n",
        "#  %pip install ninja\n",
        "#  import pickle\n",
        "  import copy\n",
        "  import os\n",
        "  #from time import perf_counter\n",
        "\n",
        "  #import click\n",
        "  import imageio\n",
        "  import numpy as np\n",
        "  import PIL.Image\n",
        "  import torch\n",
        "  import torch.nn.functional as F\n",
        "\n",
        "  %cd /content/stylegan2-nvlabs-ada-pytorch\n",
        "\n",
        "  import dnnlib\n",
        "  import legacy\n",
        "  network_pkl=files_path[0][1]\n",
        "  if len(files_path)>1:\n",
        "    network_pkl=files_path[1][1]\n",
        "\n",
        "  device = torch.device('cuda:0')\n",
        "#  device = torch.device('cuda')\n",
        "  with dnnlib.util.open_url(network_pkl) as fp:\n",
        "      G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device) # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUKP12wZTYT7"
      },
      "outputs": [],
      "source": [
        "#generate_and_plot_images_notrunc(generator, seed=396)\n",
        "\n",
        "# not using truncation\n",
        "#generate_and_plot_images(generator, seed=96, w_avg=w_average)\n",
        "\n",
        "# using truncation 0.5\n",
        "#generate_and_plot_images(generator, seed=96, w_avg=w_average, truncation_psi=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrZhTCe5USvq"
      },
      "outputs": [],
      "source": [
        "#def gen():\n",
        "#  global generator\n",
        "#  seed = 6600\n",
        "#  # creating random latent vector\n",
        "#  rnd = np.random.RandomState(seed)\n",
        "#  __z = rnd.randn(1, 512).astype('float32')\n",
        "#  # running mapping network\n",
        "#  dlatents = generator.mapping_network(__z)  \n",
        "# \n",
        "#  out = generator.synthesis_network(dlatents)\n",
        "#  #converting image/s to uint8\n",
        "#  images = convert_images_to_uint8(out, nchw_to_nhwc=True, uint8_cast=True)\n",
        "#gen()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aA95OcEv5YU"
      },
      "outputs": [],
      "source": [
        "if generate&gen_wavegan:\n",
        " if generate&gen_drums:\n",
        "  # Load the model\n",
        "  tf.reset_default_graph()\n",
        "  saver = tf.train.import_meta_graph('/content/model/infer/infer.meta')\n",
        "  graph = tf.get_default_graph()\n",
        "  sess = tf.InteractiveSession()\n",
        "  sess.close()\n",
        "  sess = tf.InteractiveSession()\n",
        "  saver.restore(sess, f'/content/model/model.ckpt-18637')\n",
        "  #dim = 100\n",
        "  break_len = 65536\n",
        "\n",
        "  z = graph.get_tensor_by_name('z:0')\n",
        "  G_z = graph.get_tensor_by_name('G_z:0')\n",
        "\n",
        "  import numpy as np\n",
        "  from IPython.display import display, Audio\n",
        "  #from google.colab import files\n",
        "  import scipy.io.wavfile\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "  !mkdir \"./neuralfunk examples\"\n",
        "\n",
        "  def generate_trajectory(n_iter, _z0=None, mov_last=None, jump=0.3, smooth=0.3, include_z0=True):\n",
        "    _z = np.empty((n_iter + int(not include_z0), dim))\n",
        "    _z[0] = _z0 if _z0 is not None else np.random.random(dim)*2-1\n",
        "    mov = mov_last if mov_last is not None else (np.random.random(dim)*2-1)*jump\n",
        "    for i in range(1, len(_z)):\n",
        "        mov = mov*smooth + (np.random.random(dim)*2-1)*jump*(1-smooth)\n",
        "        mov -= (np.abs(_z[i-1] + mov) > 1) * 2 * mov\n",
        "        _z[i] = _z[i-1] + mov\n",
        "    return _z[-n_iter:], mov  \n",
        "  !pip install pydub\n",
        "  from pydub import AudioSegment\n",
        "  !pip install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lt3tZX8iYM1",
        "outputId": "6aeee713-acb7-404a-f94d-d3c46f7d4f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mne==0.23.3\n",
            "  Downloading mne-0.23.3-py3-none-any.whl (6.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne==0.23.3) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne==0.23.3) (1.21.6)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-0.23.3\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "if generate&gen_gpu_cuda:\n",
        "  !curl https://colab.chainer.org/install | sh -\n",
        "  import chainer\n",
        "  chainer.print_runtime_info()\n",
        "  %env MNE_USE_CUDA=true\n",
        "  \n",
        "!pip install mne==0.23.3\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "\n",
        "import mne\n",
        "from mne import io\n",
        "from mne.datasets import sample\n",
        "from mne.minimum_norm import read_inverse_operator, compute_source_psd\n",
        "\n",
        "from mne.connectivity import spectral_connectivity, seed_target_indices\n",
        "if generate&gen_gpu_cuda:\n",
        "  mne.cuda.init_cuda(verbose=True)\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aHDP6GOVeVP"
      },
      "outputs": [],
      "source": [
        "if generate&gen_stylegan2:\n",
        "  !pip install ffmpeg-python\n",
        "  import ffmpeg\n",
        "  import scipy\n",
        "  import moviepy.editor\n",
        "  !pip install av\n",
        "  import av\n",
        "  from IPython.utils import io\n",
        "  !mkdir '/content/out'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWAgsxLebf3T"
      },
      "outputs": [],
      "source": [
        "if generate&gen_game:\n",
        "  import io\n",
        "  from PIL import Image as pilimage\n",
        "  import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "id": "_lsqZe1gzdbl",
        "outputId": "7b70544d-754a-4806-ffdc-7e00375bfe64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n",
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2809856/45929032 bytes (6.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6242304/45929032 bytes (13.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b9650176/45929032 bytes (21.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11837440/45929032 bytes (25.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15335424/45929032 bytes (33.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18735104/45929032 bytes (40.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22200320/45929032 bytes (48.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25616384/45929032 bytes (55.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28975104/45929032 bytes (63.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32112640/45929032 bytes (69.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35504128/45929032 bytes (77.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39034880/45929032 bytes (85.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42418176/45929032 bytes (92.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n",
            "Collecting av\n",
            "  Downloading av-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.2 MB 17 kB/s \n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-9.2.0\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6084 sha256=698011397d2eb00edc73b9829736854d70b88fa04b6756359964d90a3e323941\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/80/6e/caa3e16deb0267c3cbfd36862058a724144e19fdb9eb03af0f\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ffmpeg"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pysndfile in /usr/local/lib/python3.7/dist-packages (1.3.8)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from pysndfile) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "if generate&gen_silent_speech:\n",
        "  !pip install ffmpeg-python\n",
        "  import ffmpeg\n",
        "  import scipy\n",
        "  import moviepy.editor\n",
        "  !pip install av\n",
        "  import av\n",
        "  from IPython.utils import io\n",
        "  import numpy as np\n",
        "  from IPython.display import display, Audio\n",
        "  import scipy.io.wavfile\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "  !pip install pydub\n",
        "  from pydub import AudioSegment\n",
        "  !pip install ffmpeg\n",
        "\n",
        "  !pip install pysndfile\n",
        "  !pip install absl-py librosa soundfile matplotlib scipy numba jiwer unidecode deepspeech==0.8.2 praat-textgrids\n",
        "#  !unzip -o emg_data.zip\n",
        "  !unzip -n emg_data.zip\n",
        "  if generate&gen_ss_wm50_tm07_dm070:\n",
        "#    !ln -s /content/emg_data ./emg_data\n",
        "#    !ln -s /content/text_alignments ./text_alignments\n",
        "#    %cd /content/silent_speech\n",
        "    %cd /content/silent_speech-dgaddy-pytorch/\n",
        "\n",
        "#    !python evaluate.py --models ./models/transduction_model/model_07.pt --pretrained_wavenet_model ./models/wavenet_model/wavenet_model_50.pt --output_directory evaluation_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0IYRVtf9I1v"
      },
      "outputs": [],
      "source": [
        "if generate&gen_silent_speech:\n",
        " if False:  \n",
        "  !pip install brainflow time\n",
        "  import time\n",
        "  import brainflow\n",
        "  from brainflow.board_shim import BoardShim, BrainFlowInputParams, BoardIds\n",
        "\n",
        "  import mne\n",
        "  from mne.channels import read_layout\n",
        "\n",
        "  # use synthetic board for demo\n",
        "  params = BrainFlowInputParams()\n",
        "  board = BoardShim(BoardIds.SYNTHETIC_BOARD.value, params)\n",
        "  board.release_all_sessions()\n",
        "  board.prepare_session()\n",
        "  board.start_stream()\n",
        "  time.sleep(10)\n",
        "  data = board.get_board_data()\n",
        "  board.stop_stream()\n",
        "  board.release_session()\n",
        "\n",
        "  eeg_channels = BoardShim.get_eeg_channels(BoardIds.SYNTHETIC_BOARD.value)\n",
        "  eeg_data = data[eeg_channels, :]\n",
        "  eeg_data = eeg_data / 1000000 # BrainFlow returns uV, convert to V for MNE\n",
        "\n",
        "  # Creating MNE objects from brainflow data arrays\n",
        "  ch_types = ['eeg'] * len(eeg_channels)\n",
        "  ch_names = BoardShim.get_eeg_names(BoardIds.SYNTHETIC_BOARD.value)\n",
        "  sfreq = BoardShim.get_sampling_rate(BoardIds.SYNTHETIC_BOARD.value)\n",
        "  info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
        "  raw = mne.io.RawArray(eeg_data, info)\n",
        "  # its time to plot something!\n",
        "  raw.plot_psd(average=False)  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hkldZZIdFGq"
      },
      "outputs": [],
      "source": [
        "if generate&gen_silent_speech:\n",
        "\n",
        "  !pip install brainflow\n",
        "\n",
        "  !pip install mne==0.23.3\n",
        "  #!pip install pandas\n",
        "  #!pip install matplotlib\n",
        "\n",
        "  #!pip install brainflow time pyserial\n",
        "  !pip install pyserial\n",
        "  #import os, pty, serial\n",
        "\n",
        "  !apt install -y socat\n",
        "\n",
        "  generate = generate | gen_stylegan2\n",
        "  generate = generate | gen_wavegan\n",
        "\n",
        "  !pip install sounddevice\n",
        "  !apt-get install libasound-dev libportaudio2 -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9inLq7xc0yO"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngXGUGvQWYST"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nltk\n",
        "\n",
        "class Book(object):\n",
        "    def __init__(self, book_file):\n",
        "        self.file = book_file\n",
        "\n",
        "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "        with open(book_file) as f:\n",
        "            all_text = f.read()\n",
        "        paragraphs = all_text.split('\\n\\n')\n",
        "        sentences = [s for p in paragraphs for s in sent_detector.tokenize(p.strip())]\n",
        "        self.sentences = [s.replace('\\n', ' ') for s in sentences]\n",
        "\n",
        "        bookmark_file = self.file + '.bookmark'\n",
        "        if os.path.exists(bookmark_file):\n",
        "            with open(bookmark_file) as f:\n",
        "                self.current_index = int(f.read().strip())\n",
        "        else:\n",
        "            self.current_index = 0\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        bookmark_file = self.file + '.bookmark'\n",
        "        with open(bookmark_file, 'w') as f:\n",
        "            f.write(str(self.current_index))\n",
        "\n",
        "    def current_sentence(self):\n",
        "        return self.sentences[self.current_index]\n",
        "\n",
        "    def next(self):\n",
        "        self.current_index = (self.current_index+1) % len(self.sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvwXo6I9fkYw"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "if True:\n",
        "  subprocess.Popen([\"socat\", \"PTY,link=/dev/ttyS10\", \"PTY,link=/dev/ttyS11\"]) \n",
        "\n",
        "#!pip install brainflow\n",
        "import time\n",
        "import brainflow\n",
        "from brainflow.board_shim import BoardShim, BrainFlowInputParams, BoardIds\n",
        "\n",
        "#!pip install mne==0.23.3\n",
        "#!pip install pandas\n",
        "#!pip install matplotlib\n",
        "import mne\n",
        "from mne.channels import read_layout\n",
        "\n",
        "#!pip install brainflow time pyserial\n",
        "#!pip install pyserial\n",
        "import os, pty, serial\n",
        "\n",
        "\n",
        "from IPython.display import Javascript\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from IPython.display import Javascript\n",
        "import json\n",
        " \n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "from time import perf_counter\n",
        "\n",
        "\n",
        "#master, slave = pty.openpty()\n",
        "#s_name = os.ttyname(slave)\n",
        "\n",
        "#ser = serial.Serial(s_name)\n",
        "\n",
        "if True:\n",
        "  \n",
        "  ser = serial.Serial('/dev/ttyS10')\n",
        "\n",
        "  # use synthetic board for demo\n",
        "  params = BrainFlowInputParams()\n",
        "  params.serial_port = '/dev/ttyS11'\n",
        "  #params.serial_port = os.ttyname(slave)\n",
        "  sample_rate = 512\n",
        "  board = BoardShim(BoardIds.FREEEEG32_BOARD.value, params)\n",
        "  board.release_all_sessions()\n",
        "  board.prepare_session()\n",
        "  board.start_stream()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFdB1PnioRrs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvvp4qtQa6sc"
      },
      "outputs": [],
      "source": [
        "%cd /content/silent_speech-dgaddy-pytorch\n",
        "!ln -s /content/silent_speech-dgaddy-pytorch/nv_wavenet/pytorch/nv_wavenet_ext.egg-info ./nv_wavenet_ext.egg-info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42E-tvnsbxxS"
      },
      "outputs": [],
      "source": [
        "%cd /content/silent_speech-dgaddy-pytorch\n",
        "\n",
        "from nv_wavenet.pytorch.wavenet import WaveNet\n",
        "%cd /content/silent_speech-dgaddy-pytorch/nv_wavenet/pytorch\n",
        "import nv_wavenet\n",
        "%cd /content/silent_speech-dgaddy-pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxoBl335nsf4"
      },
      "outputs": [],
      "source": [
        "%cd /content/silent_speech-dgaddy-pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvWf5XzL9Mpk"
      },
      "outputs": [],
      "source": [
        "!pip show nv_wavenet_ext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "egg_path='/usr/local/lib/python3.7/dist-packages/nv_wavenet_ext-0.0.0-py3.7-linux-x86_64.egg'\n",
        "\n",
        "import sys\n",
        "sys.path.append(egg_path)\n",
        "import nv_wavenet_ext"
      ],
      "metadata": {
        "id": "ug3WLsdQMUpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NrbHq2lcd0H"
      },
      "outputs": [],
      "source": [
        "from absl import flags\n",
        "FLAGS = flags.FLAGS\n",
        "for name in list(flags.FLAGS):\n",
        "  delattr(flags.FLAGS, name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnL-_K0igPft"
      },
      "outputs": [],
      "source": [
        "from absl import flags\n",
        "FLAGS = flags.FLAGS\n",
        "for name in list(flags.FLAGS):\n",
        "  delattr(flags.FLAGS, name)\n",
        "  \n",
        "#data_utils.py\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from absl import flags\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_boolean('mel_spectrogram', False, 'use mel spectrogram features instead of mfccs for audio')\n",
        "flags.DEFINE_string('normalizers_file', 'normalizers.pkl', 'file with pickled feature normalizers')\n",
        "\n",
        "phoneme_inventory = ['aa','ae','ah','ao','aw','ax','axr','ay','b','ch','d','dh','dx','eh','el','em','en','er','ey','f','g','hh','hv','ih','iy','jh','k','l','m','n','nx','ng','ow','oy','p','r','s','sh','t','th','uh','uw','v','w','y','z','zh','sil']\n",
        "\n",
        "def normalize_volume(audio):\n",
        "    rms = librosa.feature.rms(audio)\n",
        "    max_rms = rms.max() + 0.01\n",
        "    target_rms = 0.2\n",
        "    audio = audio * (target_rms/max_rms)\n",
        "    max_val = np.abs(audio).max()\n",
        "    if max_val > 1.0: # this shouldn't happen too often with the target_rms of 0.2\n",
        "        audio = audio / max_val\n",
        "    return audio\n",
        "\n",
        "def load_audio(filename, start=None, end=None, max_frames=None, renormalize_volume=False):\n",
        "    audio, r = sf.read(filename)\n",
        "    assert r == 16000\n",
        "\n",
        "    if len(audio.shape) > 1:\n",
        "        audio = audio[:,0] # select first channel of stero audio\n",
        "    if start is not None or end is not None:\n",
        "        audio = audio[start:end]\n",
        "\n",
        "    if renormalize_volume:\n",
        "        audio = normalize_volume(audio)\n",
        "    if FLAGS.mel_spectrogram:\n",
        "        mfccs = librosa.feature.melspectrogram(audio, sr=16000, n_mels=128, center=False, n_fft=512, win_length=432, hop_length=160).T\n",
        "        mfccs = np.log(mfccs+1e-5)\n",
        "    else:\n",
        "        mfccs = librosa.feature.mfcc(audio, sr=16000, n_mfcc=26, n_fft=512, win_length=432, hop_length=160, center=False).T\n",
        "    audio_discrete = librosa.core.mu_compress(audio, mu=255, quantize=True)+128\n",
        "    if max_frames is not None and mfccs.shape[0] > max_frames:\n",
        "        mfccs = mfccs[:max_frames,:]\n",
        "    audio_length = 160*mfccs.shape[0]+(432-160)\n",
        "    audio_discrete = audio_discrete[:audio_length] # cut off audio to match framed length\n",
        "    return mfccs.astype(np.float32), audio_discrete\n",
        "\n",
        "def double_average(x):\n",
        "    assert len(x.shape) == 1\n",
        "    f = np.ones(9)/9.0\n",
        "    v = np.convolve(x, f, mode='same')\n",
        "    w = np.convolve(v, f, mode='same')\n",
        "    return w\n",
        "\n",
        "def get_emg_features(emg_data, debug=False):\n",
        "    xs = emg_data - emg_data.mean(axis=0, keepdims=True)\n",
        "    frame_features = []\n",
        "    for i in range(emg_data.shape[1]):\n",
        "        x = xs[:,i]\n",
        "        w = double_average(x)\n",
        "        p = x - w\n",
        "        r = np.abs(p)\n",
        "\n",
        "        w_h = librosa.util.frame(w, frame_length=16, hop_length=6).mean(axis=0)\n",
        "        p_w = librosa.feature.rms(w, frame_length=16, hop_length=6, center=False)\n",
        "        p_w = np.squeeze(p_w, 0)\n",
        "        p_r = librosa.feature.rms(r, frame_length=16, hop_length=6, center=False)\n",
        "        p_r = np.squeeze(p_r, 0)\n",
        "        z_p = librosa.feature.zero_crossing_rate(p, frame_length=16, hop_length=6, center=False)\n",
        "        z_p = np.squeeze(z_p, 0)\n",
        "        r_h = librosa.util.frame(r, frame_length=16, hop_length=6).mean(axis=0)\n",
        "\n",
        "        s = abs(librosa.stft(np.ascontiguousarray(x), n_fft=16, hop_length=6, center=False))\n",
        "        # s has feature dimension first and time second\n",
        "\n",
        "        if debug:\n",
        "            plt.subplot(7,1,1)\n",
        "            plt.plot(x)\n",
        "            plt.subplot(7,1,2)\n",
        "            plt.plot(w_h)\n",
        "            plt.subplot(7,1,3)\n",
        "            plt.plot(p_w)\n",
        "            plt.subplot(7,1,4)\n",
        "            plt.plot(p_r)\n",
        "            plt.subplot(7,1,5)\n",
        "            plt.plot(z_p)\n",
        "            plt.subplot(7,1,6)\n",
        "            plt.plot(r_h)\n",
        "\n",
        "            plt.subplot(7,1,7)\n",
        "            plt.imshow(s, origin='lower', aspect='auto', interpolation='nearest')\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "        frame_features.append(np.stack([w_h, p_w, p_r, z_p, r_h], axis=1))\n",
        "        frame_features.append(s.T)\n",
        "\n",
        "    frame_features = np.concatenate(frame_features, axis=1)\n",
        "    return frame_features.astype(np.float32)\n",
        "\n",
        "class FeatureNormalizer(object):\n",
        "    def __init__(self, feature_samples, share_scale=False):\n",
        "        \"\"\" features_samples should be list of 2d matrices with dimension (time, feature) \"\"\"\n",
        "        feature_samples = np.concatenate(feature_samples, axis=0)\n",
        "        self.feature_means = feature_samples.mean(axis=0, keepdims=True)\n",
        "        if share_scale:\n",
        "            self.feature_stddevs = feature_samples.std()\n",
        "        else:\n",
        "            self.feature_stddevs = feature_samples.std(axis=0, keepdims=True)\n",
        "\n",
        "    def normalize(self, sample):\n",
        "        sample -= self.feature_means\n",
        "        sample /= self.feature_stddevs\n",
        "        return sample\n",
        "\n",
        "    def inverse(self, sample):\n",
        "        sample = sample * self.feature_stddevs\n",
        "        sample = sample + self.feature_means\n",
        "        return sample\n",
        "\n",
        "def combine_fixed_length(tensor_list, length):\n",
        "    total_length = sum(t.size(0) for t in tensor_list)\n",
        "    if total_length % length != 0:\n",
        "        pad_length = length - (total_length % length)\n",
        "        tensor_list = list(tensor_list) # copy\n",
        "        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype))\n",
        "        total_length += pad_length\n",
        "    tensor = torch.cat(tensor_list, 0)\n",
        "    n = total_length // length\n",
        "    return tensor.view(n, length, *tensor.size()[1:])\n",
        "\n",
        "def decollate_tensor(tensor, lengths):\n",
        "    b, s, d = tensor.size()\n",
        "    tensor = tensor.view(b*s, d)\n",
        "    results = []\n",
        "    idx = 0\n",
        "    for length in lengths:\n",
        "        assert idx + length <= b * s\n",
        "        results.append(tensor[idx:idx+length])\n",
        "        idx += length\n",
        "    return results\n",
        "\n",
        "def splice_audio(chunks, overlap):\n",
        "    chunks = [c.copy() for c in chunks] # copy so we can modify in place\n",
        "\n",
        "    assert np.all([c.shape[0]>=overlap for c in chunks])\n",
        "\n",
        "    result_len = sum(c.shape[0] for c in chunks) - overlap*(len(chunks)-1)\n",
        "    result = np.zeros(result_len, dtype=chunks[0].dtype)\n",
        "\n",
        "    ramp_up = np.linspace(0,1,overlap)\n",
        "    ramp_down = np.linspace(1,0,overlap)\n",
        "\n",
        "    i = 0\n",
        "    for chunk in chunks:\n",
        "        l = chunk.shape[0]\n",
        "\n",
        "        # note: this will also fade the beginning and end of the result\n",
        "        chunk[:overlap] *= ramp_up\n",
        "        chunk[-overlap:] *= ramp_down\n",
        "\n",
        "        result[i:i+l] += chunk\n",
        "        i += l-overlap\n",
        "\n",
        "    return result\n",
        "\n",
        "def print_confusion(confusion_mat, n=10):\n",
        "    # axes are (pred, target)\n",
        "    target_counts = confusion_mat.sum(0) + 1e-4\n",
        "    aslist = []\n",
        "    for p1 in range(len(phoneme_inventory)):\n",
        "        for p2 in range(p1):\n",
        "            if p1 != p2:\n",
        "                aslist.append(((confusion_mat[p1,p2]+confusion_mat[p2,p1])/(target_counts[p1]+target_counts[p2]), p1, p2))\n",
        "    aslist.sort()\n",
        "    aslist = aslist[-n:]\n",
        "    max_val = aslist[-1][0]\n",
        "    min_val = aslist[0][0]\n",
        "    val_range = max_val - min_val\n",
        "    print('Common confusions (confusion, accuracy)')\n",
        "    for v, p1, p2 in aslist:\n",
        "        p1s = phoneme_inventory[p1]\n",
        "        p2s = phoneme_inventory[p2]\n",
        "        print(f'{p1s} {p2s} {v*100:.1f} {(confusion_mat[p1,p1]+confusion_mat[p2,p2])/(target_counts[p1]+target_counts[p2])*100:.1f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nxl_cApsqj0J"
      },
      "outputs": [],
      "source": [
        "#nv_wavenet.py\n",
        "\n",
        "# *****************************************************************************\n",
        "#  Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "# \n",
        "#  Redistribution and use in source and binary forms, with or without\n",
        "#  modification, are permitted provided that the following conditions are met:\n",
        "#      * Redistributions of source code must retain the above copyright\n",
        "#        notice, this list of conditions and the following disclaimer.\n",
        "#      * Redistributions in binary form must reproduce the above copyright\n",
        "#        notice, this list of conditions and the following disclaimer in the\n",
        "#        documentation and/or other materials provided with the distribution.\n",
        "#      * Neither the name of the NVIDIA CORPORATION nor the\n",
        "#        names of its contributors may be used to endorse or promote products\n",
        "#        derived from this software without specific prior written permission.\n",
        "# \n",
        "#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
        "#  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
        "#  WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "#  DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n",
        "#  DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n",
        "#  (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n",
        "#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n",
        "#  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        "#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n",
        "#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "# \n",
        "# *****************************************************************************\n",
        "import torch\n",
        "import nv_wavenet_ext\n",
        "\n",
        "def interleave_lists(a, b, c, d, e, f, g):\n",
        "    return [x for t in zip(a, b, c, d, e, f, g) for x in t] \n",
        "\n",
        "def column_major(x):\n",
        "    \"\"\"\n",
        "    PyTorch Tensors are row major, so this just returns a contiguous transpose\n",
        "    \"\"\"\n",
        "    assert(x.is_contiguous)\n",
        "    if len(x.size()) == 1:\n",
        "        return x\n",
        "    \n",
        "    if len(x.size()) == 3:\n",
        "        assert(x.size(2)==1)\n",
        "        x = torch.squeeze(x)\n",
        "    \n",
        "    if len(x.size())==2:\n",
        "        return torch.t(x).contiguous()\n",
        "    \n",
        "    if len(x.size())==4:\n",
        "        return x.permute(3,2,1,0).contiguous()\n",
        "\n",
        "def enum(**enums):\n",
        "    return type('Enum', (), enums)\n",
        "Impl = enum(AUTO=0, SINGLE_BLOCK=1, DUAL_BLOCK=2, PERSISTENT=3)\n",
        "\n",
        "class NVWaveNet:\n",
        "    def __init__(self, embedding_prev,\n",
        "                       embedding_curr,\n",
        "                       conv_out_weight,\n",
        "                       conv_end_weight,\n",
        "                       dilate_weights,\n",
        "                       dilate_biases,\n",
        "                       max_dilation,\n",
        "                       res_weights,\n",
        "                       res_biases,\n",
        "                       skip_weights,\n",
        "                       skip_biases,\n",
        "                       use_embed_tanh):\n",
        "        self.R = nv_wavenet_ext.num_res_channels()\n",
        "        self.S = nv_wavenet_ext.num_skip_channels() \n",
        "        self.A = nv_wavenet_ext.num_out_channels()\n",
        "\n",
        "        self.max_dilation = max_dilation\n",
        "        self.use_embed_tanh = use_embed_tanh\n",
        "        assert embedding_prev.size() == (self.A, self.R), \\\n",
        "                (\"embedding_prev: {} doesn't match compiled\" \n",
        "                \" nv-wavenet size: {}\").format(embedding_prev.size(),\n",
        "                                               (self.A, self.R))\n",
        "        self.embedding_prev = column_major(torch.t(embedding_prev))\n",
        "        \n",
        "        assert embedding_curr.size() == (self.A, self.R), \\\n",
        "                (\"embedding_curr: {} doesn't match compiled\" \n",
        "                \" nv-wavenet size: {}\").format(embedding_curr.size(),\n",
        "                                               (self.A, self.R))\n",
        "        self.embedding_curr = column_major(torch.t(embedding_curr))\n",
        "        \n",
        "        assert conv_out_weight.size()[:2] == (self.A, self.S), \\\n",
        "                (\"conv_out_weight: {} doesn't match compiled\" \n",
        "                 \" nv-wavenet size: {}\").format(conv_out_weight.size()[:2],\n",
        "                                               (self.A, self.S))\n",
        "        self.conv_out = column_major(conv_out_weight)\n",
        "        \n",
        "        assert conv_end_weight.size()[:2] == (self.A, self.A), \\\n",
        "                (\"conv_end_weight: {} doesn't match compiled\" \n",
        "                 \" nv-wavenet size: {}\").format(conv_end_weight.size()[:2],\n",
        "                                               (self.A, self.A))\n",
        "        self.conv_end = column_major(conv_end_weight)\n",
        "        \n",
        "        dilate_weights_prev = [] \n",
        "        dilate_weights_curr = [] \n",
        "        for weight in dilate_weights:\n",
        "            assert weight.size(2) == 2, \\\n",
        "                    \"nv-wavenet only supports kernel_size 2\"\n",
        "            assert weight.size()[:2] == (2*self.R, self.R), \\\n",
        "                    (\"dilated weight: {} doesn't match compiled\" \n",
        "                     \" nv-wavenet size: {}\").format(weight.size()[:2],\n",
        "                                                   (2*self.R, self.R))\n",
        "            Wprev = column_major(weight[:,:,0])\n",
        "            Wcurr = column_major(weight[:,:,1])\n",
        "            dilate_weights_prev.append(Wprev) \n",
        "            dilate_weights_curr.append(Wcurr)\n",
        "        \n",
        "        for bias in dilate_biases:\n",
        "            assert(bias.size(0) == 2*self.R)\n",
        "        for weight in res_weights:\n",
        "            assert weight.size()[:2] == (self.R, self.R), \\\n",
        "                    (\"residual weight: {} doesn't match compiled\" \n",
        "                     \" nv-wavenet size: {}\").format(weight.size()[:2],\n",
        "                                                   (self.R, self.R))\n",
        "        for bias in res_biases:\n",
        "            assert(bias.size(0) == self.R), \\\n",
        "                    (\"residual bias: {} doesn't match compiled\" \n",
        "                     \" nv-wavenet size: {}\").format(bias.size(0), self.R)\n",
        "        for weight in skip_weights:\n",
        "            assert weight.size()[:2] == (self.S, self.R), \\\n",
        "                    (\"skip weight: {} doesn't match compiled\" \n",
        "                     \" nv-wavenet size: {}\").format(weight.size()[:2],\n",
        "                                                   (self.S, self.R))\n",
        "        for bias in skip_biases:\n",
        "            assert(bias.size(0) == self.S), \\\n",
        "                    (\"skip bias: {} doesn't match compiled\" \n",
        "                     \" nv-wavenet size: {}\").format(bias.size(0), self.S)\n",
        "        \n",
        "        dilate_biases = [column_major(bias) for bias in dilate_biases]\n",
        "        res_weights = [column_major(weight) for weight in res_weights]\n",
        "        res_biases = [column_major(bias) for bias in res_biases]\n",
        "        skip_weights = [column_major(weight) for weight in skip_weights]\n",
        "        skip_biases = [column_major(bias) for bias in skip_biases]\n",
        "\n",
        "        # There's an extra residual layer that's not used\n",
        "        res_weights.append(torch.zeros(self.R,self.R))\n",
        "        res_biases.append(torch.zeros(self.R))\n",
        "\n",
        "        assert(len(res_biases)==len(skip_biases) and \n",
        "        len(res_biases)==len(dilate_biases) and \n",
        "        len(res_weights)==len(skip_weights) and \n",
        "        len(res_weights)==len(dilate_weights)), \\\n",
        "        \"\"\"Number of layers is inconsistent for different parameter types.\n",
        "        The list sizes should be the same for skip weights/biases and \n",
        "        dilate weights/biases.  Additionally the residual weights/biases\n",
        "        lists should be one shorter.  But their sizes are:\n",
        "        len(dilate_weights) = {}\n",
        "        len(dilale_biases) = {}\n",
        "        len(skip_weights) = {}\n",
        "        len(skip_biases) = {}\n",
        "        len(res_weights) = {}\n",
        "        len(res_biases) = {}\"\"\".format(len(dilate_weights),\n",
        "                                       len(dilate_biases),\n",
        "                                       len(skip_weights),\n",
        "                                       len(skip_biases),\n",
        "                                       len(res_weights)-1,\n",
        "                                       len(res_biases)-1)\n",
        "\n",
        "        self.num_layers = len(res_biases)\n",
        "        self.layers = interleave_lists(dilate_weights_prev,\n",
        "                                       dilate_weights_curr,\n",
        "                                       dilate_biases,\n",
        "                                       res_weights,\n",
        "                                       res_biases,\n",
        "                                       skip_weights,\n",
        "                                       skip_biases)\n",
        "\n",
        "    def infer(self, cond_input, implementation):\n",
        "        # cond_input is channels x batch x num_layers x samples\n",
        "        assert(cond_input.size()[0:3:2] == (2*self.R, self.num_layers)), \\\n",
        "        \"\"\"Inputs are channels x batch x num_layers x samples.\n",
        "        Channels and num_layers should be sizes: {}\n",
        "        But input is: {}\"\"\".format((2*self.R, self.num_layers),\n",
        "                                    cond_input.size()[0:3:2])\n",
        "        batch_size = cond_input.size(1)\n",
        "        sample_count = cond_input.size(3)\n",
        "        cond_input = column_major(cond_input)\n",
        "        samples = torch.cuda.IntTensor(batch_size, sample_count)\n",
        "        nv_wavenet_ext.infer(samples,\n",
        "                             sample_count,\n",
        "                             batch_size,\n",
        "                             self.embedding_prev,\n",
        "                             self.embedding_curr,\n",
        "                             self.conv_out,\n",
        "                             self.conv_end,\n",
        "                             cond_input,\n",
        "                             self.num_layers,\n",
        "                             self.use_embed_tanh,\n",
        "                             self.max_dilation,\n",
        "                             implementation,\n",
        "                             self.layers)\n",
        "        return samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm4yPyA5uFr6"
      },
      "outputs": [],
      "source": [
        "from nv_wavenet.pytorch import nv_wavenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_vP8gL3f_54"
      },
      "outputs": [],
      "source": [
        "from absl import flags\n",
        "FLAGS = flags.FLAGS\n",
        "for name in list(flags.FLAGS):\n",
        "  delattr(flags.FLAGS, name)\n",
        "\n",
        "#read_emg.py\n",
        "\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import scipy\n",
        "import json\n",
        "import copy\n",
        "import sys\n",
        "import pickle\n",
        "import string\n",
        "import logging\n",
        "\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from textgrids import TextGrid\n",
        "\n",
        "import torch\n",
        "\n",
        "#from data_utils import load_audio, get_emg_features, FeatureNormalizer, combine_fixed_length, phoneme_inventory\n",
        "\n",
        "from absl import flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "flags.DEFINE_boolean('mel_spectrogram', False, 'use mel spectrogram features instead of mfccs for audio')\n",
        "flags.DEFINE_string('normalizers_file', 'normalizers.pkl', 'file with pickled feature normalizers')\n",
        "\n",
        "flags.DEFINE_list('remove_channels', [], 'channels to remove')\n",
        "#flags.DEFINE_list('silent_data_directories', ['./emg_data/silent_parallel_data'], 'silent data locations')\n",
        "#flags.DEFINE_list('voiced_data_directories', ['./emg_data/voiced_parallel_data','./emg_data/nonparallel_data'], 'voiced data locations')\n",
        "#flags.DEFINE_string('testset_file', 'testset_largedev.json', 'file with testset indices')\n",
        "flags.DEFINE_list('silent_data_directories', ['./out'], 'silent data locations')\n",
        "flags.DEFINE_list('voiced_data_directories', ['./out','./out'], 'voiced data locations')\n",
        "flags.DEFINE_string('testset_file', 'testset_onlinedev.json', 'file with testset indices')\n",
        "flags.DEFINE_string('text_align_directory', 'text_alignments', 'directory with alignment files')\n",
        "\n",
        "def remove_drift(signal, fs):\n",
        "    b, a = scipy.signal.butter(3, 2, 'highpass', fs=fs)\n",
        "    return scipy.signal.filtfilt(b, a, signal)\n",
        "\n",
        "def notch(signal, freq, sample_frequency):\n",
        "    b, a = scipy.signal.iirnotch(freq, 30, sample_frequency)\n",
        "    return scipy.signal.filtfilt(b, a, signal)\n",
        "\n",
        "def notch_harmonics(signal, freq, sample_frequency):\n",
        "    max_harmonic=(sample_frequency//freq)//2\n",
        "    for harmonic in range(1,max_harmonic):\n",
        "#    for harmonic in range(1,8):\n",
        "        signal = notch(signal, freq*harmonic, sample_frequency)\n",
        "    return signal\n",
        "\n",
        "def subsample(signal, new_freq, old_freq):\n",
        "    times = np.arange(len(signal))/old_freq\n",
        "    sample_times = np.arange(0, times[-1], 1/new_freq)\n",
        "    result = np.interp(sample_times, times, signal)\n",
        "    return result\n",
        "\n",
        "def apply_to_all(function, signal_array, *args, **kwargs):\n",
        "    results = []\n",
        "    for i in range(signal_array.shape[1]):\n",
        "        results.append(function(signal_array[:,i], *args, **kwargs))\n",
        "    return np.stack(results, 1)\n",
        "\n",
        "def load_utterance(base_dir, index, limit_length=False, debug=False, text_align_directory=None):\n",
        "    index = int(index)\n",
        "    raw_emg = np.load(os.path.join(base_dir, f'{index}_emg.npy'))\n",
        "    before = os.path.join(base_dir, f'{index-1}_emg.npy')\n",
        "    after = os.path.join(base_dir, f'{index+1}_emg.npy')\n",
        "    if os.path.exists(before):\n",
        "        raw_emg_before = np.load(before)\n",
        "    else:\n",
        "        raw_emg_before = np.zeros([0,raw_emg.shape[1]])\n",
        "    if os.path.exists(after):\n",
        "        raw_emg_after = np.load(after)\n",
        "    else:\n",
        "        raw_emg_after = np.zeros([0,raw_emg.shape[1]])\n",
        "\n",
        "    if 'out' in base_dir:\n",
        "      raw_emg_freq=512\n",
        "#      raw_emg_freq=1000\n",
        "    else:\n",
        "      raw_emg_freq=1000\n",
        "\n",
        "    x = np.concatenate([raw_emg_before, raw_emg, raw_emg_after], 0)\n",
        "    x = apply_to_all(notch_harmonics, x, 60, raw_emg_freq)\n",
        "    x = apply_to_all(remove_drift, x, raw_emg_freq)\n",
        "    x = x[raw_emg_before.shape[0]:x.shape[0]-raw_emg_after.shape[0],:]\n",
        "    emg_orig = apply_to_all(subsample, x, 800, raw_emg_freq)\n",
        "    x = apply_to_all(subsample, x, 600, raw_emg_freq)\n",
        "    emg = x\n",
        "\n",
        "    for c in FLAGS.remove_channels:\n",
        "        emg[:,int(c)] = 0\n",
        "        emg_orig[:,int(c)] = 0\n",
        "\n",
        "    emg_features = get_emg_features(emg)\n",
        "\n",
        "    mfccs, audio_discrete = load_audio(os.path.join(base_dir, f'{index}_audio_clean.flac'),\n",
        "            max_frames=min(emg_features.shape[0], 800 if limit_length else float('inf')))\n",
        "\n",
        "    if emg_features.shape[0] > mfccs.shape[0]:\n",
        "        emg_features = emg_features[:mfccs.shape[0],:]\n",
        "    emg = emg[6:6+6*emg_features.shape[0],:]\n",
        "    emg_orig = emg_orig[8:8+8*emg_features.shape[0],:]\n",
        "    assert emg.shape[0] == emg_features.shape[0]*6\n",
        "\n",
        "    with open(os.path.join(base_dir, f'{index}_info.json')) as f:\n",
        "        info = json.load(f)\n",
        "\n",
        "    sess = os.path.basename(base_dir)\n",
        "    tg_fname = f'{text_align_directory}/{sess}/{sess}_{index}_audio.TextGrid'\n",
        "    if os.path.exists(tg_fname):\n",
        "        phonemes = read_phonemes(tg_fname, mfccs.shape[0], phoneme_inventory)\n",
        "    else:\n",
        "        phonemes = np.zeros(mfccs.shape[0], dtype=np.int64)+phoneme_inventory.index('sil')\n",
        "\n",
        "    return mfccs, audio_discrete, emg_features, info['text'], (info['book'],info['sentence_index']), phonemes, emg_orig.astype(np.float32)\n",
        "\n",
        "\n",
        "def read_phonemes(textgrid_fname, mfcc_len, phone_inventory):\n",
        "    tg = TextGrid(textgrid_fname)\n",
        "    phone_ids = np.zeros(int(tg['phones'][-1].xmax*100), dtype=np.int64)\n",
        "    phone_ids[:] = -1\n",
        "    for interval in tg['phones']:\n",
        "        phone = interval.text.lower()\n",
        "        if phone in ['', 'sp', 'spn']:\n",
        "            phone = 'sil'\n",
        "        if phone[-1] in string.digits:\n",
        "            phone = phone[:-1]\n",
        "        ph_id = phone_inventory.index(phone)\n",
        "        phone_ids[int(interval.xmin*100):int(interval.xmax*100)] = ph_id\n",
        "    assert (phone_ids >= 0).all(), 'missing aligned phones'\n",
        "\n",
        "    phone_ids = phone_ids[1:mfcc_len+1] # mfccs is 2-3 shorter due to edge effects\n",
        "    return phone_ids\n",
        "\n",
        "class EMGDirectory(object):\n",
        "    def __init__(self, session_index, directory, silent, exclude_from_testset=False):\n",
        "        self.session_index = session_index\n",
        "        self.directory = directory\n",
        "        self.silent = silent\n",
        "        self.exclude_from_testset = exclude_from_testset\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return self.session_index < other.session_index\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.directory\n",
        "\n",
        "class SizeAwareSampler(torch.utils.data.Sampler):\n",
        "    def __init__(self, emg_dataset, max_len):\n",
        "        self.dataset = emg_dataset\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __iter__(self):\n",
        "        indices = list(range(len(self.dataset)))\n",
        "        random.shuffle(indices)\n",
        "        batch = []\n",
        "        batch_length = 0\n",
        "        for idx in indices:\n",
        "            directory_info, file_idx = self.dataset.example_indices[idx]\n",
        "            with open(os.path.join(directory_info.directory, f'{file_idx}_info.json')) as f:\n",
        "                info = json.load(f)\n",
        "            if not np.any([l in string.ascii_letters for l in info['text']]):\n",
        "                continue\n",
        "            length = sum([emg_len for emg_len, _, _ in info['chunks']])\n",
        "            if length > self.max_len:\n",
        "                logging.warning(f'Warning: example {idx} cannot fit within desired batch length')\n",
        "            if length + batch_length > self.max_len:\n",
        "                yield batch\n",
        "                batch = []\n",
        "                batch_length = 0\n",
        "            batch.append(idx)\n",
        "            batch_length += length\n",
        "        # dropping last incomplete batch\n",
        "\n",
        "class EMGDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_dir=None, limit_length=False, dev=False, test=False, no_testset=False, no_normalizers=False):\n",
        "\n",
        "        self.text_align_directory = FLAGS.text_align_directory\n",
        "\n",
        "        if no_testset:\n",
        "            devset = []\n",
        "            testset = []\n",
        "        else:\n",
        "            with open(FLAGS.testset_file) as f:\n",
        "                testset_json = json.load(f)\n",
        "                devset = testset_json['dev']\n",
        "                testset = testset_json['test']\n",
        "\n",
        "        #print(testset)\n",
        "        directories = []\n",
        "        if base_dir is not None:\n",
        "            directories.append(EMGDirectory(0, base_dir, False))\n",
        "        else:\n",
        "            for sd in FLAGS.silent_data_directories:\n",
        "                for session_dir in sorted(os.listdir(sd)):\n",
        "                    directories.append(EMGDirectory(len(directories), os.path.join(sd, session_dir), True))\n",
        "\n",
        "            has_silent = len(FLAGS.silent_data_directories) > 0\n",
        "            for vd in FLAGS.voiced_data_directories:\n",
        "                for session_dir in sorted(os.listdir(vd)):\n",
        "                    directories.append(EMGDirectory(len(directories), os.path.join(vd, session_dir), False, exclude_from_testset=has_silent))\n",
        "\n",
        "        self.example_indices = []\n",
        "        self.voiced_data_locations = {} # map from book/sentence_index to directory_info/index\n",
        "        for directory_info in directories:\n",
        "            for fname in os.listdir(directory_info.directory):\n",
        "                m = re.match(r'(\\d+)_info.json', fname)\n",
        "                if m is not None:\n",
        "                    idx_str = m.group(1)\n",
        "                    with open(os.path.join(directory_info.directory, fname)) as f:\n",
        "                        info = json.load(f)\n",
        "                        #print(info['book'],info['sentence_index'])\n",
        "                        if info['sentence_index'] >= 0: # boundary clips of silence are marked -1\n",
        "                            location_in_testset = [info['book'], info['sentence_index']] in testset\n",
        "                            location_in_devset = [info['book'], info['sentence_index']] in devset\n",
        "                            #print(location_in_testset,location_in_devset)\n",
        "                            if (test and location_in_testset and not directory_info.exclude_from_testset) \\\n",
        "                                    or (dev and location_in_devset and not directory_info.exclude_from_testset) \\\n",
        "                                    or (not test and not dev and not location_in_testset and not location_in_devset):\n",
        "                                self.example_indices.append((directory_info,int(idx_str)))\n",
        "\n",
        "                            if not directory_info.silent:\n",
        "                                location = (info['book'], info['sentence_index'])\n",
        "                                self.voiced_data_locations[location] = (directory_info,int(idx_str))\n",
        "\n",
        "        self.example_indices.sort()\n",
        "        random.seed(0)\n",
        "        random.shuffle(self.example_indices)\n",
        "\n",
        "        self.no_normalizers = no_normalizers\n",
        "        if not self.no_normalizers:\n",
        "            self.mfcc_norm, self.emg_norm = pickle.load(open(FLAGS.normalizers_file,'rb'))\n",
        "\n",
        "        sample_mfccs, _, sample_emg, _, _, _, _ = load_utterance(self.example_indices[0][0].directory, self.example_indices[0][1])\n",
        "        self.num_speech_features = sample_mfccs.shape[1]\n",
        "        self.num_features = sample_emg.shape[1]\n",
        "        self.limit_length = limit_length\n",
        "        self.num_sessions = len(directories)\n",
        "\n",
        "    def silent_subset(self):\n",
        "        silent_indices = []\n",
        "        for i, (d, _) in enumerate(self.example_indices):\n",
        "            if d.silent:\n",
        "                silent_indices.append(i)\n",
        "        return torch.utils.data.Subset(self, silent_indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.example_indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        directory_info, idx = self.example_indices[i]\n",
        "        mfccs, audio, emg, text, book_location, phonemes, raw_emg = load_utterance(directory_info.directory, idx, self.limit_length, text_align_directory=self.text_align_directory)\n",
        "        raw_emg = raw_emg / 10\n",
        "\n",
        "        if not self.no_normalizers:\n",
        "            mfccs = self.mfcc_norm.normalize(mfccs)\n",
        "            emg = self.emg_norm.normalize(emg)\n",
        "            emg = 8*np.tanh(emg/8.)\n",
        "\n",
        "        session_ids = np.full(emg.shape[0], directory_info.session_index, dtype=np.int64)\n",
        "\n",
        "        result = {'audio_features':mfccs, 'quantized_audio':audio, 'emg':emg, 'text':text, 'file_label':idx, 'session_ids':session_ids, 'book_location':book_location, 'silent':directory_info.silent, 'raw_emg':raw_emg}\n",
        "\n",
        "        if directory_info.silent:\n",
        "            voiced_directory, voiced_idx = self.voiced_data_locations[book_location]\n",
        "            voiced_mfccs, _, voiced_emg, _, _, phonemes, _ = load_utterance(voiced_directory.directory, voiced_idx, False, text_align_directory=self.text_align_directory)\n",
        "\n",
        "            if not self.no_normalizers:\n",
        "                voiced_mfccs = self.mfcc_norm.normalize(voiced_mfccs)\n",
        "                voiced_emg = self.emg_norm.normalize(voiced_emg)\n",
        "                voiced_emg = 8*np.tanh(voiced_emg/8.)\n",
        "\n",
        "            result['parallel_voiced_audio_features'] = voiced_mfccs\n",
        "            result['parallel_voiced_emg'] = voiced_emg\n",
        "\n",
        "        result['phonemes'] = phonemes # either from this example if vocalized or aligned example if silent\n",
        "\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fixed_length(batch):\n",
        "        batch_size = len(batch)\n",
        "        audio_features = []\n",
        "        audio_feature_lengths = []\n",
        "        parallel_emg = []\n",
        "        for ex in batch:\n",
        "            if ex['silent']:\n",
        "                audio_features.append(ex['parallel_voiced_audio_features'])\n",
        "                audio_feature_lengths.append(ex['parallel_voiced_audio_features'].shape[0])\n",
        "                parallel_emg.append(ex['parallel_voiced_emg'])\n",
        "            else:\n",
        "                audio_features.append(ex['audio_features'])\n",
        "                audio_feature_lengths.append(ex['audio_features'].shape[0])\n",
        "                parallel_emg.append(np.zeros(1))\n",
        "        audio_features = [torch.from_numpy(af) for af in audio_features]\n",
        "        parallel_emg = [torch.from_numpy(pe) for pe in parallel_emg]\n",
        "        phonemes = [torch.from_numpy(ex['phonemes']) for ex in batch]\n",
        "        emg = [torch.from_numpy(ex['emg']) for ex in batch]\n",
        "        raw_emg = [torch.from_numpy(ex['raw_emg']) for ex in batch]\n",
        "        session_ids = [torch.from_numpy(ex['session_ids']) for ex in batch]\n",
        "        lengths = [ex['emg'].shape[0] for ex in batch]\n",
        "        silent = [ex['silent'] for ex in batch]\n",
        "\n",
        "        seq_len = 200\n",
        "        result = {'audio_features':combine_fixed_length(audio_features, seq_len),\n",
        "                  'audio_feature_lengths':audio_feature_lengths,\n",
        "                  'emg':combine_fixed_length(emg, seq_len),\n",
        "                  'raw_emg':combine_fixed_length(raw_emg, seq_len*8),\n",
        "                  'parallel_voiced_emg':parallel_emg,\n",
        "                  'phonemes':phonemes,\n",
        "                  'session_ids':combine_fixed_length(session_ids, seq_len),\n",
        "                  'lengths':lengths,\n",
        "                  'silent':silent}\n",
        "        return result\n",
        "\n",
        "def make_normalizers():\n",
        "    dataset = EMGDataset(no_normalizers=True)\n",
        "    mfcc_samples = []\n",
        "    emg_samples = []\n",
        "    for d in dataset:\n",
        "        mfcc_samples.append(d['audio_features'])\n",
        "        emg_samples.append(d['emg'])\n",
        "        if len(emg_samples) > 50:\n",
        "            break\n",
        "    mfcc_norm = FeatureNormalizer(mfcc_samples, share_scale=True)\n",
        "    emg_norm = FeatureNormalizer(emg_samples, share_scale=False)\n",
        "    pickle.dump((mfcc_norm, emg_norm), open(FLAGS.normalizers_file, 'wb'))\n",
        "\n",
        "if False:\n",
        "    FLAGS(sys.argv)\n",
        "    d = EMGDataset()\n",
        "    for i in range(1000):\n",
        "        d[i]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szcXBWSHZvsA"
      },
      "outputs": [],
      "source": [
        "from absl import flags\n",
        "FLAGS = flags.FLAGS\n",
        "for name in list(flags.FLAGS):\n",
        "  delattr(flags.FLAGS, name)\n",
        "  \n",
        "#wavenet_model.py\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as torchdata\n",
        "\n",
        "from nv_wavenet.pytorch.wavenet import WaveNet\n",
        "#from nv_wavenet.pytorch import nv_wavenet\n",
        "\n",
        "from data_utils import splice_audio\n",
        "#from read_emg import EMGDataset\n",
        "from read_librispeech import SpeechDataset\n",
        "\n",
        "from absl import flags\n",
        "FLAGS = flags.FLAGS\n",
        "#flags.DEFINE_boolean('mel_spectrogram', False, 'use mel spectrogram features instead of mfccs for audio')\n",
        "#flags.DEFINE_string('normalizers_file', 'normalizers.pkl', 'file with pickled feature normalizers')\n",
        "\n",
        "flags.DEFINE_list('remove_channels', [], 'channels to remove')\n",
        "#flags.DEFINE_list('silent_data_directories', ['./emg_data/silent_parallel_data'], 'silent data locations')\n",
        "#flags.DEFINE_list('voiced_data_directories', ['./emg_data/voiced_parallel_data','./emg_data/nonparallel_data'], 'voiced data locations')\n",
        "#flags.DEFINE_string('testset_file', 'testset_largedev.json', 'file with testset indices')\n",
        "flags.DEFINE_list('silent_data_directories', ['./out'], 'silent data locations')\n",
        "flags.DEFINE_list('voiced_data_directories', ['./out','./out'], 'voiced data locations')\n",
        "flags.DEFINE_string('testset_file', 'testset_onlinedev.json', 'file with testset indices')\n",
        "flags.DEFINE_string('text_align_directory', 'text_alignments', 'directory with alignment files')\n",
        "\n",
        "flags.DEFINE_boolean('debug', False, 'debug')\n",
        "flags.DEFINE_string('output_directory', 'output', 'where to save models and outputs')\n",
        "flags.DEFINE_boolean('librispeech', False, 'train with librispeech data')\n",
        "flags.DEFINE_string('pretrained_wavenet_model', None, 'filename of model to start training with')\n",
        "flags.DEFINE_float('clip_norm', 0.1, 'gradient clipping max norm')\n",
        "flags.DEFINE_boolean('wavenet_no_lstm', False, \"don't use a LSTM before the wavenet\")\n",
        "\n",
        "class WavenetModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        if not FLAGS.wavenet_no_lstm:\n",
        "            self.lstm = nn.LSTM(input_dim, 512, bidirectional=True, batch_first=True)\n",
        "            self.projection_layer = nn.Linear(512*2, 128)\n",
        "        else:\n",
        "            self.projection_layer = nn.Linear(input_dim, 128)\n",
        "        self.wavenet = WaveNet(n_in_channels=256, n_layers=16, max_dilation=128, n_residual_channels=64, n_skip_channels=256, n_out_channels=256, n_cond_channels=128, upsamp_window=432, upsamp_stride=160)\n",
        "\n",
        "    def pre_wavenet_processing(self, x):\n",
        "        if not FLAGS.wavenet_no_lstm:\n",
        "            x, _ = self.lstm(x)\n",
        "            x = F.dropout(x, 0.5, training=self.training)\n",
        "        x = self.projection_layer(x)\n",
        "        return x.permute(0,2,1)\n",
        "\n",
        "    def forward(self, x, audio):\n",
        "        x = self.pre_wavenet_processing(x)\n",
        "        return self.wavenet((x, audio))\n",
        "\n",
        "def test(wavenet_model, testset, device):\n",
        "    wavenet_model.eval()\n",
        "\n",
        "    errors = []\n",
        "    dataloader = torchdata.DataLoader(testset, batch_size=1, shuffle=True, pin_memory=(device=='cuda'))\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            mfcc = batch['audio_features'].to(device)\n",
        "            audio = batch['quantized_audio'].to(device)\n",
        "\n",
        "            audio_out = wavenet_model(mfcc, audio)\n",
        "            loss = F.cross_entropy(audio_out, audio)\n",
        "            errors.append(loss.item())\n",
        "\n",
        "    wavenet_model.train()\n",
        "    return np.mean(errors)\n",
        "\n",
        "#def save_output(wavenet_model, input_data, filename, device):\n",
        "def save_wavenet_output(wavenet_model, input_data, filename, device):\n",
        "    wavenet_model.eval()\n",
        "\n",
        "    assert len(input_data.shape) == 2\n",
        "    X = torch.tensor(input_data, dtype=torch.float32).to(device).unsqueeze(0)\n",
        "\n",
        "    wavenet = wavenet_model.wavenet\n",
        "    inference_wavenet = NVWaveNet(**wavenet.export_weights())\n",
        "#    inference_wavenet = nv_wavenet.NVWaveNet(**wavenet.export_weights())\n",
        "    cond_input = wavenet_model.pre_wavenet_processing(X)\n",
        "\n",
        "    chunk_len = 400\n",
        "    overlap = 1\n",
        "    audio_chunks = []\n",
        "    for i in range(0, cond_input.size(2), chunk_len-overlap):\n",
        "        if cond_input.size(2)-i < overlap:\n",
        "            break # don't make segment at end that doesn't go past overlapped part\n",
        "        cond_chunk = cond_input[:,:,i:i+chunk_len]\n",
        "        wavenet_cond_input = wavenet.get_cond_input(cond_chunk)\n",
        "        audio_data = inference_wavenet.infer(wavenet_cond_input, nv_wavenet.Impl.SINGLE_BLOCK)\n",
        "        audio_chunk = librosa.core.mu_expand(audio_data.squeeze(0).cpu().numpy()-128, 255, True)\n",
        "        audio_chunks.append(audio_chunk)\n",
        "\n",
        "    audio_out = splice_audio(audio_chunks, overlap*160)\n",
        "\n",
        "    sf.write(filename, audio_out, 16000)\n",
        "\n",
        "    wavenet_model.train()\n",
        "\n",
        "def train():\n",
        "    if FLAGS.librispeech:\n",
        "        dataset = SpeechDataset('LibriSpeech/train-clean-100-sliced', 'M', 'LibriSpeech/SPEAKERS.TXT')\n",
        "        testset = torch.utils.data.Subset(dataset, list(range(10)))\n",
        "        trainset = torch.utils.data.Subset(dataset, list(range(10,len(dataset))))\n",
        "        num_features = dataset.num_speech_features\n",
        "        batch_size = 4\n",
        "        logging.info('output example: %s', dataset.filenames[0])\n",
        "    else:\n",
        "        trainset = EMGDataset(dev=False, test=False, limit_length=True)\n",
        "        testset = EMGDataset(dev=True, limit_length=True)\n",
        "        num_features = testset.num_speech_features\n",
        "        batch_size = 1\n",
        "        logging.info('output example: %s', testset.example_indices[0])\n",
        "\n",
        "    if not os.path.exists(FLAGS.output_directory):\n",
        "        os.makedirs(FLAGS.output_directory)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'\n",
        "\n",
        "    wavenet_model = WavenetModel(num_features).to(device)\n",
        "    if FLAGS.pretrained_wavenet_model is not None:\n",
        "        wavenet_model.load_state_dict(torch.load(FLAGS.pretrained_wavenet_model))\n",
        "\n",
        "    optim = torch.optim.Adam(wavenet_model.parameters(), weight_decay=1e-7)\n",
        "    lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, 'min', 0.5, patience=2)\n",
        "    dataloader = torchdata.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=(device=='cuda'))\n",
        "\n",
        "    best_dev_err = float('inf')\n",
        "    for epoch_idx in range(50):\n",
        "        losses = []\n",
        "        for batch in dataloader:\n",
        "            mfcc = batch['audio_features'].to(device)\n",
        "            audio = batch['quantized_audio'].to(device)\n",
        "\n",
        "            optim.zero_grad()\n",
        "\n",
        "            audio_out = wavenet_model(mfcc, audio)\n",
        "            loss = F.cross_entropy(audio_out, audio)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(wavenet_model.parameters(), FLAGS.clip_norm)\n",
        "            optim.step()\n",
        "        train_err = np.mean(losses)\n",
        "        dev_err = test(wavenet_model, testset, device)\n",
        "        lr_sched.step(dev_err)\n",
        "        logging.info(f'finished epoch {epoch_idx+1} with error {dev_err:.2f}')\n",
        "        logging.info(f' train error {train_err:.2f}')\n",
        "        if dev_err < best_dev_err:\n",
        "            logging.info('saving model')\n",
        "            torch.save(wavenet_model.state_dict(), os.path.join(FLAGS.output_directory, 'wavenet_model.pt'))\n",
        "            best_dev_err = dev_err\n",
        "\n",
        "    wavenet_model.load_state_dict(torch.load(os.path.join(FLAGS.output_directory,'wavenet_model.pt'))) # re-load best parameters\n",
        "    for i, datapoint in enumerate(testset):\n",
        "        save_output(wavenet_model, datapoint['audio_features'], os.path.join(FLAGS.output_directory, f'wavenet_output_{i}.wav'), device)\n",
        "\n",
        "if False:\n",
        "    FLAGS(sys.argv)\n",
        "\n",
        "    os.makedirs(FLAGS.output_directory, exist_ok=True)\n",
        "    logging.basicConfig(handlers=[\n",
        "            logging.FileHandler(os.path.join(FLAGS.output_directory, 'log.txt'), 'w'),\n",
        "            logging.StreamHandler()\n",
        "            ], level=logging.INFO, format=\"%(message)s\")\n",
        "    logging.info(sys.argv)\n",
        "\n",
        "    train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPGIBL4XFfj9"
      },
      "outputs": [],
      "source": [
        "from absl import flags\n",
        "FLAGS = flags.FLAGS\n",
        "for name in list(flags.FLAGS):\n",
        "  delattr(flags.FLAGS, name)\n",
        "\n",
        "#transduction_model.py\n",
        "\n",
        "import sys\n",
        "#sys.argv = \" --train_dir training/\".split(\" \")\n",
        "sys.argv = \" \".split(\" \")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import logging\n",
        "import subprocess\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from read_emg import EMGDataset, SizeAwareSampler\n",
        "#from wavenet_model import WavenetModel, save_output as save_wavenet_output\n",
        "from align import align_from_distances\n",
        "from asr import evaluate\n",
        "from transformer import TransformerEncoderLayer\n",
        "#from data_utils import phoneme_inventory, decollate_tensor\n",
        "\n",
        "from absl import app, flags\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_integer('verbosity', 1, 'number of hidden dimensions')\n",
        "flags.DEFINE_bool('debug', False, 'debug mode')\n",
        "\n",
        "flags.DEFINE_boolean('mel_spectrogram', False, 'use mel spectrogram features instead of mfccs for audio')\n",
        "flags.DEFINE_string('normalizers_file', 'normalizers.pkl', 'file with pickled feature normalizers')\n",
        "\n",
        "flags.DEFINE_list('remove_channels', [], 'channels to remove')\n",
        "flags.DEFINE_list('silent_data_directories', ['./emg_data/silent_parallel_data'], 'silent data locations')\n",
        "flags.DEFINE_list('voiced_data_directories', ['./emg_data/voiced_parallel_data','./emg_data/nonparallel_data'], 'voiced data locations')\n",
        "flags.DEFINE_string('testset_file', 'testset_largedev.json', 'file with testset indices')\n",
        "flags.DEFINE_string('text_align_directory', 'text_alignments', 'directory with alignment files')\n",
        "\n",
        "flags.DEFINE_boolean('run_with_pdb', False, 'Set to true for PDB debug mode')\n",
        "flags.DEFINE_boolean('pdb_post_mortem', False,\n",
        "                     'Set to true to handle uncaught exceptions with PDB '\n",
        "                     'post mortem.')\n",
        "flags.DEFINE_alias('pdb', 'pdb_post_mortem')\n",
        "flags.DEFINE_boolean('run_with_profiling', False,\n",
        "                     'Set to true for profiling the script. '\n",
        "                     'Execution will be slower, and the output format might '\n",
        "                     'change over time.')\n",
        "flags.DEFINE_string('profile_file', None,\n",
        "                    'Dump profile information to a file (for python -m '\n",
        "                    'pstats). Implies --run_with_profiling.')\n",
        "flags.DEFINE_boolean('use_cprofile_for_profiling', True,\n",
        "                     'Use cProfile instead of the profile module for '\n",
        "                     'profiling. This has no effect unless '\n",
        "                     '--run_with_profiling is set.')\n",
        "flags.DEFINE_boolean('only_check_args', False,\n",
        "                     'Set to true to validate args and exit.',\n",
        "                     allow_hide_cpp=True)\n",
        "\n",
        "\n",
        "flags.DEFINE_integer('model_size', 768, 'number of hidden dimensions')\n",
        "flags.DEFINE_integer('num_layers', 6, 'number of layers')\n",
        "flags.DEFINE_integer('batch_size', 32, 'training batch size')\n",
        "flags.DEFINE_float('learning_rate', 1e-3, 'learning rate')\n",
        "flags.DEFINE_integer('learning_rate_patience', 5, 'learning rate decay patience')\n",
        "flags.DEFINE_integer('learning_rate_warmup', 500, 'steps of linear warmup')\n",
        "#flags.DEFINE_string('start_training_from', None, 'start training from this model')\n",
        "flags.DEFINE_float('data_size_fraction', 1.0, 'fraction of training data to use')\n",
        "flags.DEFINE_boolean('no_session_embed', False, \"don't use a session embedding\")\n",
        "flags.DEFINE_float('phoneme_loss_weight', 0.1, 'weight of auxiliary phoneme prediction loss')\n",
        "flags.DEFINE_float('l2', 1e-7, 'weight decay')\n",
        "\n",
        "flags.DEFINE_string('start_training_from', './models/transduction_model/model_07.pt', 'start training from this model')\n",
        "flags.DEFINE_string('pretrained_wavenet_model', \"./models/wavenet_model/wavenet_model_50.pt\", 'start training from this model')\n",
        "flags.DEFINE_string('output_directory', \"./models/transduction_model/07/\", 'start training from this model')\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, num_ins, num_outs, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(num_ins, num_outs, 3, padding=1, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm1d(num_outs)\n",
        "        self.conv2 = nn.Conv1d(num_outs, num_outs, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(num_outs)\n",
        "\n",
        "        if stride != 1 or num_ins != num_outs:\n",
        "            self.residual_path = nn.Conv1d(num_ins, num_outs, 1, stride=stride)\n",
        "            self.res_norm = nn.BatchNorm1d(num_outs)\n",
        "        else:\n",
        "            self.residual_path = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_value = x\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.bn2(self.conv2(x))\n",
        "\n",
        "        if self.residual_path is not None:\n",
        "            res = self.res_norm(self.residual_path(input_value))\n",
        "        else:\n",
        "            res = input_value\n",
        "\n",
        "        return F.relu(x + res)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_ins, num_outs, num_aux_outs, num_sessions):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            ResBlock(8, FLAGS.model_size, 2),\n",
        "            ResBlock(FLAGS.model_size, FLAGS.model_size, 2),\n",
        "            ResBlock(FLAGS.model_size, FLAGS.model_size, 2),\n",
        "        )\n",
        "        self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)\n",
        "\n",
        "        if not FLAGS.no_session_embed:\n",
        "            emb_size = 32\n",
        "            self.session_emb = nn.Embedding(num_sessions, emb_size)\n",
        "            self.w_emb = nn.Linear(emb_size, FLAGS.model_size)\n",
        "\n",
        "        encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)\n",
        "        self.w_out = nn.Linear(FLAGS.model_size, num_outs)\n",
        "        self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)\n",
        "\n",
        "    def forward(self, x_feat, x_raw, session_ids):\n",
        "        # x shape is (batch, time, electrode)\n",
        "\n",
        "        x_raw = x_raw.transpose(1,2) # put channel before time for conv\n",
        "        x_raw = self.conv_blocks(x_raw)\n",
        "        x_raw = x_raw.transpose(1,2)\n",
        "        x_raw = self.w_raw_in(x_raw)\n",
        "\n",
        "        if FLAGS.no_session_embed:\n",
        "            x = x_raw\n",
        "        else:\n",
        "            emb = self.session_emb(session_ids)\n",
        "            x = x_raw + self.w_emb(emb)\n",
        "\n",
        "        x = x.transpose(0,1) # put time first\n",
        "        x = self.transformer(x)\n",
        "        x = x.transpose(0,1)\n",
        "        return self.w_out(x), self.w_aux(x)\n",
        "\n",
        "def test(model, testset, device):\n",
        "    model.eval()\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(testset, batch_size=32, collate_fn=testset.collate_fixed_length)\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    phoneme_confusion = np.zeros((len(phoneme_inventory),len(phoneme_inventory)))\n",
        "    with torch.no_grad():\n",
        "        for example in dataloader:\n",
        "            X = example['emg'].to(device)\n",
        "            X_raw = example['raw_emg'].to(device)\n",
        "            sess = example['session_ids'].to(device)\n",
        "\n",
        "            pred, phoneme_pred = model(X, X_raw, sess)\n",
        "\n",
        "            loss, phon_acc = dtw_loss(pred, phoneme_pred, example, True, phoneme_confusion)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            accuracies.append(phon_acc)\n",
        "\n",
        "    model.train()\n",
        "    return np.mean(losses), np.mean(accuracies), phoneme_confusion #TODO size-weight average\n",
        "\n",
        "def save_output(model, datapoint, filename, device, gold_mfcc=False):\n",
        "    model.eval()\n",
        "    if gold_mfcc:\n",
        "        y = datapoint['audio_features']\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            sess = torch.tensor(datapoint['session_ids'], device=device).unsqueeze(0)\n",
        "            X = torch.tensor(datapoint['emg'], dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            X_raw = torch.tensor(datapoint['raw_emg'], dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "            pred, _ = model(X, X_raw, sess)\n",
        "            pred = pred.squeeze(0)\n",
        "\n",
        "            y = pred.cpu().detach().numpy()\n",
        "\n",
        "    wavenet_model = WavenetModel(y.shape[1]).to(device)\n",
        "    assert FLAGS.pretrained_wavenet_model is not None\n",
        "    wavenet_model.load_state_dict(torch.load(FLAGS.pretrained_wavenet_model))\n",
        "    save_wavenet_output(wavenet_model, y, filename, device)\n",
        "    model.train()\n",
        "\n",
        "def dtw_loss(predictions, phoneme_predictions, example, phoneme_eval=False, phoneme_confusion=None):\n",
        "    device = predictions.device\n",
        "    predictions = decollate_tensor(predictions, example['lengths'])\n",
        "    phoneme_predictions = decollate_tensor(phoneme_predictions, example['lengths'])\n",
        "\n",
        "    audio_features = example['audio_features'].to(device)\n",
        "\n",
        "    phoneme_targets = example['phonemes']\n",
        "\n",
        "    audio_features = decollate_tensor(audio_features, example['audio_feature_lengths'])\n",
        "\n",
        "    losses = []\n",
        "    correct_phones = 0\n",
        "    total_length = 0\n",
        "    for pred, y, pred_phone, y_phone, silent in zip(predictions, audio_features, phoneme_predictions, phoneme_targets, example['silent']):\n",
        "        assert len(pred.size()) == 2 and len(y.size()) == 2\n",
        "        y_phone = y_phone.to(device)\n",
        "\n",
        "        if silent:\n",
        "            dists = torch.cdist(pred.unsqueeze(0), y.unsqueeze(0))\n",
        "            costs = dists.squeeze(0)\n",
        "\n",
        "            # pred_phone (seq1_len, 48), y_phone (seq2_len)\n",
        "            # phone_probs (seq1_len, seq2_len)\n",
        "            pred_phone = F.log_softmax(pred_phone, -1)\n",
        "            phone_lprobs = pred_phone[:,y_phone]\n",
        "\n",
        "            costs = costs + FLAGS.phoneme_loss_weight * -phone_lprobs\n",
        "\n",
        "            alignment = align_from_distances(costs.T.cpu().detach().numpy())\n",
        "\n",
        "            loss = costs[alignment,range(len(alignment))].sum()\n",
        "\n",
        "            if phoneme_eval:\n",
        "                alignment = align_from_distances(costs.T.cpu().detach().numpy())\n",
        "\n",
        "                pred_phone = pred_phone.argmax(-1)\n",
        "                correct_phones += (pred_phone[alignment] == y_phone).sum().item()\n",
        "\n",
        "                for p, t in zip(pred_phone[alignment].tolist(), y_phone.tolist()):\n",
        "                    phoneme_confusion[p, t] += 1\n",
        "        else:\n",
        "            assert y.size(0) == pred.size(0)\n",
        "\n",
        "            dists = F.pairwise_distance(y, pred)\n",
        "\n",
        "            assert len(pred_phone.size()) == 2 and len(y_phone.size()) == 1\n",
        "            phoneme_loss = F.cross_entropy(pred_phone, y_phone, reduction='sum')\n",
        "            loss = dists.cpu().sum() + FLAGS.phoneme_loss_weight * phoneme_loss.cpu()\n",
        "\n",
        "            if phoneme_eval:\n",
        "                pred_phone = pred_phone.argmax(-1)\n",
        "                correct_phones += (pred_phone == y_phone).sum().item()\n",
        "\n",
        "                for p, t in zip(pred_phone.tolist(), y_phone.tolist()):\n",
        "                    phoneme_confusion[p, t] += 1\n",
        "\n",
        "        losses.append(loss)\n",
        "        total_length += y.size(0)\n",
        "\n",
        "    return sum(losses)/total_length, correct_phones/total_length\n",
        "\n",
        "def train_model(trainset, devset, device, save_sound_outputs=True, n_epochs=80):\n",
        "    if FLAGS.data_size_fraction >= 1:\n",
        "        training_subset = trainset\n",
        "    else:\n",
        "        training_subset = torch.utils.data.Subset(trainset, list(range(int(len(trainset)*FLAGS.data_size_fraction))))\n",
        "    dataloader = torch.utils.data.DataLoader(training_subset, pin_memory=(device=='cuda'), collate_fn=devset.collate_fixed_length, num_workers=8, batch_sampler=SizeAwareSampler(trainset, 256000))\n",
        "\n",
        "    n_phones = len(phoneme_inventory)\n",
        "    model = Model(devset.num_features, devset.num_speech_features, n_phones, devset.num_sessions).to(device)\n",
        "\n",
        "    if FLAGS.start_training_from is not None:\n",
        "        state_dict = torch.load(FLAGS.start_training_from)\n",
        "        del state_dict['session_emb.weight']\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    optim = torch.optim.AdamW(model.parameters(), weight_decay=FLAGS.l2)\n",
        "    lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, 'min', 0.5, patience=FLAGS.learning_rate_patience)\n",
        "\n",
        "    def set_lr(new_lr):\n",
        "        for param_group in optim.param_groups:\n",
        "            param_group['lr'] = new_lr\n",
        "\n",
        "    target_lr = FLAGS.learning_rate\n",
        "    def schedule_lr(iteration):\n",
        "        iteration = iteration + 1\n",
        "        if iteration <= FLAGS.learning_rate_warmup:\n",
        "            set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)\n",
        "\n",
        "    batch_idx = 0\n",
        "    for epoch_idx in range(n_epochs):\n",
        "        losses = []\n",
        "        for example in dataloader:\n",
        "            optim.zero_grad()\n",
        "            schedule_lr(batch_idx)\n",
        "\n",
        "            X = example['emg'].to(device)\n",
        "            X_raw = example['raw_emg'].to(device)\n",
        "            sess = example['session_ids'].to(device)\n",
        "\n",
        "            pred, phoneme_pred = model(X, X_raw, sess)\n",
        "\n",
        "            loss, _ = dtw_loss(pred, phoneme_pred, example)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            batch_idx += 1\n",
        "        train_loss = np.mean(losses)\n",
        "        val, phoneme_acc, _ = test(model, devset, device)\n",
        "        lr_sched.step(val)\n",
        "        logging.info(f'finished epoch {epoch_idx+1} - validation loss: {val:.4f} training loss: {train_loss:.4f} phoneme accuracy: {phoneme_acc*100:.2f}')\n",
        "        torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))\n",
        "        if save_sound_outputs:\n",
        "            save_output(model, devset[0], os.path.join(FLAGS.output_directory, f'epoch_{epoch_idx}_output.wav'), device)\n",
        "\n",
        "    model.load_state_dict(torch.load(os.path.join(FLAGS.output_directory,'model.pt'))) # re-load best parameters\n",
        "\n",
        "    if save_sound_outputs:\n",
        "        for i, datapoint in enumerate(devset):\n",
        "            save_output(model, datapoint, os.path.join(FLAGS.output_directory, f'example_output_{i}.wav'), device)\n",
        "\n",
        "    evaluate(devset, FLAGS.output_directory)\n",
        "\n",
        "    return model\n",
        "\n",
        "def main(argvs):\n",
        "    os.makedirs(FLAGS.output_directory, exist_ok=True)\n",
        "    logging.basicConfig(handlers=[\n",
        "            logging.FileHandler(os.path.join(FLAGS.output_directory, 'log.txt'), 'w'),\n",
        "            logging.StreamHandler()\n",
        "            ], level=logging.INFO, format=\"%(message)s\")\n",
        "\n",
        "    logging.info(subprocess.run(['git','rev-parse','HEAD'], stdout=subprocess.PIPE, universal_newlines=True).stdout)\n",
        "    logging.info(subprocess.run(['git','diff'], stdout=subprocess.PIPE, universal_newlines=True).stdout)\n",
        "\n",
        "    logging.info(sys.argv)\n",
        "\n",
        "    trainset = EMGDataset(dev=False,test=False)\n",
        "    devset = EMGDataset(dev=True)\n",
        "    logging.info('output example: %s', devset.example_indices[0])\n",
        "    logging.info('train / dev split: %d %d',len(trainset),len(devset))\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'\n",
        "\n",
        "    model = train_model(trainset, devset, device, save_sound_outputs=(FLAGS.pretrained_wavenet_model is not None))\n",
        "\n",
        "#app.run(main)\n",
        "#main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from absl import flags\n",
        "FLAGS = flags.FLAGS\n",
        "for name in list(flags.FLAGS):\n",
        "  delattr(flags.FLAGS, name)\n",
        "\n",
        "import sys\n",
        "#sys.argv = \" --train_dir training/\".split(\" \")\n",
        "sys.argv = \" \".split(\" \")\n",
        "sys.argv = \" --models ./models/transduction_model/model_07.pt --pretrained_wavenet_model ./models/wavenet_model/wavenet_model_50.pt --output_directory evaluation_output\".split(\" \")\n",
        "\n",
        "#evaluate.py\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "#from transduction_model import test, save_output, Model\n",
        "#from read_emg import EMGDataset\n",
        "from asr import evaluate\n",
        "#from data_utils import phoneme_inventory, print_confusion\n",
        "\n",
        "from absl import flags, app#, logging\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_list('models', [], 'identifiers of models to evaluate')\n",
        "\n",
        "flags.DEFINE_integer('verbosity', 1, 'number of hidden dimensions')\n",
        "flags.DEFINE_bool('debug', False, 'debug mode')\n",
        "\n",
        "flags.DEFINE_boolean('mel_spectrogram', False, 'use mel spectrogram features instead of mfccs for audio')\n",
        "flags.DEFINE_string('normalizers_file', 'normalizers.pkl', 'file with pickled feature normalizers')\n",
        "\n",
        "flags.DEFINE_list('remove_channels', [], 'channels to remove')\n",
        "#flags.DEFINE_list('silent_data_directories', ['./emg_data/silent_parallel_data'], 'silent data locations')\n",
        "#flags.DEFINE_list('voiced_data_directories', ['./emg_data/voiced_parallel_data','./emg_data/nonparallel_data'], 'voiced data locations')\n",
        "#flags.DEFINE_string('testset_file', 'testset_largedev.json', 'file with testset indices')\n",
        "flags.DEFINE_list('silent_data_directories', ['./out'], 'silent data locations')\n",
        "flags.DEFINE_list('voiced_data_directories', ['./out','./out'], 'voiced data locations')\n",
        "flags.DEFINE_string('testset_file', 'testset_onlinedev.json', 'file with testset indices')\n",
        "flags.DEFINE_string('text_align_directory', 'text_alignments', 'directory with alignment files')\n",
        "\n",
        "flags.DEFINE_boolean('run_with_pdb', False, 'Set to true for PDB debug mode')\n",
        "flags.DEFINE_boolean('pdb_post_mortem', False,\n",
        "                     'Set to true to handle uncaught exceptions with PDB '\n",
        "                     'post mortem.')\n",
        "flags.DEFINE_alias('pdb', 'pdb_post_mortem')\n",
        "flags.DEFINE_boolean('run_with_profiling', False,\n",
        "                     'Set to true for profiling the script. '\n",
        "                     'Execution will be slower, and the output format might '\n",
        "                     'change over time.')\n",
        "flags.DEFINE_string('profile_file', None,\n",
        "                    'Dump profile information to a file (for python -m '\n",
        "                    'pstats). Implies --run_with_profiling.')\n",
        "flags.DEFINE_boolean('use_cprofile_for_profiling', True,\n",
        "                     'Use cProfile instead of the profile module for '\n",
        "                     'profiling. This has no effect unless '\n",
        "                     '--run_with_profiling is set.')\n",
        "flags.DEFINE_boolean('only_check_args', False,\n",
        "                     'Set to true to validate args and exit.',\n",
        "                     allow_hide_cpp=True)\n",
        "\n",
        "flags.DEFINE_integer('model_size', 768, 'number of hidden dimensions')\n",
        "flags.DEFINE_integer('num_layers', 6, 'number of layers')\n",
        "flags.DEFINE_integer('batch_size', 32, 'training batch size')\n",
        "flags.DEFINE_float('learning_rate', 1e-3, 'learning rate')\n",
        "flags.DEFINE_integer('learning_rate_patience', 5, 'learning rate decay patience')\n",
        "flags.DEFINE_integer('learning_rate_warmup', 500, 'steps of linear warmup')\n",
        "#flags.DEFINE_string('start_training_from', None, 'start training from this model')\n",
        "flags.DEFINE_float('data_size_fraction', 1.0, 'fraction of training data to use')\n",
        "flags.DEFINE_boolean('no_session_embed', False, \"don't use a session embedding\")\n",
        "flags.DEFINE_float('phoneme_loss_weight', 0.1, 'weight of auxiliary phoneme prediction loss')\n",
        "flags.DEFINE_float('l2', 1e-7, 'weight decay')\n",
        "\n",
        "#flags.DEFINE_boolean('debug', False, 'debug')\n",
        "#flags.DEFINE_string('output_directory', 'output', 'where to save models and outputs')\n",
        "flags.DEFINE_boolean('librispeech', False, 'train with librispeech data')\n",
        "#flags.DEFINE_string('pretrained_wavenet_model', None, 'filename of model to start training with')\n",
        "flags.DEFINE_float('clip_norm', 0.1, 'gradient clipping max norm')\n",
        "flags.DEFINE_boolean('wavenet_no_lstm', False, \"don't use a LSTM before the wavenet\")\n",
        "\n",
        "flags.DEFINE_string('start_training_from', './models/transduction_model/model_07.pt', 'start training from this model')\n",
        "flags.DEFINE_string('pretrained_wavenet_model', \"./models/wavenet_model/wavenet_model_50.pt\", '')\n",
        "flags.DEFINE_string('output_directory', \"./evaluation_output\", '')\n",
        "#flags.DEFINE_string('output_directory', \"./models/transduction_model/07/\", 'start training from this model')\n",
        "\n",
        "class EnsembleModel(nn.Module):\n",
        "    def __init__(self, models):\n",
        "        super().__init__()\n",
        "        self.models = nn.ModuleList(models)\n",
        "\n",
        "    def forward(self, x, x_raw, sess):\n",
        "        ys = []\n",
        "        ps = []\n",
        "        for model in self.models:\n",
        "            y, p = model(x, x_raw, sess)\n",
        "            ys.append(y)\n",
        "            ps.append(p)\n",
        "        return torch.stack(ys,0).mean(0), torch.stack(ps,0).mean(0)\n",
        "\n",
        "def main(argvs):\n",
        "    os.makedirs(FLAGS.output_directory, exist_ok=True)\n",
        "    logging.basicConfig(handlers=[\n",
        "            logging.FileHandler(os.path.join(FLAGS.output_directory, 'eval_log.txt'), 'w'),\n",
        "            logging.StreamHandler()\n",
        "            ], level=logging.INFO, format=\"%(message)s\")\n",
        "\n",
        "    testset = EMGDataset(test=True)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'\n",
        "\n",
        "    models = []\n",
        "    for fname in FLAGS.models:\n",
        "        state_dict = torch.load(fname)\n",
        "        n_sess = 1 if FLAGS.no_session_embed else state_dict[\"session_emb.weight\"].size(0)\n",
        "        model = Model(testset.num_features, testset.num_speech_features, len(phoneme_inventory), n_sess).to(device)\n",
        "        model.load_state_dict(state_dict)\n",
        "        models.append(model)\n",
        "    ensemble = EnsembleModel(models)\n",
        "\n",
        "    _, _, confusion = test(ensemble, testset, device)\n",
        "    print_confusion(confusion)\n",
        "\n",
        "    for i, datapoint in enumerate(testset):\n",
        "        save_output(ensemble, datapoint, os.path.join(FLAGS.output_directory, f'example_output_{i}.wav'), device)\n",
        "\n",
        "    evaluate(testset, FLAGS.output_directory)\n",
        "\n",
        "if False:\n",
        "    FLAGS(sys.argv)\n",
        "    main()\n",
        "\n",
        "#app.run(main)"
      ],
      "metadata": {
        "id": "yVGWXG-huOJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FLAGS(sys.argv)"
      ],
      "metadata": {
        "id": "Ri7HdNvuuY8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGNLwPiJdWZ-"
      },
      "outputs": [],
      "source": [
        "%rm -rf ./out\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import textwrap\n",
        "#import curses\n",
        "import soundfile as sf\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "book_file='books/War_of_the_Worlds.txt'\n",
        "output_directory='./out/0'\n",
        "if True:\n",
        "\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "#    os.makedirs(FLAGS.output_directory, exist_ok=False)\n",
        "    output_idx = 0\n",
        "    book = Book(book_file)\n",
        "\n",
        "def display_sentence(sentence):#, win):\n",
        "    #height, width = win.getmaxyx()\n",
        "    height=20\n",
        "    width=80\n",
        "    #win.clear()\n",
        "    print(' ')\n",
        "    wrapped_sentence = textwrap.wrap(sentence, width)\n",
        "    for i, text in enumerate(wrapped_sentence):\n",
        "        if i >= height:\n",
        "            break\n",
        "        #win.addstr(i, 0, text)\n",
        "        print(text)\n",
        "    print(' ')\n",
        "    #win.refresh()\n",
        "\n",
        "def save_data(output_idx, data, book):\n",
        "    emg, audio, button, chunk_info = data\n",
        "    emg_file = os.path.join(output_directory, f'{output_idx}_emg.npy')\n",
        "#    audio_file = os.path.join(output_directory, f'{output_idx}_audio.flac')\n",
        "    audio_file = os.path.join(output_directory, f'{output_idx}_audio_clean.flac')\n",
        "    button_file = os.path.join(output_directory, f'{output_idx}_button.npy')\n",
        "    info_file = os.path.join(output_directory, f'{output_idx}_info.json')\n",
        "    #assert not os.path.exists(emg_file), 'trying to overwrite existing file'\n",
        "    np.save(emg_file, emg)\n",
        "    sf.write(audio_file, audio, 16000)\n",
        "    np.save(button_file, button)\n",
        "\n",
        "    if book is None:\n",
        "        # special silence segment\n",
        "        bf = ''\n",
        "        bi = -1\n",
        "        t = ''\n",
        "    else:\n",
        "        bf = book.file\n",
        "        bi = book.current_index\n",
        "        t = book.current_sentence()\n",
        "\n",
        "    with open(info_file, 'w') as f:\n",
        "        json.dump({'book':bf, 'sentence_index':bi, 'text':t, 'chunks':chunk_info}, f)\n",
        "\n",
        "\n",
        "def get_ends(data):\n",
        "    emg, audio, button, chunk_info = data\n",
        "    emg_start = emg[:500,:]\n",
        "    emg_end = emg[-500:,:]\n",
        "    dummy_audio = np.zeros(8000)\n",
        "    dummy_button = np.zeros(500, dtype=bool)\n",
        "    chunk_info = [(500,8000,500)]\n",
        "    return (emg_start, dummy_audio, dummy_button, chunk_info), (emg_end, dummy_audio, dummy_button, chunk_info)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okl1tri4CLYn"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sounddevice as sd\n",
        "import scipy.signal\n",
        "\n",
        "import brainflow\n",
        "from brainflow.board_shim import BoardShim, BrainFlowInputParams, BoardIds, IpProtocolType\n",
        "from brainflow.data_filter import DataFilter, FilterTypes, AggOperations\n",
        "\n",
        "def remove_drift(signal, fs):\n",
        "    b, a = scipy.signal.butter(3, 2, 'highpass', fs=fs)\n",
        "    return scipy.signal.filtfilt(b, a, signal)\n",
        "\n",
        "def notch(signal, freq, sample_frequency):\n",
        "    b, a = scipy.signal.iirnotch(freq, 30, sample_frequency)\n",
        "    return scipy.signal.filtfilt(b, a, signal)\n",
        "\n",
        "def notch_harmonics(signal, freq, sample_frequency):\n",
        "    for f in range(freq, sample_frequency//2, freq):\n",
        "        signal = notch(signal, f, sample_frequency)\n",
        "    return signal\n",
        "\n",
        "def filter_signal(signals, fs):\n",
        "    \"\"\" signals is 2d: time, channels \"\"\"\n",
        "    result = np.zeros_like(signals)\n",
        "    for i in range(signals.shape[1]):\n",
        "        x = signals[:,i]\n",
        "        x = notch_harmonics(x, 60, fs)\n",
        "        x = remove_drift(x, fs)\n",
        "        result[:,i] = x\n",
        "    return result\n",
        "\n",
        "def get_last_sequence(chunk_list, n, k, do_filtering, fs):\n",
        "    cumulative_size = 0\n",
        "    selected_chunks = [np.zeros((0,k))]\n",
        "    for chunk in reversed(chunk_list):\n",
        "        selected_chunks.append(chunk)\n",
        "        cumulative_size += chunk.shape[0]\n",
        "        if cumulative_size > n:\n",
        "            break\n",
        "    selected_chunks.reverse()\n",
        "    result = np.concatenate(selected_chunks, 0)[-n:,:]\n",
        "    if do_filtering and result.shape[0] > 12:\n",
        "        result = filter_signal(result, fs)\n",
        "\n",
        "    if result.shape[0] < n:\n",
        "        result_padded = np.concatenate([np.zeros((n-result.shape[0],result.shape[1])), result], 0)\n",
        "    else:\n",
        "        result_padded = result\n",
        "    return result_padded\n",
        "\n",
        "\n",
        "time000=perf_counter()\n",
        "last_frame_sg2=0\n",
        "\n",
        "\n",
        "class Recorder(object):\n",
        "    def __init__(self, debug=False, display=True, num_channels=None, wifi=True):\n",
        "        # make audio stream\n",
        "\n",
        "#        self.audio_stream = sd.InputStream(device=None, channels=1, samplerate=16000)\n",
        "\n",
        "        # make emg stream\n",
        "\n",
        "        params = BrainFlowInputParams()\n",
        "        if debug:\n",
        "            board_id = -1 # synthetic\n",
        "            self.sample_rate = 256\n",
        "        else:\n",
        "            board_id = BoardIds.FREEEEG32_BOARD.value\n",
        "            params.serial_port = '/dev/ttyS11'\n",
        "            self.sample_rate = 512\n",
        "        self.emg_channels = BoardShim.get_emg_channels(board_id)\n",
        "        if num_channels is not None:\n",
        "            self.emg_channels = self.emg_channels[:num_channels]\n",
        "\n",
        "        #board = BoardShim(board_id, params)\n",
        "        global board\n",
        "        board.release_all_sessions()\n",
        "\n",
        "        board.prepare_session()\n",
        "        board.start_stream()\n",
        "        self.board = board\n",
        "\n",
        "        # config and make data holders\n",
        "\n",
        "        audio_multiplier = int(16000/self.sample_rate)\n",
        "        self.window = self.sample_rate*5\n",
        "\n",
        "        self.audio_data = []\n",
        "        self.emg_data = []\n",
        "        self.button_data = []\n",
        "\n",
        "        self.debug = debug\n",
        "        self.previous_sample_number = -1\n",
        "\n",
        "        # plot setup\n",
        "\n",
        "        self.display = display\n",
        "        if display:\n",
        "            print('init')\n",
        "            #plt.ion()\n",
        "            plt.figure()\n",
        "            fig, (audio_ax, emg_ax) = plt.subplots(2)\n",
        "            #audio_ax.axis((0, window*audio_multiplier, -1, 1))\n",
        "            audio_ax.axis((0, self.window, -300, 300))\n",
        "            emg_ax.axis((0, self.window, -300, 300))\n",
        "            #audio_lines = audio_ax.plot(np.zeros(window*audio_multiplier))\n",
        "            #audio_lines = audio_ax.plot(np.zeros(window),len(self.emg_channels))\n",
        "            emg_lines = emg_ax.plot(np.zeros((self.window,len(self.emg_channels))))\n",
        "            for l,c in zip(emg_lines, ['grey', 'mediumpurple', 'blue', 'green', 'yellow', 'orange', 'red', 'sienna']):\n",
        "                l.set_color(c)\n",
        "            text = emg_ax.text(50,-250,'RMS: 0')\n",
        "\n",
        "            for ax in (audio_ax, emg_ax):\n",
        "                ax.set_yticks([0])\n",
        "                ax.yaxis.grid(True)\n",
        "                ax.tick_params(bottom=False, top=False, labelbottom=False,\n",
        "                               right=False, left=False, labelleft=False)\n",
        "            #self.fig.tight_layout(pad=0)\n",
        "            plt.close('all')\n",
        "\n",
        "            def update_plot(frame):\n",
        "                \"\"\" This is called by matplotlib for each plot update. \"\"\"\n",
        "#                audio_to_plot = get_last_sequence(self.audio_data, window*audio_multiplier, 1, False, sample_rate)\n",
        "#                audio_to_plot = audio_to_plot.squeeze(1)\n",
        "#                audio_lines[0].set_ydata(audio_to_plot)\n",
        "\n",
        "                emg_to_plot = get_last_sequence(self.emg_data, self.window, len(self.emg_channels), True, self.sample_rate)\n",
        "                for column, line in enumerate(emg_lines):\n",
        "                    line.set_ydata(emg_to_plot[:, column])\n",
        "                text.set_text('RMS: '+str(emg_to_plot[-self.sample_rate*2:-self.sample_rate//2].std()))\n",
        "                return emg_lines\n",
        "                #return audio_lines + emg_lines\n",
        "\n",
        "            #self.ani = FuncAnimation(self.fig, update_plot, interval=30)\n",
        "\n",
        "    def update(self):\n",
        "        global output_idx, book\n",
        "        #if self.display:\n",
        "            # next two lines seem to be a better alternative to plt.pause(0.005)\n",
        "            # https://github.com/matplotlib/matplotlib/issues/11131\n",
        "        #    plt.gcf().canvas.draw_idle()\n",
        "        #    plt.gcf().canvas.start_event_loop(0.005)\n",
        "        #else:\n",
        "        #    time.sleep(0.005)\n",
        "\n",
        "        current_audio = []\n",
        "        #print(self.board.get_board_data_count())\n",
        "        while self.board.get_board_data_count() > 0: # because stream.read_available seems to max out, leading us to not read enough with one read\n",
        "            data = self.board.get_board_data()\n",
        "            #print(data)\n",
        "            #assert not overflowed\n",
        "            current_audio.append(data)\n",
        "        if len(current_audio) > 0:\n",
        "            ##self.audio_data.append(np.concatenate(current_audio,0))\n",
        "            #self.audio_data.append(data[self.emg_channels,0].T)\n",
        "            #print(len(data[0]))\n",
        "            self.audio_data.append(np.zeros(int(len(data[0])*(16000/self.sample_rate))))\n",
        "            #data = self.board.get_board_data() # get all data and remove it from internal buffer\n",
        "            self.emg_data.append(data[self.emg_channels,:].T)\n",
        "            #print('update:', self.emg_data)\n",
        "\n",
        "            if True:\n",
        "#            if not self.debug:\n",
        "                for sn in data[0,:]:\n",
        "                    if self.previous_sample_number != -1 and sn != (self.previous_sample_number+1)%256:\n",
        "                        print(f'skip from {self.previous_sample_number} to {sn}')\n",
        "                    self.previous_sample_number = sn\n",
        "\n",
        "                is_digital_inputs = data[12,:] == 193\n",
        "                button_data = data[16,is_digital_inputs].astype(bool)\n",
        "                self.button_data.append(button_data)\n",
        "                if sum(button_data) != 0:\n",
        "                    print('button pressed')\n",
        "\n",
        "            time100=perf_counter()\n",
        "            global time000, last_frame_sg2, fps_sg2\n",
        "            this_frame_sg2=int((time100-time000)*fps_sg2)\n",
        "            send_sg2=False\n",
        "            if this_frame_sg2>last_frame_sg2:\n",
        "              last_frame_sg2=this_frame_sg2\n",
        "              send_sg2=True\n",
        "            if send_sg2:\n",
        "\n",
        "\n",
        "              if True:\n",
        "                data1 = self.get_data()\n",
        "\n",
        "                output_idx=0\n",
        "                book.current_index=0\n",
        "\n",
        "                save_data(output_idx, data1, book)\n",
        "\n",
        "                return [True]\n",
        "\n",
        "              plt.figure()\n",
        "              #print('plt.figure()')\n",
        "              fig, (audio_ax, emg_ax) = plt.subplots(2)\n",
        "              #audio_ax.axis((0, window*audio_multiplier, -1, 1))\n",
        "              audio_ax.axis((0, self.window, -300, 300))\n",
        "              emg_ax.axis((0, self.window, -300, 300))\n",
        "              #audio_lines = audio_ax.plot(np.zeros(window*audio_multiplier))\n",
        "              #audio_lines = audio_ax.plot(np.zeros(window),len(self.emg_channels))\n",
        "              emg_lines = emg_ax.plot(np.zeros((self.window,len(self.emg_channels))))\n",
        "              for l,c in zip(emg_lines, ['grey', 'mediumpurple', 'blue', 'green', 'yellow', 'orange', 'red', 'sienna']):\n",
        "                l.set_color(c)\n",
        "              text = emg_ax.text(50,-250,'RMS: 0')\n",
        "\n",
        "              for ax in (audio_ax, emg_ax):\n",
        "                ax.set_yticks([0])\n",
        "                ax.yaxis.grid(True)\n",
        "                ax.tick_params(bottom=False, top=False, labelbottom=False,\n",
        "                               right=False, left=False, labelleft=False)\n",
        "\n",
        "              emg_to_plot = get_last_sequence(self.emg_data, self.window, len(self.emg_channels), True, self.sample_rate)\n",
        "              for column, line in enumerate(emg_lines):\n",
        "                  line.set_ydata(emg_to_plot[:, column])\n",
        "              text.set_text('RMS: '+str(emg_to_plot[-self.sample_rate*2:-self.sample_rate//2].std()))                               \n",
        "\n",
        "              buf2 = BytesIO()\n",
        "              buf2.seek(0)\n",
        "              #self.fig\n",
        "              plt.savefig(buf2, format='png')\n",
        "              #plt.show()\n",
        "              myimage=buf2.getvalue()\n",
        "              plt.close('all')\n",
        "              #print('plt.close()')\n",
        "              buf2.close()\n",
        "              #msg['buffers']=\n",
        "\n",
        "              if True:\n",
        "                data1 = self.get_data()\n",
        "\n",
        "                output_idx=0\n",
        "                book.current_index=0\n",
        "\n",
        "                save_data(output_idx, data1, book)\n",
        "\n",
        "#                if output_idx == 0:\n",
        "#                  save_data(output_idx, data1, None)\n",
        "#                else:\n",
        "#                  save_data(output_idx, data1, book)\n",
        "#                  book.next()\n",
        "#\n",
        "#                output_idx += 1\n",
        "#                display_sentence(book.current_sentence())#, text_win)\n",
        "\n",
        "              return [memoryview(myimage)]\n",
        "            else:\n",
        "              return []\n",
        "\n",
        "    def get_data(self):\n",
        "        #print('get_data:', self.emg_data)\n",
        "        emg = np.concatenate(self.emg_data, 0)\n",
        "        audio = np.concatenate(self.audio_data, 0)\n",
        "        #audio = np.concatenate(self.audio_data, 0).squeeze(1)\n",
        "        button = np.concatenate(self.button_data, 0)\n",
        "        chunk_sizes = [(e.shape[0],a.shape[0],b.shape[0]) for e, a, b in zip(self.emg_data, self.audio_data, self.button_data)]\n",
        "        self.emg_data = []\n",
        "        self.audio_data = []\n",
        "        self.button_data = []\n",
        "        return emg, audio, button, chunk_sizes\n",
        "\n",
        "    def __enter__(self):\n",
        "#        self.audio_stream.start()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "#        self.audio_stream.stop()\n",
        "#        self.audio_stream.close()\n",
        "\n",
        "        #self.board.stop_stream()\n",
        "        #self.board.release_session()\n",
        "        #print('plt.close()')\n",
        "        #plt.close()\n",
        "        return 0\n",
        "\n",
        "if True:\n",
        "    r= Recorder(debug=False, display=True, wifi=False, num_channels=8)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxupESH8TxGZ"
      },
      "outputs": [],
      "source": [
        "def target_func1(comm, msg):\n",
        "  global ser, master, board, r\n",
        "\n",
        "  # To Write to the device\n",
        "  msg_buffers_0_tobytes = msg['buffers'][0].tobytes()\n",
        "  msg['buffers']=[]\n",
        "  #print(msg_buffers_0_tobytes)\n",
        "#  print(len(msg_buffers_0_tobytes))\n",
        "  ser.write(msg_buffers_0_tobytes)\n",
        "  \n",
        "#  print(len(msg_buffers_0_tobytes))\n",
        "  # To read from the device\n",
        "#  os.read(master,len(msg_buffers_0_tobytes))\n",
        "\n",
        "#  time.sleep(10)\n",
        "  if False:\n",
        "    data = board.get_board_data()\n",
        "    print(data)\n",
        "#  board.stop_stream()\n",
        "#  board.release_session()\n",
        "\n",
        "  if True:\n",
        "   msg['buffers']=r.update()\n",
        "   encoded=''\n",
        "   #print(len(msg['buffers']))\n",
        "   if len(msg['buffers'])>0:\n",
        "    msg['buffers']=[]\n",
        "#  if True:\n",
        "    global FLAGS\n",
        "    os.makedirs(FLAGS.output_directory, exist_ok=True)\n",
        "    logging.basicConfig(handlers=[\n",
        "            logging.FileHandler(os.path.join(FLAGS.output_directory, 'eval_log.txt'), 'w'),\n",
        "            logging.StreamHandler()\n",
        "            ], level=logging.INFO, format=\"%(message)s\")\n",
        "\n",
        "    testset = EMGDataset(test=True)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'\n",
        "\n",
        "    models = []\n",
        "    for fname in FLAGS.models:\n",
        "#        state_dict = torch.load(fname, map_location=torch.device('cpu'))\n",
        "        state_dict = torch.load(fname)\n",
        "        n_sess = 1 if FLAGS.no_session_embed else state_dict[\"session_emb.weight\"].size(0)\n",
        "        model = Model(testset.num_features, testset.num_speech_features, len(phoneme_inventory), n_sess).to(device)\n",
        "        model.load_state_dict(state_dict)\n",
        "        models.append(model)\n",
        "    ensemble = EnsembleModel(models)\n",
        "\n",
        "    _, _, confusion = test(ensemble, testset, device)\n",
        "    print_confusion(confusion)\n",
        "\n",
        "    for i1, datapoint in enumerate(testset):\n",
        "     #if i == 0:\n",
        "#        save_output(ensemble, datapoint, os.path.join(FLAGS.output_directory, f'example_output_{i}.wav'), device)\n",
        "      model = ensemble\n",
        "      #datapoint\n",
        "      filename = os.path.join(FLAGS.output_directory, f'example_output_{i1}.wav')\n",
        "      #device\n",
        "      gold_mfcc=False\n",
        "\n",
        "      model.eval()\n",
        "      if gold_mfcc:\n",
        "          y = datapoint['audio_features']\n",
        "      else:\n",
        "        with torch.no_grad():\n",
        "            sess = torch.tensor(datapoint['session_ids'], device=device).unsqueeze(0)\n",
        "            X1 = torch.tensor(datapoint['emg'], dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            X_raw = torch.tensor(datapoint['raw_emg'], dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "            pred, _ = model(X1, X_raw, sess)\n",
        "            pred = pred.squeeze(0)\n",
        "\n",
        "            y = pred.cpu().detach().numpy()\n",
        "\n",
        "      wavenet_model = WavenetModel(y.shape[1]).to(device)\n",
        "      assert FLAGS.pretrained_wavenet_model is not None\n",
        "      wavenet_model.load_state_dict(torch.load(FLAGS.pretrained_wavenet_model))\n",
        "#      wavenet_model.load_state_dict(torch.load(FLAGS.pretrained_wavenet_model, map_location=torch.device('cpu')))\n",
        "\n",
        "      #save_wavenet_output(wavenet_model, y, filename, device)\n",
        "      \n",
        "      #wavenet_model\n",
        "      input_data = y\n",
        "      #filename\n",
        "      #device\n",
        "      \n",
        "      wavenet_model.eval()\n",
        "\n",
        "      assert len(input_data.shape) == 2\n",
        "      X = torch.tensor(input_data, dtype=torch.float32).to(device).unsqueeze(0)\n",
        "\n",
        "      wavenet = wavenet_model.wavenet\n",
        "      inference_wavenet = NVWaveNet(**wavenet.export_weights())\n",
        "  #    inference_wavenet = nv_wavenet.NVWaveNet(**wavenet.export_weights())\n",
        "      cond_input = wavenet_model.pre_wavenet_processing(X)\n",
        "\n",
        "      chunk_len = 400\n",
        "      overlap = 1\n",
        "      audio_chunks = []\n",
        "      for i in range(0, cond_input.size(2), chunk_len-overlap):\n",
        "          if cond_input.size(2)-i < overlap:\n",
        "              break # don't make segment at end that doesn't go past overlapped part\n",
        "          cond_chunk = cond_input[:,:,i:i+chunk_len]\n",
        "          wavenet_cond_input = wavenet.get_cond_input(cond_chunk)\n",
        "          audio_data = inference_wavenet.infer(wavenet_cond_input, nv_wavenet.Impl.SINGLE_BLOCK)\n",
        "          audio_chunk = librosa.core.mu_expand(audio_data.squeeze(0).cpu().numpy()-128, 255, True)\n",
        "          audio_chunks.append(audio_chunk)\n",
        "\n",
        "      audio_out = splice_audio(audio_chunks, overlap*160)\n",
        "  \n",
        "      sf.write(filename, audio_out, 16000)\n",
        "\n",
        "      if True:\n",
        "                    buffer = BytesIO()\n",
        "                    if generate&gen_mp3:\n",
        "                      buffer_wav = BytesIO()\n",
        "                      sf.write(buffer_wav, audio_out, 16000, format='wav')\n",
        "                      AudioSegment.from_wav(buffer_wav).export(buffer, format=\"mp3\")\n",
        "                    if generate&gen_wav:\n",
        "                      sf.write(buffer, audio_out, 16000, format='wav')\n",
        " \n",
        "                    buffer.seek(0)\n",
        "                    mysound = buffer.getvalue()\n",
        "                    msg['buffers']=[]\n",
        "                    #msg['buffers']=[memoryview(mysound)]\n",
        "                    if generate&gen_mp3:\n",
        "                      encoded= \"data:audio/mp3;base64,\"+base64.b64encode(mysound).decode()\n",
        "                    if generate&gen_wav:\n",
        "                      encoded= \"data:audio/wav;base64,\"+base64.b64encode(mysound).decode()\n",
        "                    #print('audio encoded')\n",
        "       \n",
        "\n",
        "      #wavenet_model.train()\n",
        "\n",
        "      #model.train()\n",
        "\n",
        "#    evaluate(testset, FLAGS.output_directory)\n",
        "\n",
        "\n",
        "  if False:\n",
        "#    with Recorder(debug=False, display=True, wifi=False, num_channels=1) as r:\n",
        "#    with Recorder(debug=True, display=False, wifi=False, num_channels=1) as r:\n",
        "#        while True:\n",
        "            msg['buffers']=r.update()\n",
        "            if len(msg['buffers'])>0:\n",
        "              encoded= \"binary:data:image/png\"\n",
        "            else:\n",
        "              encoded=''\n",
        "\n",
        "  if True:\n",
        "    comm.send({\n",
        "      'response':  encoded,\n",
        "#      'response': 'close',\n",
        "      }, None, msg['buffers']);\n",
        "\n",
        "if False:\n",
        "  eeg_channels = BoardShim.get_eeg_channels(BoardIds.FREEEEG32_BOARD.value)\n",
        "  eeg_data = data[eeg_channels, :]\n",
        "  eeg_data = eeg_data / 1000000 # BrainFlow returns uV, convert to V for MNE\n",
        "\n",
        "  # Creating MNE objects from brainflow data arrays\n",
        "  ch_types = ['eeg'] * len(eeg_channels)\n",
        "  ch_names = [str(x) for x in range(len(eeg_channels))]\n",
        "  #ch_names = BoardShim.get_eeg_names(BoardIds.FREEEEG32_BOARD.value)\n",
        "  sfreq = BoardShim.get_sampling_rate(BoardIds.FREEEEG32_BOARD.value)\n",
        "  info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
        "  raw = mne.io.RawArray(eeg_data, info)\n",
        "  # its time to plot something!\n",
        "  raw.plot_psd(average=False)  \n",
        "\n",
        "  if True:\n",
        "    comm.send({\n",
        "      'response': 'close',\n",
        "      }, None, msg['buffers']);\n",
        " \n",
        "get_ipython().kernel.comm_manager.register_target('comm_target1', target_func1)\n",
        " \n",
        "Javascript('''\n",
        "//Joshua Brewster, GPL (copyleft)\n",
        " \n",
        "//import 'regenerator-runtime/runtime' //For async functions on node\\\\\n",
        " \n",
        " class eeg32 { //Contains structs and necessary functions/API calls to analyze serial data for the FreeEEG32\n",
        " \n",
        "    constructor(\n",
        "        onDecodedCallback = this.onDecodedCallback,\n",
        "        onConnectedCallback = this.onConnectedCallback,\n",
        "        onDisconnectedCallback = this.onDisconnectedCallback,\n",
        "        CustomDecoder = this.decode,\n",
        "        //baudrate = 1500000//115200\n",
        "        baudrate = 921600//115200\n",
        "        ) {\n",
        " \n",
        "        this.onDecodedCallback = onDecodedCallback;\n",
        "        this.onConnectedCallback = onConnectedCallback;\n",
        "        this.onDisconnectedCallback = onDisconnectedCallback;\n",
        "        this.decode = CustomDecoder;\n",
        "        //Free EEG 32 data structure:\n",
        "        \n",
        "        //    [stop byte, start byte, counter byte, 32x3 channel data bytes (24 bit), 3x2 accelerometer data bytes, stop byte, start byte...] Gyroscope not enabled yet but would be printed after the accelerometer..\n",
        "        //    Total = 105 bytes/line\n",
        "        \n",
        "        this.connected = false;\n",
        "        this.subscribed = false;\n",
        "        this.buffer = [];\n",
        "        this.startByte = 160; // Start byte value\n",
        "        this.stopByte = 192; // Stop byte value\n",
        "        this.searchString = new Uint8Array([this.stopByte,this.startByte]); //Byte search string\n",
        "        this.readRate = 16.666667; //Throttle EEG read speed. (1.953ms/sample min @103 bytes/line)\n",
        "        this.nChannels=%(data_channels)d;\n",
        "        if(this.nChannels==128)\n",
        "        {\n",
        "          this.readBufferSize = 8000; //Serial read buffer size, increase for slower read speeds (~1030bytes every 20ms) to keep up with the stream (or it will crash)\n",
        "        }\n",
        "        else\n",
        "        {\n",
        "          this.readBufferSize = 8000; //Serial read buffer size, increase for slower read speeds (~1030bytes every 20ms) to keep up with the stream (or it will crash)\n",
        "//        this.readBufferSize = 4000; //Serial read buffer size, increase for slower read speeds (~1030bytes every 20ms) to keep up with the stream (or it will crash)\n",
        "          //this.readBufferSize = 1000; //Serial read buffer size, increase for slower read speeds (~1030bytes every 20ms) to keep up with the stream (or it will crash)\n",
        "          //this.readBufferSize = 2000; //Serial read buffer size, increase for slower read speeds (~1030bytes every 20ms) to keep up with the stream (or it will crash)\n",
        "        }\n",
        "\n",
        "        //this.sps = 512; // Sample rate\n",
        "        //this.sps = 250; // Sample rate\n",
        "        this.sps=%(sfreq)d;\n",
        "        //this.sps=%(sfreq)f;\n",
        "        //this.nChannels = 128;\n",
        "        //this.nChannels = 32;\n",
        "        this.generate_game=%(generate_game)d;\n",
        "        this.generate_game_mode1=%(generate_game_mode1)d;\n",
        "        this.generate_game_mode3=%(generate_game_mode3)d;\n",
        "        \n",
        "        this.nPeripheralChannels = 6; // accelerometer and gyroscope (2 bytes * 3 coordinates each)\n",
        "        this.updateMs = 1000/this.sps; //even spacing\n",
        "        this.stepSize = 1/Math.pow(2,24);\n",
        "        //this.vref = 2.50; //2.5V voltage ref +/- 250nV\n",
        "        //this.gain = 8;\n",
        "        //this.vref = 1.25; //2.5V voltage ref +/- 250nV\n",
        "        //this.gain = 32;\n",
        "        this.vref =%(vref)f; //2.5V voltage ref +/- 250nV\n",
        "        this.gain = %(gain)d;\n",
        " \n",
        "        this.vscale = (this.vref/this.gain)*this.stepSize; //volts per step.\n",
        "        this.uVperStep = 1000000 * ((this.vref/this.gain)*this.stepSize); //uV per step.\n",
        "        this.scalar = 1/(1000000 / ((this.vref/this.gain)*this.stepSize)); //steps per uV.\n",
        " \n",
        "        this.maxBufferedSamples = this.sps*60*2; //max samples in buffer this.sps*60*nMinutes = max minutes of data\n",
        "        \n",
        "        this.data = { //Data object to keep our head from exploding. Get current data with e.g. this.data.A0[this.data.count-1]\n",
        "            count: 0,\n",
        "            startms: 0,\n",
        "            ms: [],\n",
        "            'A0': [],'A1': [],'A2': [],'A3': [],'A4': [],'A5': [],'A6': [],'A7': [], //ADC 0\n",
        "            'A8': [],'A9': [],'A10': [],'A11': [],'A12': [],'A13': [],'A14': [],'A15': [], //ADC 1\n",
        "            'A16': [],'A17': [],'A18': [],'A19': [],'A20': [],'A21': [],'A22': [],'A23': [], //ADC 2\n",
        "            'A24': [],'A25': [],'A26': [],'A27': [],'A28': [],'A29': [],'A30': [],'A31': [], //ADC 3\n",
        "            //'A0': [],'A1': [],'A2': [],'A3': [],'A4': [],'A5': [],'A6': [],'A7': [], //ADC 0\n",
        "            //'A8': [],'A9': [],'A10': [],'A11': [],'A12': [],'A13': [],'A14': [],'A15': [], //ADC 1\n",
        "            //'A16': [],'A17': [],'A18': [],'A19': [],'A20': [],'A21': [],'A22': [],'A23': [], //ADC 2\n",
        "            //'A24': [],'A25': [],'A26': [],'A27': [],'A28': [],'A29': [],'A30': [],'A31': [], //ADC 3\n",
        "            //'A0': [],'A1': [],'A2': [],'A3': [],'A4': [],'A5': [],'A6': [],'A7': [], //ADC 0\n",
        "            //'A8': [],'A9': [],'A10': [],'A11': [],'A12': [],'A13': [],'A14': [],'A15': [], //ADC 1\n",
        "            //'A16': [],'A17': [],'A18': [],'A19': [],'A20': [],'A21': [],'A22': [],'A23': [], //ADC 2\n",
        "            //'A24': [],'A25': [],'A26': [],'A27': [],'A28': [],'A29': [],'A30': [],'A31': [], //ADC 3\n",
        "            //'A0': [],'A1': [],'A2': [],'A3': [],'A4': [],'A5': [],'A6': [],'A7': [], //ADC 0\n",
        "            //'A8': [],'A9': [],'A10': [],'A11': [],'A12': [],'A13': [],'A14': [],'A15': [], //ADC 1\n",
        "            //'A16': [],'A17': [],'A18': [],'A19': [],'A20': [],'A21': [],'A22': [],'A23': [], //ADC 2\n",
        "            //'A24': [],'A25': [],'A26': [],'A27': [],'A28': [],'A29': [],'A30': [],'A31': [], //ADC 3\n",
        "            'Ax': [], 'Ay': [], 'Az': [], 'Gx': [], 'Gy': [], 'Gz': []  //Peripheral data (accelerometer, gyroscope)\n",
        "        };\n",
        " \n",
        "    this.bufferednewLines = 0;\n",
        "    this.data_slice=[];\n",
        "    this.data_slice_size=this.sps*(5*1/8+0.1);\n",
        "    this.ready_to_send_data = false;\n",
        "    this.data_send_count=0;\n",
        "\n",
        "    this.generate_parallel=%(generate_parallel)d;\n",
        "    \n",
        "    this.xsize=%(xsize)d;\n",
        "    this.ysize=%(ysize)d;\n",
        "\n",
        "    this.generate_stylegan2=true;\n",
        "    this.generate_stylegan2=%(generate_stylegan2)d;\n",
        "    \n",
        "    //this.generate_stylegan2=false;\n",
        "    this.generate_wavegan=true;\n",
        "    this.generate_wavegan=%(generate_wavegan)d;\n",
        "    this.generate_heatmap=true;\n",
        "    this.generate_heatmap=%(generate_heatmap)d;\n",
        "    //data:audio/wav;base64,\n",
        "    //data:image/jpeg;base64,\n",
        "    this.time100=Date.now();\n",
        "    this.time000=Date.now();\n",
        "    this.this_frame_wg=-1;\n",
        "    this.last_frame_wg=-1;\n",
        "    this.send_wg=false;\n",
        "    this.this_frame_sg2=-1;\n",
        "    this.last_frame_sg2=-1;\n",
        "    this.send_sg2=false;\n",
        "\n",
        "    this.frame_last=0;\n",
        "\n",
        "    if(this.generate_stylegan2)\n",
        "    {\n",
        "      this.fps_sg2=1;\n",
        "    }\n",
        "    if(this.generate_wavegan)\n",
        "    {\n",
        "      this.hz=44100;\n",
        "      this.fps_wg=this.hz/(32768*2);\n",
        "      //this.fps=this.hz/(32768);\n",
        "    }\n",
        "      this.fps_sg2=this.fps_wg;\n",
        "      this.fps_wg=%(fps_wg)f;\n",
        "      this.fps_sg2=%(fps_sg2)f;\n",
        "      this.fps_hm=%(fps_hm)f;\n",
        "      this.fps=Math.max(this.fps_wg,this.fps_sg2,this.fps_hm)*4*4;\n",
        "      //this.fps=Math.max(this.fps_wg,this.fps_sg2,this.fps_hm)*5;\n",
        "      //this.fps=10;\n",
        "      \n",
        "    this.samples_count=0;\n",
        "    //this.channel=None;\n",
        " \n",
        "        this.resetDataBuffers();\n",
        " \n",
        "        //navigator.serial utils\n",
        "        if(!navigator.serial){\n",
        "            console.error(\"`navigator.serial not found! Enable #enable-experimental-web-platform-features in chrome://flags (search 'experimental')\")\n",
        "        }\n",
        "        this.port = null;\n",
        "        this.reader = null;\n",
        "        this.baudrate = baudrate;\n",
        " \n",
        "    }\n",
        "    \n",
        "    resetDataBuffers(){\n",
        "        this.data.count = 0;\n",
        "        this.data.startms = 0;\n",
        "        for(const prop in this.data) {\n",
        "            if(typeof this.data[prop] === \"object\"){\n",
        "                this.data[prop] = new Array(this.maxBufferedSamples).fill(0);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        " \n",
        "    setScalar(gain=24,stepSize=1/(Math.pow(2,23)-1),vref=4.50) {\n",
        "        this.stepSize = stepSize;\n",
        "        this.vref = vref; //2.5V voltage ref +/- 250nV\n",
        "        this.gain = gain;\n",
        " \n",
        "        this.vscale = (this.vref/this.gain)*this.stepSize; //volts per step.\n",
        "        this.uVperStep = 1000000 * ((this.vref/this.gain)*this.stepSize); //uV per step.\n",
        "        this.scalar = 1/(1000000 / ((this.vref/this.gain)*this.stepSize)); //steps per uV.\n",
        "    }\n",
        " \n",
        "    getLatestData(channel=\"A0\",count=1) { //Return slice of specified size of the latest data from the specified channel\n",
        "        let ct = count;\n",
        "        if(ct <= 1) {\n",
        "            return [this.data[channel][this.data.count-1]];\n",
        "        }\n",
        "        else {\n",
        "            if(ct > this.data.count) {\n",
        "                ct = this.data.count;\n",
        "            }\n",
        "            return this.data[channel].slice(this.data.count-ct,this.data.count);\n",
        "        }\n",
        "    }\n",
        " \n",
        "    bytesToInt16(x0,x1){\n",
        "        return x0 * 256 + x1;\n",
        "    }\n",
        " \n",
        "    int16ToBytes(y){ //Turns a 24 bit int into a 3 byte sequence\n",
        "        return [y & 0xFF , (y >> 8) & 0xFF];\n",
        "    }\n",
        " \n",
        "    bytesToInt24(x0,x1,x2){ //Turns a 3 byte sequence into a 24 bit int\n",
        "        return x0 * 65536 + x1 * 256 + x2;\n",
        "    }\n",
        " \n",
        "    int24ToBytes(y){ //Turns a 24 bit int into a 3 byte sequence\n",
        "        return [y & 0xFF , (y >> 8) & 0xFF , (y >> 16) & 0xFF];\n",
        "    }\n",
        " \n",
        "    decode(buffer = this.buffer) { //returns true if successful, returns false if not\n",
        " \n",
        "        var needle = this.searchString\n",
        "        var haystack = buffer;\n",
        "        var search = this.boyerMoore(needle);\n",
        "        var skip = search.byteLength;\n",
        "        var indices = [];\n",
        "        let newLines = 0;\n",
        "    \n",
        "        for (var i = search(haystack); i !== -1; i = search(haystack, i + skip)) {\n",
        "            indices.push(i);\n",
        "        }\n",
        "        //console.log(indices);\n",
        "        if(indices.length >= 2){\n",
        "            for(let k = 1; k < indices.length; k++) {\n",
        "                if(indices[k] - indices[k-1] !== 105) {\n",
        "                    \n",
        "                } //This is not a valid sequence going by size, drop sequence and return\n",
        "                else {\n",
        "                    var line = buffer.slice(indices[k-1],indices[k]+1); //Splice out this line to be decoded\n",
        "                    \n",
        "                    // line[0] = stop byte, line[1] = start byte, line[2] = counter, line[3:99] = ADC data 32x3 bytes, line[100-104] = Accelerometer data 3x2 bytes\n",
        " \n",
        "                    //line found, decode.\n",
        "                    if(this.data.count < this.maxBufferedSamples){\n",
        "                        this.data.count++;\n",
        "                    }\n",
        " \n",
        "                    if(this.data.count-1 === 0) {this.data.ms[this.data.count-1]= Date.now(); this.data.startms = this.data.ms[0];}\n",
        "                    else {\n",
        "                        this.data.ms[this.data.count-1]=this.data.ms[this.data.count-2]+this.updateMs;\n",
        "                        \n",
        "                        if(this.data.count >= this.maxBufferedSamples) {\n",
        "                            this.data.ms.splice(0,5120);\n",
        "                            this.data.ms.push(new Array(5120).fill(0));\n",
        "                        }\n",
        "                    }//Assume no dropped samples\n",
        "                  var sample_count = line[2];\n",
        "                  var sample_count_diff = sample_count-this.samples_count;\n",
        "          if(sample_count_diff<0){\n",
        "            sample_count_diff+=256;\n",
        "          }\n",
        "          if(sample_count_diff!=1)\n",
        "          {\n",
        "            console.error(\"dropped samples:\"+sample_count_diff.toString());\n",
        "          }\n",
        "          this.samples_count=sample_count;\n",
        " \n",
        "                    for(var i = 3; i < 99; i+=3) {\n",
        "                        var channel = \"A\"+(i-3)/3;\n",
        "                        this.data[channel][this.data.count-1]=this.bytesToInt24(line[i],line[i+1],line[i+2]);\n",
        "                        if(this.data.count >= this.maxBufferedSamples) { \n",
        "                            this.data[channel].splice(0,5120);\n",
        "                            this.data[channel].push(new Array(5120).fill(0));//shave off the last 10 seconds of data if buffer full (don't use shift())\n",
        "                        }\n",
        "                            //console.log(this.data[channel][this.data.count-1],indices[k], channel)\n",
        "                    }\n",
        " \n",
        "                    this.data[\"Ax\"][this.data.count-1]=this.bytesToInt16(line[99],line[100]);\n",
        "                    this.data[\"Ay\"][this.data.count-1]=this.bytesToInt16(line[101],line[102]);\n",
        "                    this.data[\"Az\"][this.data.count-1]=this.bytesToInt16(line[103],line[104]);\n",
        " \n",
        "                    \n",
        "                    if(this.data.count >= this.maxBufferedSamples) { \n",
        "                        this.data[\"Ax\"].splice(0,5120);\n",
        "                        this.data[\"Ay\"].splice(0,5120);\n",
        "                        this.data[\"Az\"].splice(0,5120);\n",
        "                        this.data[\"Ax\"].push(new Array(5120).fill(0))\n",
        "                        this.data[\"Ay\"].push(new Array(5120).fill(0))\n",
        "                        this.data[\"Az\"].push(new Array(5120).fill(0))\n",
        "                        this.data.count -= 5120;\n",
        "                    }\n",
        "                    //console.log(this.data)\n",
        "                    newLines++;\n",
        "                    //console.log(indices[k-1],indices[k])\n",
        "                    //console.log(buffer[indices[k-1],buffer[indices[k]]])\n",
        "                    //indices.shift();\n",
        "                }\n",
        "                \n",
        "            }\n",
        "            if(newLines > 0) buffer.splice(0,indices[indices.length-1]);\n",
        "   \n",
        "            return newLines;\n",
        "            //Continue\n",
        "        }\n",
        "        //else {this.buffer = []; return false;}\n",
        "    }\n",
        "    //Callbacks\n",
        "    onDecodedCallback(newLinesInt){\n",
        "        //console.log(\"new samples:\", newLinesInt);\n",
        "        this.bufferednewLines=this.bufferednewLines+newLinesInt;\n",
        "    }\n",
        " \n",
        "    onConnectedCallback() {\n",
        "        console.log(\"port connected!\");\n",
        "    }\n",
        " \n",
        "    onDisconnectedCallback() {\n",
        "        console.log(\"port disconnected!\");\n",
        "    }\n",
        " \n",
        "    onReceive(value){\n",
        "        this.buffer.push(...value);\n",
        "        let newLines=this.buffer.length;\n",
        "        this.onDecodedCallback(newLines);\n",
        "        \n",
        "        if(this.ready_to_send_data)\n",
        "        {\n",
        "          //this.sendserial(this.buffer);\n",
        "          //console.log(this.buffer.length)\n",
        "          //this.buffer=[];\n",
        "        }\n",
        "        //console.log(value.length)\n",
        "\n",
        " \n",
        "        //let newLines = this.decode(this.buffer);\n",
        "        //console.log(this.data)\n",
        "        //console.log(\"decoding... \", this.buffer.length)\n",
        "        //if(newLines !== false && newLines !== 0 && !isNaN(newLines) ) this.onDecodedCallback(newLines);\n",
        "    }\n",
        " \n",
        "    async onPortSelected(port,baud=this.baudrate) {\n",
        "        try{\n",
        "            try {\n",
        "                await port.open({ baudRate: baud, bufferSize: this.readBufferSize });\n",
        "                this.onConnectedCallback();\n",
        "                this.connected = true;\n",
        "                this.subscribed = true;\n",
        "                this.subscribe(port);//this.subscribeSafe(port);\n",
        "        \n",
        "            } //API inconsistency in syntax between linux and windows\n",
        "            catch {\n",
        "                await port.open({ baudrate: baud, buffersize: this.readBufferSize });\n",
        "                this.onConnectedCallback();\n",
        "                this.connected = true;\n",
        "                this.subscribed = true;\n",
        "                this.subscribe(port);//this.subscribeSafe(port);\n",
        "            }\n",
        "        }\n",
        "        catch(err){\n",
        "            console.log(err);\n",
        "            this.connected = false;\n",
        "        }\n",
        "    }\n",
        " \n",
        "    async subscribe(port){\n",
        "        if (this.port.readable && this.subscribed === true) {\n",
        "            this.reader = port.readable.getReader();\n",
        "            const streamData = async () => {\n",
        "                try {\n",
        "                    const { value, done } = await this.reader.read();\n",
        "                    if (done || this.subscribed === false) {\n",
        "                        // Allow the serial port to be closed later.\n",
        "                        await this.reader.releaseLock();\n",
        "                        \n",
        "                    }\n",
        "                    if (value) {\n",
        "                        //console.log(value.length);\n",
        "                        try{\n",
        "                            this.onReceive(value);\n",
        "                        }\n",
        "                        catch (err) {console.log(err)}\n",
        "                        //console.log(\"new Read\");\n",
        "                        //console.log(this.decoder.decode(value));\n",
        "                    }\n",
        "                    if(this.subscribed === true) {\n",
        "                        setTimeout(()=>{streamData();}, this.readRate);//Throttled read 1/512sps = 1.953ms/sample @ 103 bytes / line or 1030bytes every 20ms\n",
        "                    }\n",
        "                } catch (error) {\n",
        "                    console.log(error);// TODO: Handle non-fatal read error.\n",
        "                    if(error.message.includes('framing') || error.message.includes('overflow') || error.message.includes('Overflow') || error.message.includes('break')) {\n",
        "                        this.subscribed = false;\n",
        "                        setTimeout(async ()=>{\n",
        "                            try{\n",
        "                            if (this.reader) {\n",
        "                                await this.reader.releaseLock();\n",
        "                                this.reader = null;\n",
        "                            }\n",
        "                            } catch (er){ console.error(er);}\n",
        "                            this.subscribed = true; \n",
        "                            this.subscribe(port);\n",
        "                            //if that fails then close port and reopen it\n",
        "                        },30); //try to resubscribe \n",
        "                    } else if (error.message.includes('parity') || error.message.includes('Parity') || error.message.includes('overrun') ) {\n",
        "                        if(this.port){\n",
        "                            this.subscribed = false;\n",
        "                            setTimeout(async () => {\n",
        "                                try{\n",
        "                                if (this.reader) {\n",
        "                                    await this.reader.releaseLock();\n",
        "                                    this.reader = null;\n",
        "                                }\n",
        "                                await port.close();\n",
        "                                } catch (er){ console.error(er);}\n",
        "                                //this.port = null;\n",
        "                                this.connected = false;\n",
        "                                setTimeout(()=>{this.onPortSelected(this.port)},100); //close the port and reopen\n",
        "                            }, 50);\n",
        "                        }\n",
        "                    }\n",
        "                     else {\n",
        "                        this.closePort();   \n",
        "                    }   \n",
        "                }\n",
        "            }\n",
        "            streamData();\n",
        "        }\n",
        "    }\n",
        " \n",
        "    //Unfinished\n",
        "    async subscribeSafe(port) { //Using promises instead of async/await to cure hangs when the serial update does not meet tick requirements\n",
        "        var readable = new Promise((resolve,reject) => {\n",
        "            while(this.port.readable && this.subscribed === true){\n",
        "                this.reader = port.readable.getReader();\n",
        "                var looper = true;\n",
        "                var prom1 = new Promise((resolve,reject) => {\n",
        "                    return this.reader.read();\n",
        "                });\n",
        " \n",
        "                var prom2 = new Promise((resolve,reject) => {\n",
        "                    setTimeout(resolve,100,\"readfail\");\n",
        "                });\n",
        "                while(looper === true ) {\n",
        "                    //console.log(\"reading...\");\n",
        "                    Promise.race([prom1,prom2]).then((result) => {\n",
        "                        console.log(\"newpromise\")\n",
        "                        if(result === \"readfail\"){\n",
        "                            console.log(result);\n",
        "                        }\n",
        "                        else{\n",
        "                            const {value, done} = result;\n",
        "                            if(done === true || this.subscribed === true) { var donezo = new Promise((resolve,reject) => {\n",
        "                                resolve(this.reader.releaseLock())}).then(() => {\n",
        "                                    looper = false;\n",
        "                                    return;\n",
        "                                });\n",
        "                            }\n",
        "                            else{\n",
        "                                this.onReceive(value);\n",
        "                            }\n",
        "                        }\n",
        "                    });\n",
        "                }\n",
        "            }\n",
        "            resolve(\"not readable\");\n",
        "        });\n",
        "    }\n",
        " \n",
        "    async closePort(port=this.port) {\n",
        "        //if(this.reader) {this.reader.releaseLock();}\n",
        "        if(this.port){\n",
        "            this.subscribed = false;\n",
        "            setTimeout(async () => {\n",
        "                if (this.reader) {\n",
        "                    await this.reader.releaseLock();\n",
        "                    this.reader = null;\n",
        "                }\n",
        "                await port.close();\n",
        "                this.port = null;\n",
        "                this.connected = false;\n",
        "                this.onDisconnectedCallback();\n",
        "            }, 100);\n",
        "        }\n",
        "    }\n",
        " \n",
        "    async setupSerialAsync(baudrate=this.baudrate) { //You can specify baudrate just in case\n",
        " \n",
        "        const filters = [\n",
        "            { usbVendorId: 0x10c4, usbProductId: 0x0043 } //CP2102 filter (e.g. for UART via ESP32)\n",
        "        ];\n",
        " \n",
        "        this.port = await navigator.serial.requestPort();\n",
        "        navigator.serial.addEventListener(\"disconnect\",(e) => {\n",
        "            this.closePort(this.port);\n",
        "        });\n",
        "        this.onPortSelected(this.port,baudrate);\n",
        " \n",
        "        //navigator.serial.addEventListener(\"onReceive\", (e) => {console.log(e)});//this.onReceive(e));\n",
        " \n",
        "    }\n",
        " \n",
        " \n",
        "    //Boyer Moore fast byte search method copied from https://codereview.stackexchange.com/questions/20136/uint8array-indexof-method-that-allows-to-search-for-byte-sequences\n",
        "    asUint8Array(input) {\n",
        "        if (input instanceof Uint8Array) {\n",
        "            return input;\n",
        "        } else if (typeof(input) === 'string') {\n",
        "            // This naive transform only supports ASCII patterns. UTF-8 support\n",
        "            // not necessary for the intended use case here.\n",
        "            var arr = new Uint8Array(input.length);\n",
        "            for (var i = 0; i < input.length; i++) {\n",
        "            var c = input.charCodeAt(i);\n",
        "            if (c > 127) {\n",
        "                throw new TypeError(\"Only ASCII patterns are supported\");\n",
        "            }\n",
        "            arr[i] = c;\n",
        "            }\n",
        "            return arr;\n",
        "        } else {\n",
        "            // Assume that it's already something that can be coerced.\n",
        "            return new Uint8Array(input);\n",
        "        }\n",
        "    }\n",
        " \n",
        "    boyerMoore(patternBuffer) {\n",
        "        // Implementation of Boyer-Moore substring search ported from page 772 of\n",
        "        // Algorithms Fourth Edition (Sedgewick, Wayne)\n",
        "        // http://algs4.cs.princeton.edu/53substring/BoyerMoore.java.html\n",
        "        \n",
        "//      USAGE:\n",
        "            // needle should be ASCII string, ArrayBuffer, or Uint8Array\n",
        "            // haystack should be an ArrayBuffer or Uint8Array\n",
        "//          var search = boyerMoore(needle);\n",
        "//          var skip = search.byteLength;\n",
        "//          var indices = [];\n",
        "//          for (var i = search(haystack); i !== -1; i = search(haystack, i + skip)) {\n",
        "//              indices.push(i);\n",
        "//          }\n",
        "        \n",
        "        var pattern = this.asUint8Array(patternBuffer);\n",
        "        var M = pattern.length;\n",
        "        if (M === 0) {\n",
        "            throw new TypeError(\"patternBuffer must be at least 1 byte long\");\n",
        "        }\n",
        "        // radix\n",
        "        var R = 256;\n",
        "        var rightmost_positions = new Int32Array(R);\n",
        "        // position of the rightmost occurrence of the byte c in the pattern\n",
        "        for (var c = 0; c < R; c++) {\n",
        "            // -1 for bytes not in pattern\n",
        "            rightmost_positions[c] = -1;\n",
        "        }\n",
        "        for (var j = 0; j < M; j++) {\n",
        "            // rightmost position for bytes in pattern\n",
        "            rightmost_positions[pattern[j]] = j;\n",
        "        }\n",
        "        var boyerMooreSearch = (txtBuffer, start, end) => {\n",
        "            // Return offset of first match, -1 if no match.\n",
        "            var txt = this.asUint8Array(txtBuffer);\n",
        "            if (start === undefined) start = 0;\n",
        "            if (end === undefined) end = txt.length;\n",
        "            var pat = pattern;\n",
        "            var right = rightmost_positions;\n",
        "            var lastIndex = end - pat.length;\n",
        "            var lastPatIndex = pat.length - 1;\n",
        "            var skip;\n",
        "            for (var i = start; i <= lastIndex; i += skip) {\n",
        "                skip = 0;\n",
        "                for (var j = lastPatIndex; j >= 0; j--) {\n",
        "                var c = txt[i + j];\n",
        "                if (pat[j] !== c) {\n",
        "                    skip = Math.max(1, j - right[c]);\n",
        "                    break;\n",
        "                }\n",
        "                }\n",
        "                if (skip === 0) {\n",
        "                return i;\n",
        "                }\n",
        "            }\n",
        "            return -1;\n",
        "        };\n",
        "        boyerMooreSearch.byteLength = pattern.byteLength;\n",
        "        return boyerMooreSearch;\n",
        "    }\n",
        "    //---------------------end copy/pasted solution------------------------\n",
        " \n",
        "     async  sendserial() {\n",
        "    //console.log('sending sendserial');\n",
        "     if(this.ready_to_send_data&&this.bufferednewLines)//&&this.data_slice[0].length)\n",
        "     {\n",
        "        if(!this.generate_parallel)\n",
        "       {\n",
        "         this.ready_to_send_data=false;\n",
        "       }\n",
        "  this.data_send_count++;\n",
        "  var array_to_send_as_json='';\n",
        "    var value = new Uint8Array(this.buffer);\n",
        "    this.bufferednewLines=0;\n",
        "    this.buffer=[];\n",
        "    //console.log('sending buffer');\n",
        "            //document.body.appendChild(document.createTextNode('sending buffer'));\n",
        "\n",
        "  const channel = await google.colab.kernel.comms.open('comm_target1', array_to_send_as_json, [value.buffer]);\n",
        "  let success = false;\n",
        "  for await (const message of channel.messages) {\n",
        "       //this.ready_to_send_data=true;\n",
        "    if (message.data.response == 'close') {\n",
        "    //if (message.data.response == 'got comm open!') {\n",
        "      //const responseBuffer = new Uint8Array(message.buffers[0]);\n",
        "      //for (let i = 0; i < buffer.length; ++i) {\n",
        "      //  if (responseBuffer[i] != buffer[i]) {\n",
        "      //    console.error('comm buffer different at ' + i);\n",
        "      //    document.body.appendChild(document.createTextNode('comm buffer different at2 ' + i));\n",
        "      //    return;\n",
        "      //  }\n",
        "      //}\n",
        "      // Close the channel once the expected message is received. This should\n",
        "      // cause the messages iterator to complete and for the for-await loop to\n",
        "      // end.\n",
        "      //console.error('comm buffer same ' + responseBuffer);\n",
        "      //document.body.appendChild(document.createTextNode('comm buffer same2 ' + responseBuffer));\n",
        "      channel.close();\n",
        "    }\n",
        "    //console.log('audio&image received');\n",
        "    //var message_parsed=JSON.parse(message.data.response);\n",
        "    //console.log('audio&image decoded');\n",
        "    //for(let i = 0; i < message_parsed.length; ++i)\n",
        "    {\n",
        "      \n",
        "      if (this.generate_wavegan)\n",
        "      {\n",
        "        //if((typeof message_parsed[i]) === 'string')\n",
        "        {\n",
        "          //console.log(\"audio to set\")\n",
        "          if(message.data.response.startsWith('data:audio'))\n",
        "          //if(message_parsed[i].startsWith('data:audio'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('audio decoded'));\n",
        "            //await \n",
        "            //playAudio1(message_parsed[i]);\n",
        "            //await \n",
        "            //playAudio1(message.buffers[0]);\n",
        "            playAudio1(message.data.response);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_wg));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(playAudio2,next_time);\n",
        "            //console.log(\"audio set\")\n",
        "          }\n",
        "          if(0)\n",
        "          if(message.data.response.startsWith('data:video'))\n",
        "          //if(message_parsed[i].startsWith('data:audio'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('audio decoded'));\n",
        "            //await \n",
        "            //playAudio1(message_parsed[i]);\n",
        "            //await \n",
        "            playvideo1(message.data.response);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_wg));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(playVideo2,next_time);\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      if (this.generate_stylegan2)\n",
        "      {\n",
        "        //if((typeof message_parsed[i]) === 'string')\n",
        "          //console.log(message.data.response);\n",
        "        {\n",
        "          //console.log(message.buffers);\n",
        "          //if(message.buffers[0].length>0)\n",
        "          if(message.data.response.startsWith('binary:data:image'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('image decoded'));\n",
        "\n",
        "            var image_type='';\n",
        "            if(message.data.response.startsWith('binary:data:image/png'))\n",
        "            {\n",
        "              image_type='image/png';\n",
        "            }\n",
        "            if(message.data.response.startsWith('binary:data:image/jpeg'))\n",
        "            {\n",
        "              image_type='image/jpeg';\n",
        "            }\n",
        "            if(message.data.response.includes('user:killed'))\n",
        "            {\n",
        "              console.log('user:killed');\n",
        "              avic01.clear();\n",
        "              avic02.clear();\n",
        "            }\n",
        "            if(message.data.response.includes('enemy:killed'))\n",
        "            {\n",
        "              console.log('enemy:killed');\n",
        "              avic02.clear();\n",
        "            }\n",
        "            if(message.data.response.includes('user:add'))\n",
        "            {\n",
        "              console.log('user:add');\n",
        "              avic01.displayPhoto1(message.buffers[0],this.xsize,this.ysize,image_type);\n",
        "              //displayPhoto1001(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic01.displayPhoto4();},next_time);\n",
        "            }\n",
        "            if(message.data.response.includes('user:attack'))\n",
        "            if(0)\n",
        "            {\n",
        "              console.log('user:attack');\n",
        "              displayPhoto10011(message.buffers[0],this.xsize,this.ysize,image_type);\n",
        "              //displayPhoto1001(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(displayPhoto40011,next_time);\n",
        "            }\n",
        "            if(message.data.response.includes('enemy:add'))\n",
        "            {\n",
        "              console.log('enemy:add');\n",
        "              avic02.displayPhoto1(message.buffers[1],this.xsize,this.ysize,image_type);\n",
        "              //displayPhoto1001(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic02.displayPhoto4();},next_time);\n",
        "            }\n",
        "            if(message.data.response.includes('enemy:attack'))\n",
        "            if(0)\n",
        "            {\n",
        "              console.log('enemy:attack');\n",
        "              displayPhoto10021(message.buffers[1],this.xsize,this.ysize,image_type);\n",
        "              //displayPhoto1001(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(displayPhoto40021,next_time);\n",
        "            }\n",
        "            if(message.data.response.includes('user:restored'))\n",
        "            if(0)\n",
        "            {\n",
        "              console.log('user:restored');\n",
        "            }\n",
        "            var image_buffer_shift=0;\n",
        "            if(message.data.response.includes('mode:3'))\n",
        "            {\n",
        "              var image_buffer_shift=3;\n",
        "              avic1.displayPhoto1(message.buffers[2],this.xsize,this.ysize,image_type);\n",
        "              //displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic1.displayPhoto4();},next_time);\n",
        "              avic2.displayPhoto1(message.buffers[3],device.xsize,device.ysize,image_type);\n",
        "              //displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic2.displayPhoto4();},next_time);\n",
        "              avic3.displayPhoto1(message.buffers[4],this.xsize,this.ysize,image_type);\n",
        "              //displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic3.displayPhoto4();},next_time);\n",
        "            }\n",
        "            if(message.data.response.includes('user:cards_life'))\n",
        "            {\n",
        "              console.log('user:cards_life');\n",
        "              avic002.displayPhoto1(message.buffers[2+image_buffer_shift],this.xsize*2,this.ysize,'image/png');\n",
        "              //displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic002.displayPhoto4();},next_time);\n",
        "            }\n",
        "            if(message.data.response.includes('enemy:cards_life'))\n",
        "            {\n",
        "              console.log('enemy:cards_life');\n",
        "              avic001.displayPhoto1(message.buffers[3+image_buffer_shift],this.xsize*2,this.ysize,'image/png');\n",
        "              //displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic001.displayPhoto4();},next_time);\n",
        "            }\n",
        "            if(message.data.response.includes('user_attack_enemy:cards_life'))\n",
        "            {\n",
        "              console.log('user_attack_enemy:cards_life');\n",
        "              avic.displayPhoto1(message.buffers[4+image_buffer_shift],this.xsize*7,this.ysize*3,'image/png');\n",
        "              //avic.displayPhoto1(message.buffers[4],128,128,'image/png');\n",
        "              //displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic.displayPhoto4();},next_time);\n",
        "            }\n",
        "            if(this.generate_game)\n",
        "            {\n",
        "              if(this.generate_game_mode1)\n",
        "              {\n",
        "                avic00.displayPhoto1(message.buffers[0],this.xsize,this.ysize,image_type);\n",
        "              }\n",
        "            }\n",
        "            else\n",
        "            {\n",
        "              console.log('avic00');\n",
        "              avic00.displayPhoto1(message.buffers[0],this.xsize,this.ysize,image_type);\n",
        "              //avic00.displayPhoto1(message.buffers[0],512,512,image_type);\n",
        "            }\n",
        "            //displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "            this.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/this.fps_sg2));\n",
        "            var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "            setTimeout(function(){avic00.displayPhoto4();},next_time);\n",
        "            //displayPhoto10(message.buffers[0],128,128,image_type);\n",
        "            ////displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "            //device.time000=Date.now();\n",
        "            //var frame_time=parseInt((1000/device.fps_sg2));\n",
        "            //var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            //setTimeout(displayPhoto40,next_time);\n",
        "            var frame_now=(this.time000-this.time100)/frame_time;\n",
        "            if(Math.round(frame_now-this.frame_last)!=1)\n",
        "            {\n",
        "              console.log('f2:'+next_time+','+frame_time+','+\n",
        "                frame_now+','+this.frame_last+','+\n",
        "                (frame_now-this.frame_last));\n",
        "            }\n",
        "            this.frame_last=frame_now;\n",
        "            //console.log('show');\n",
        "          }\n",
        "          if(0)\n",
        "          if(message.data.response.startsWith('binary:data:video'))\n",
        "          //if(message_parsed[i].startsWith('data:image'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('image decoded'));\n",
        "            //await \n",
        "            //displayPhoto1(message_parsed[i]);\n",
        "            //await \n",
        "            var video_type='';\n",
        "            if(message.data.response.startsWith('binary:data:video/webm'))\n",
        "            {\n",
        "              video_type='video/webm';\n",
        "            }\n",
        "            if(message.data.response.startsWith('binary:data:video/mp4'))\n",
        "            {\n",
        "              video_type='video/mp4';\n",
        "            }\n",
        "            displayVideo10(message.buffers[0],device.xsize,device.ysize,video_type);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_sg2));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(displayVideo40,next_time);\n",
        "            var frame_now=(device.time000-device.time100)/frame_time;\n",
        "            if(Math.round(frame_now-this.frame_last)!=1)\n",
        "            {\n",
        "              console.log('f2:'+next_time+','+frame_time+','+\n",
        "                frame_now+','+this.frame_last+','+\n",
        "                (frame_now-this.frame_last));\n",
        "            }\n",
        "            this.frame_last=frame_now;\n",
        "          }\n",
        "          //console.log(\"image string\");\n",
        "          if(0)\n",
        "          if(message.data.response.startsWith('data:image'))\n",
        "          //if(message_parsed[i].startsWith('data:image'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('image decoded'));\n",
        "            //await \n",
        "            //displayPhoto1(message_parsed[i]);\n",
        "            //await \n",
        "            displayPhoto1(message.data.response,device.xsize,device.ysize);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_sg2));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(displayPhoto4,next_time);\n",
        "            var frame_now=(device.time000-device.time100)/frame_time;\n",
        "            if(Math.round(frame_now-this.frame_last)!=1)\n",
        "            {\n",
        "              console.log('f2:'+next_time+','+frame_time+','+\n",
        "                frame_now+','+this.frame_last+','+\n",
        "                (frame_now-this.frame_last));\n",
        "            }\n",
        "            this.frame_last=frame_now;\n",
        "          }\n",
        "          if(0)\n",
        "          if(message.data.response.startsWith('data:video'))\n",
        "          //if(message_parsed[i].startsWith('data:image'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('image decoded'));\n",
        "            //await \n",
        "            //displayPhoto1(message_parsed[i]);\n",
        "            //await \n",
        "            var video_type='';\n",
        "            if(message.data.response.startsWith('data:video/webm'))\n",
        "            {\n",
        "              video_type='video/webm';\n",
        "            }\n",
        "            if(message.data.response.startsWith('data:video/mp4'))\n",
        "            {\n",
        "              video_type='video/mp4';\n",
        "            }\n",
        "            displayVideo1(message.data.response,device.xsize,device.ysize,video_type);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_sg2));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(displayVideo2,next_time);\n",
        "            var frame_now=(device.time000-device.time100)/frame_time;\n",
        "            if(Math.round(frame_now-this.frame_last)!=1)\n",
        "            {\n",
        "              console.log('f2:'+next_time+','+frame_time+','+\n",
        "                frame_now+','+this.frame_last+','+\n",
        "                (frame_now-this.frame_last));\n",
        "            }\n",
        "            this.frame_last=frame_now;\n",
        "          }\n",
        "        }\n",
        "        //else\n",
        "        {\n",
        "          //console.log(\"image not string\");\n",
        "          //displayPhoto3(message_parsed[i]);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    //console.log(\"close\");\n",
        "    channel.close();\n",
        "  }\n",
        "      this.ready_to_send_data = true;\n",
        "    //console.log(\"ready_to_send_data\");\n",
        "  //document.body.appendChild(document.createTextNode('done2.'));\n",
        "     } \n",
        "     }\n",
        " \n",
        "    async  takeandsendSlice(data_slice_from,data_slice_to) {\n",
        "        //this.lastnewLines=this.bufferednewLines;\n",
        "     //if(this.ready_to_send_data&&this.bufferednewLines){//&&this.data_slice[0].length)\n",
        "     if(this.ready_to_send_data&&this.bufferednewLines)//&&this.data_slice[0].length)\n",
        "     {\n",
        "      //this.ready_to_send_data = false;\n",
        "      this.data_slice = [\n",
        "        //device.data.ms.slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+0].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+1].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+2].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+3].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+4].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+5].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+6].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+7].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+8].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+9].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+10].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+11].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+12].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+13].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+14].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+15].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+16].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+17].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+18].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+19].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+20].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+21].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+22].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+23].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+24].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+25].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+26].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+27].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+28].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+29].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+30].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+31].slice(data_slice_from,data_slice_to)\n",
        "        ];\n",
        "  this.bufferednewLines=0;\n",
        "  //const buffer = new Uint8Array(10);\n",
        "  //for (let i = 0; i < buffer.byteLength; ++i) {\n",
        "  //  buffer[i] = i\n",
        "  //}\n",
        "  var data_slice_uint32array=new Uint32Array(this.data_slice.length*this.data_slice[0].length);\n",
        "  for(let i=0;i<this.data_slice.length;i++)\n",
        "  {\n",
        "    for(let j=0;j<this.data_slice[i].length;j++)\n",
        "    {\n",
        "      data_slice_uint32array[i*this.data_slice[i].length + j]=this.data_slice[i][j];\n",
        "    }\n",
        "  }\n",
        "  //var data_slice_uint8array = new Int8Array(this.data_slice_array.buffer);\n",
        "\n",
        "  //const buffer = new Uint8Array(this.data_slice.byteLength);\n",
        "  //for (let i = 0; i < buffer.byteLength; ++i) {\n",
        "  //  buffer[i] = i\n",
        "  //}\n",
        "  \n",
        "  \n",
        "  //var array_to_send_as_json = JSON.stringify(this.data_slice);\n",
        "  var array_to_send_as_json = JSON.stringify([]);\n",
        "  //document.body.appendChild(document.createTextNode('sending ready'));\n",
        "  this.data_send_count++;\n",
        "  //if(this.channel==None)\n",
        "  //{\n",
        "  //  this.channel = await google.colab.kernel.comms.open('comm_target1', array_to_send_as_json, []);\n",
        "    //this.channel = await google.colab.kernel.comms.open(this.data_send_count.toString(), array_to_send_as_json, []);\n",
        "  //} else \n",
        "  //{\n",
        "    //this.channel.send(this.data_send_count.toString())\n",
        "  //}\n",
        "  //document.body.appendChild(document.createTextNode(array_to_send_as_json));\n",
        "  //const channel = await google.colab.kernel.comms.open('comm_target1', array_to_send_as_json, []);\n",
        "//  const channel = await google.colab.kernel.comms.open('comm_target1', array_to_send_as_json, [this.data_slice.buffer]);\n",
        "  const channel = await google.colab.kernel.comms.open('comm_target1', array_to_send_as_json, [data_slice_uint32array.buffer]);\n",
        "  //const channel = await google.colab.kernel.comms.open('comm_target1', array_to_send_as_json, [buffer.buffer]);\n",
        "  //const channel = await google.colab.kernel.comms.open('comm_target1', array_to_send_as_json, [this.data_slice]);\n",
        "  //const channel = await google.colab.kernel.comms.open('comm_target1', 'the data', [buffer.buffer]);\n",
        "  let success = false;\n",
        "  for await (const message of channel.messages) {\n",
        "    if (message.data.response == 'close') {\n",
        "    //if (message.data.response == 'got comm open!') {\n",
        "      //const responseBuffer = new Uint8Array(message.buffers[0]);\n",
        "      //for (let i = 0; i < buffer.length; ++i) {\n",
        "      //  if (responseBuffer[i] != buffer[i]) {\n",
        "      //    console.error('comm buffer different at ' + i);\n",
        "      //    document.body.appendChild(document.createTextNode('comm buffer different at2 ' + i));\n",
        "      //    return;\n",
        "      //  }\n",
        "      //}\n",
        "      // Close the channel once the expected message is received. This should\n",
        "      // cause the messages iterator to complete and for the for-await loop to\n",
        "      // end.\n",
        "      //console.error('comm buffer same ' + responseBuffer);\n",
        "      //document.body.appendChild(document.createTextNode('comm buffer same2 ' + responseBuffer));\n",
        "      channel.close();\n",
        "    }\n",
        "    //console.log('audio&image received');\n",
        "    //var message_parsed=JSON.parse(message.data.response);\n",
        "    //console.log('audio&image decoded');\n",
        "    //for(let i = 0; i < message_parsed.length; ++i)\n",
        "    {\n",
        "      \n",
        "      if (this.generate_wavegan)\n",
        "      {\n",
        "        //if((typeof message_parsed[i]) === 'string')\n",
        "        {\n",
        "          if(message.data.response.startsWith('data:audio'))\n",
        "          //if(message_parsed[i].startsWith('data:audio'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('audio decoded'));\n",
        "            //await \n",
        "            //playAudio1(message_parsed[i]);\n",
        "            //await \n",
        "            //playAudio1(message.buffers[0]);\n",
        "            playAudio1(message.data.response);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_wg));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(playAudio2,next_time);\n",
        "          }\n",
        "          if(0)\n",
        "          if(message.data.response.startsWith('data:video'))\n",
        "          //if(message_parsed[i].startsWith('data:audio'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('audio decoded'));\n",
        "            //await \n",
        "            //playAudio1(message_parsed[i]);\n",
        "            //await \n",
        "            playvideo1(message.data.response);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_wg));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(playVideo2,next_time);\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      if (this.generate_stylegan2)\n",
        "      {\n",
        "        //if((typeof message_parsed[i]) === 'string')\n",
        "        {\n",
        "          //console.log(message.buffers);\n",
        "          //if(message.buffers[0].length>0)\n",
        "          if(0)\n",
        "          {\n",
        "            displayPhoto10(message.buffers[0],device.xsize,device.ysize);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_sg2));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(displayPhoto40,next_time);\n",
        "            var frame_now=(device.time000-device.time100)/frame_time;\n",
        "            if(Math.round(frame_now-this.frame_last)!=1)\n",
        "            {\n",
        "              console.log('f2:'+next_time+','+frame_time+','+\n",
        "                frame_now+','+this.frame_last+','+\n",
        "                (frame_now-this.frame_last));\n",
        "            }\n",
        "            this.frame_last=frame_now;\n",
        "          }\n",
        "          //console.log(\"image string\");\n",
        "          if(message.data.response.startsWith('data:image'))\n",
        "          //if(message_parsed[i].startsWith('data:image'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('image decoded'));\n",
        "            //await \n",
        "            //displayPhoto1(message_parsed[i]);\n",
        "            //await \n",
        "            \n",
        "            \n",
        "            displayPhoto1(message.data.response,device.xsize,device.ysize);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_sg2));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(displayPhoto4,next_time);\n",
        "            var frame_now=(device.time000-device.time100)/frame_time;\n",
        "            if(Math.round(frame_now-this.frame_last)!=1)\n",
        "            {\n",
        "              console.log('f2:'+next_time+','+frame_time+','+\n",
        "                frame_now+','+this.frame_last+','+\n",
        "                (frame_now-this.frame_last));\n",
        "            }\n",
        "            this.frame_last=frame_now;\n",
        "          }\n",
        "          if(message.data.response.startsWith('data:video'))\n",
        "          //if(message_parsed[i].startsWith('data:image'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('image decoded'));\n",
        "            //await \n",
        "            //displayPhoto1(message_parsed[i]);\n",
        "            //await \n",
        "            var video_type='';\n",
        "            if(message.data.response.startsWith('data:video/webm'))\n",
        "            {\n",
        "              video_type='video/webm';\n",
        "            }\n",
        "            if(message.data.response.startsWith('data:video/mp4'))\n",
        "            {\n",
        "              video_type='video/mp4';\n",
        "            }\n",
        "            displayVideo1(message.data.response,device.xsize,device.ysize,video_type);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_sg2));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(displayVideo2,next_time);\n",
        "            var frame_now=(device.time000-device.time100)/frame_time;\n",
        "            if(Math.round(frame_now-this.frame_last)!=1)\n",
        "            {\n",
        "              console.log('f2:'+next_time+','+frame_time+','+\n",
        "                frame_now+','+this.frame_last+','+\n",
        "                (frame_now-this.frame_last));\n",
        "            }\n",
        "            this.frame_last=frame_now;\n",
        "          }\n",
        "        }\n",
        "        //else\n",
        "        {\n",
        "          //console.log(\"image not string\");\n",
        "          //displayPhoto3(message_parsed[i]);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "      this.ready_to_send_data = true;\n",
        "  //document.body.appendChild(document.createTextNode('done2.'));\n",
        "     } \n",
        " \n",
        "    }\n",
        " \n",
        "}\n",
        " \n",
        "device = new eeg32();\n",
        " \n",
        "    connect = async () => {\n",
        "        await this.device.setupSerialAsync();\n",
        "    }\n",
        " \n",
        "    disconnect = () => {\n",
        "        if (this.ui) this.ui.deleteNode()\n",
        "        this.device.closePort();\n",
        "    }\n",
        " \n",
        "      //const canvas = document.createElement('canvas');\n",
        "      //const audio = document.createElement('audio');\n",
        "      //const audio1 = document.createElement('audio');\n",
        "      //const audio2 = document.createElement('audio');\n",
        "      var audios = new Array();\n",
        "      var videos1 = new Array();\n",
        "      if(device.generate_wavegan)\n",
        "      {\n",
        "        const audios_length = 5;\n",
        "        for(let i = 0; i < audios_length; i++)\n",
        "        {\n",
        "          audios[i]=document.createElement('audio');\n",
        "        }\n",
        "        const videos1_length = 1;\n",
        "        for(let i = 0; i < videos1_length; i++)\n",
        "        {\n",
        "          videos1[i]=document.createElement('video');\n",
        "        }\n",
        "      }\n",
        "\n",
        " class audio_video_image_canvas { \n",
        " \n",
        "    constructor(\n",
        "        canvases_length = 1,\n",
        "        images_length = 1,\n",
        "        videos_length = 1,\n",
        "        width = 128,\n",
        "        height = 128\n",
        "        ) {\n",
        "          this.canvases = new Array();\n",
        "          this.ctxs = new Array();\n",
        "        this.canvases_length = canvases_length;//images_length;\n",
        "        for(let i = 0; i < this.canvases_length; i++)\n",
        "        {\n",
        "          //var ctx = canvas.getContext(\"2d\");\n",
        "          this.canvases[i]=document.createElement('canvas');\n",
        "          this.ctx = this.canvases[i].getContext(\"2d\");\n",
        "          this.ctxs[i]=this.canvases[i].getContext(\"2d\");\n",
        "          this.canvases[i].width=width;\n",
        "          this.canvases[i].height=height;\n",
        "          this.ctxs[i].clearRect(0, 0, this.canvases[i].width, this.canvases[i].height);\n",
        "        }\n",
        "      this.images = new Array();\n",
        "      this.videos = new Array();\n",
        "      //if(device.generate_stylegan2)\n",
        "      {\n",
        "        this.images_length = images_length;\n",
        "        //const canvases_length = 1;//images_length;\n",
        "        for(let i = 0; i < this.images_length; i++)\n",
        "        {\n",
        "          //images[i]=document.createElement('image');\n",
        "          //canvases[i]=document.createElement('canvas');\n",
        "          //var ctx = canvases[i].getContext(\"2d\");\n",
        "          this.images[i]=new Image();\n",
        "          //images[i].onload = function() {\n",
        "          //  ctx.drawImage(images[i], 0, 0);\n",
        "          //};\n",
        "        }\n",
        "        this.videos_length = videos_length;\n",
        "        for(let i = 0; i < this.videos_length; i++)\n",
        "        {\n",
        "          //videos2[i]=new Video();\n",
        "          this.videos[i]=document.createElement('video');\n",
        "          this.videos[i].addEventListener('play', function() {\n",
        "            var $this = this; //cache\n",
        "            (function loop() {\n",
        "              if (!$this.paused && !$this.ended) {\n",
        "                this.ctx.drawImage($this, 0, 0);\n",
        "                setTimeout(loop, 1000 / 10); // drawing at 30fps\n",
        "              }\n",
        "            })();\n",
        "          }, 0);\n",
        "        }\n",
        "      }\n",
        "\n",
        "          this.div = document.createElement('div');\n",
        "\n",
        "      //if(device.generate_stylegan2)\n",
        "      {\n",
        "        for(let i = 0; i <this.canvases.length; i++)\n",
        "        {\n",
        "          this.div.appendChild(this.canvases[i]);\n",
        "        }\n",
        "        for(let i = 0; i < this.videos.length; i++)\n",
        "        {\n",
        "          //div3.appendChild(videos2[i]);\n",
        "          //videos2[i].controls = true;\n",
        "          //videos2[i].autoplay = true;\n",
        "        }\n",
        "      }\n",
        "\n",
        "    this.image_now=0;\n",
        "    this.canvas_now=0;\n",
        "\n",
        "        }\n",
        "\n",
        "        async  clear() {\n",
        "              for(let i = 0; i < this.canvases.length; i++)\n",
        "              {\n",
        "                this.ctxs[i].clearRect(0, 0, this.canvases[i].width, this.canvases[i].height);\n",
        "              }\n",
        "              this.canvas_now=0;\n",
        "              this.image_now=0;\n",
        "\n",
        "        }\n",
        "\n",
        "    async  displayPhoto1(photodata,photoWidth=512,photoHeight=512,image_type=\"image/jpeg\") {\n",
        "      //if(canvas.width != photoWidth) canvas.width = photoWidth;\n",
        "      //if(canvas.height != photoHeight) canvas.height = photoHeight;\n",
        "      await this.displayPhoto2(photodata,photoWidth,photoHeight,image_type);\n",
        "    }\n",
        "    async  displayPhoto2(photodata,photoWidth,photoHeight,image_type=\"image/jpeg\") {\n",
        "      //if(canvas.width != photoWidth) canvas.width = photoWidth;\n",
        "      //if(canvas.height != photoHeight) canvas.height = photoHeight;\n",
        "     //image.src = photodata;\n",
        "      {\n",
        "       if(this.canvases[this.canvas_now%%this.canvases.length].width != photoWidth)\n",
        "       {\n",
        "         this.canvases[this.canvas_now%%this.canvases.length].width = photoWidth;\n",
        "       }\n",
        "       if(this.canvases[this.canvas_now%%this.canvases.length].height != photoHeight) \n",
        "       {\n",
        "         this.canvases[this.canvas_now%%this.canvases.length].height = photoHeight;\n",
        "       }\n",
        "       //if(canvases[0].width != photoWidth)\n",
        "       //{\n",
        "       //  canvases[0].width = photoWidth;\n",
        "       //}\n",
        "       //if(canvases[0].height != photoHeight) \n",
        "       //{\n",
        "       //  canvases[0].height = photoHeight;\n",
        "       //}\n",
        "       //console.log(photodata);\n",
        "       var arrayBufferView = new Uint8Array( photodata );\n",
        "       var blob = new Blob( [ arrayBufferView ], { type: image_type } );\n",
        "       var urlCreator = window.URL || window.webkitURL;\n",
        "       var imageUrl = urlCreator.createObjectURL( blob );\n",
        "       //images[image_now%%images.length].src = photodata;\n",
        "       //if(images[image_now%%images.length].src)\n",
        "       //{\n",
        "       //  URL.revokeObjectURL(images[image_now%%images.length].src);\n",
        "       //}\n",
        "       this.images[this.image_now%%this.images.length].src = imageUrl;\n",
        "        //audios[audio_now%%audios.length].play();\n",
        "        //console.log(imageUrl);\n",
        "      }\n",
        "      //image_now++;\n",
        "    }\n",
        "    async  displayPhoto4(photodata){//},photoWidth,photoHeight) {\n",
        "      this.ctxs[this.canvas_now%%this.canvases.length].drawImage(this.images[this.image_now%%this.images.length], 0, 0, this.canvases[this.canvas_now%%this.canvases.length].width, this.canvases[this.canvas_now%%this.canvases.length].height);\n",
        "      //ctxs01[image01_now%%images01.length].drawImage(images01[image01_now%%images01.length], 0, 0);\n",
        "      if((this.image_now-1)%%this.images.length>=0)\n",
        "      {\n",
        "        if(this.images[(this.image_now-1)%%this.images.length].src)\n",
        "        {\n",
        "          URL.revokeObjectURL(this.images[(this.image_now-1)%%this.images.length].src);\n",
        "        }\n",
        "      }\n",
        "      this.image_now++;\n",
        "      this.canvas_now++;\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "const div = document.createElement('div');\n",
        "const div01 = document.createElement('div');\n",
        "const div02 = document.createElement('div');\n",
        "const div03 = document.createElement('div');\n",
        "const div2 = document.createElement('div');\n",
        "const div3 = document.createElement('div');\n",
        "const div4 = document.createElement('div');\n",
        "const div5 = document.createElement('div');\n",
        "const div6 = document.createElement('div');\n",
        "const div7 = document.createElement('div');\n",
        "const div8 = document.createElement('div');\n",
        "const btnconnect = document.createElement('button');\n",
        "const btndisconnect = document.createElement('button');\n",
        "const capture = document.createElement('button');\n",
        "\n",
        "if (device.generate_game)\n",
        "{\n",
        "  if (device.generate_game_mode3)\n",
        "  {\n",
        "    avic1 = new audio_video_image_canvas(1,1,0);\n",
        "    avic2 = new audio_video_image_canvas(1,1,0);\n",
        "    avic3 = new audio_video_image_canvas(1,1,0);\n",
        "          div01.appendChild(avic1.div);\n",
        "          div02.appendChild(avic2.div);\n",
        "          div03.appendChild(avic3.div);\n",
        "  }\n",
        "  avic = new audio_video_image_canvas(1,1,0);\n",
        "  avic01 = new audio_video_image_canvas(7,7,0);\n",
        "  avic02 = new audio_video_image_canvas(7,7,0);\n",
        "          div3.appendChild(avic.div);\n",
        "          div4.appendChild(avic01.div);\n",
        "          div5.appendChild(avic02.div);\n",
        "  if (device.generate_game_mode1)\n",
        "  {\n",
        "    avic00 = new audio_video_image_canvas(1,1,0,device.xsize,device.ysize);\n",
        "          div6.appendChild(avic00.div);\n",
        "  }\n",
        "  avic001 = new audio_video_image_canvas(1,1,0,device.xsize*2,device.ysize);\n",
        "  avic002 = new audio_video_image_canvas(1,1,0,device.xsize*2,device.ysize);\n",
        "          div7.appendChild(avic001.div);\n",
        "          div8.appendChild(avic002.div);\n",
        "}\n",
        "else\n",
        "{\n",
        "  avic00 = new audio_video_image_canvas(1,1,0,device.xsize,device.ysize);\n",
        "//  avic00 = new audio_video_image_canvas(1,1,0,512,512);\n",
        "          div6.appendChild(avic00.div);\n",
        "}\n",
        "\n",
        "\n",
        "    async function takePhoto2(quality=1) {\n",
        "      btnconnect.remove();\n",
        "      capture.remove();\n",
        "      device.ready_to_send_data = true;\n",
        "    }\n",
        " \n",
        "    async function takePhoto(quality=1) {\n",
        " \n",
        "      btnconnect.textContent = 'connect';\n",
        "      div.appendChild(btnconnect);\n",
        "      btnconnect.onclick = this.connect;\n",
        "      \n",
        "      btndisconnect.textContent = 'disconnect';\n",
        "      div.appendChild(btndisconnect);\n",
        "      btndisconnect.onclick = this.disconnect;\n",
        "      \n",
        "      capture.textContent = 'Capture';\n",
        "      capture.onclick = takePhoto2;\n",
        "      div.appendChild(capture);\n",
        "     \n",
        "      //div.appendChild(canvas);\n",
        "      //div.appendChild(audio);\n",
        "      //div.appendChild(audio1);\n",
        "      //div.appendChild(audio2);\n",
        "      if(device.generate_wavegan)\n",
        "      {\n",
        "        for(let i = 0; i < audios.length; i++)\n",
        "        {\n",
        "          div2.appendChild(audios[i]);\n",
        "          //audios[i].controls = true;\n",
        "          //audios[i].autoplay = true;\n",
        "        }\n",
        "        for(let i = 0; i < videos1.length; i++)\n",
        "        {\n",
        "          div2.appendChild(videos1[i]);\n",
        "          //videos1[i].controls = true;\n",
        "          //videos1[i].autoplay = true;\n",
        "        }\n",
        "      }\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      if(device.generate_wavegan)\n",
        "      {\n",
        "        //console.log(\"audio init\")\n",
        "        document.body.appendChild(div2);\n",
        "      }\n",
        "      if(device.generate_game)\n",
        "      {\n",
        "        document.body.appendChild(div5);\n",
        "        document.body.appendChild(div2);\n",
        "\n",
        "        document.body.appendChild(div7);\n",
        "        div7.style.styleFloat = 'left';\n",
        "        div7.style.cssFloat = 'left';\n",
        "      }\n",
        "      if(device.generate_game_mode3)\n",
        "      {\n",
        "        document.body.appendChild(div01);\n",
        "        div01.style.styleFloat = 'left';\n",
        "        div01.style.cssFloat = 'left';\n",
        "        document.body.appendChild(div02);\n",
        "        div02.style.styleFloat = 'left';\n",
        "        div02.style.cssFloat = 'left';\n",
        "        document.body.appendChild(div03);\n",
        "        div03.style.styleFloat = 'left';\n",
        "        div03.style.cssFloat = 'left';\n",
        "      }\n",
        "      else\n",
        "      {\n",
        "        document.body.appendChild(div6);\n",
        "        div6.style.styleFloat = 'left';\n",
        "        div6.style.cssFloat = 'left';\n",
        "      }\n",
        "      if(device.generate_game)\n",
        "      {\n",
        "        document.body.appendChild(div8);\n",
        "\n",
        "        document.body.appendChild(div4);\n",
        "        document.body.appendChild(div3);\n",
        "      }\n",
        "         await new Promise((resolve) => capture.onclick = resolve);\n",
        " \n",
        "      btnconnect.remove();\n",
        "      capture.remove();\n",
        "      device.ready_to_send_data = true;\n",
        "    }\n",
        "\n",
        "\n",
        "    async function takePhoto1(quality=1) {  \n",
        "      //var data_slice_send=this.device.data_slice;\n",
        "      //var data_slice_send=[this.device.data_slice[0],this.device.data_slice[1]];\n",
        "      //var data_slice_send=[this.device.data_slice[0]];\n",
        "      var data_slice_send=[[this.device.data_slice[0][0]]];\n",
        "        //console.log(\"data_slice_send[0].length:\", data_slice_send[0].length);\n",
        "        //console.log(\"device.bufferednewLines:\", device.bufferednewLines);\n",
        "      device.bufferednewLines=0;\n",
        "      return data_slice_send;      \n",
        "    }\n",
        " \n",
        "    var audio_now=0;\n",
        "    async function playAudio1(audiodata){//},photoWidth=512,photoHeight=512) {\n",
        "      //const canvas = document.createElement('canvas');\n",
        "      //if(canvas.width != photoWidth) canvas.width = photoWidth;\n",
        "      //if(canvas.height != photoHeight) canvas.height = photoHeight;\n",
        "      //audio.controls = true;\n",
        "      //audio.autoplay = true;\n",
        "      //audio1.controls = true;\n",
        "      //audio1.autoplay = true;\n",
        "      //audio2.controls = true;\n",
        "      //audio2.autoplay = true;\n",
        " \n",
        "      //canvas.getContext('2d').drawImage(photodata, 0, 0);\n",
        "      //var canvas = document.getElementById(\"c\");\n",
        "      ///var ctx = canvas.getContext(\"2d\");\n",
        " \n",
        "      ///var image = new Image();\n",
        "      ///image.onload = function() {\n",
        "      ///  ctx.drawImage(image, 0, 0);\n",
        "      ///};\n",
        "      //audio.src = audiodata;\n",
        "      //audio.play()\n",
        "      //if(audio_now%%2==0)\n",
        "      //{\n",
        "      //  audio1.src = audiodata;\n",
        "      //  audio1.play()\n",
        "      //}\n",
        "      //else\n",
        "      //{\n",
        "      //  audio2.src = audiodata;\n",
        "      //  audio2.play()\n",
        "      //}\n",
        "      //for(let i = 0; i < audios.length; ++i)\n",
        "      {\n",
        "        audios[audio_now%%audios.length].src = audiodata;\n",
        "        //if(audio_now==0)\n",
        "        {\n",
        "          //audios[audio_now%%audios.length].controls = false;\n",
        "          audios[audio_now%%audios.length].controls = true;\n",
        "          //audios[audio_now%%audios.length].autoplay = true;\n",
        "        }\n",
        "        //audios[audio_now%%audios.length].play();\n",
        "      }\n",
        "      //audio_now++;\n",
        " \n",
        "      //console.log(\"audio add\")\n",
        "    }\n",
        "    async function playAudio2(audiodata){//},photoWidth=512,photoHeight=512) {\n",
        "      audios[audio_now%%audios.length].play();\n",
        "      audio_now++;\n",
        "      //console.log(\"audio play\")\n",
        "    }\n",
        "    function addSourceToVideo(element, src, type) {\n",
        "      var source = document.createElement('source');\n",
        "\n",
        "      source.src = src;\n",
        "      source.type = type;\n",
        "\n",
        "      element.appendChild(source);\n",
        "    }\n",
        "\n",
        "    var video1_now=0;\n",
        "    async function playVideo1(videodata){//},photoWidth=512,photoHeight=512) {\n",
        "      //for(let i = 0; i < audios.length; ++i)\n",
        "      {\n",
        "        //addSourceToVideo(videos1[video1_now%%videos1.length], videodata, 'video/mp4');\n",
        "\n",
        "        videos1[video1_now%%videos1.length].src = videodata;\n",
        "        //if(audio_now==0)\n",
        "        {\n",
        "          videos1[video1_now%%videos1.length].controls = false;//true;\n",
        "          //videos1[video1_now%%videos1.length].load();\n",
        "          //audios[audio_now%%audios.length].autoplay = true;\n",
        "        }\n",
        "        //audios[audio_now%%audios.length].play();\n",
        "      }\n",
        "      //audio_now++;\n",
        " \n",
        "    }\n",
        "    async function playVideo2(videodata){//},photoWidth=512,photoHeight=512) {\n",
        "      videos1[video1_now%%videos1.length].play();\n",
        "      video1_now++;\n",
        "    }\n",
        "    var video2_now=0;\n",
        "    async function displayVideo1(videodata,photoWidth=512,photoHeight=512,video_type='video/mp4') {\n",
        "       if(videos2[video2_now%%videos2.length].width != photoWidth)\n",
        "       {\n",
        "         videos2[video2_now%%videos2.length].width = photoWidth;\n",
        "       }\n",
        "       if(videos2[video2_now%%videos2.length].height != photoHeight) \n",
        "       {\n",
        "         videos2[video2_now%%videos2.length].height = photoHeight;\n",
        "       }\n",
        "      //for(let i = 0; i < audios.length; ++i)\n",
        "      {\n",
        "        //addSourceToVideo(videos2[video2_now%%videos2.length], videodata, 'video/mp4');\n",
        "        videos2[video2_now%%videos2.length].src = videodata;\n",
        "        videos2[video2_now%%videos2.length].type = video_type;\n",
        "        //videos2[video2_now%%videos2.length].type = 'video/webm';\n",
        "//        videos2[video2_now%%videos2.length].type = 'video/mp4';\n",
        "        //if(audio_now==0)\n",
        "        {\n",
        "          //videos2[video2_now%%videos2.length].controls = true;\n",
        "          videos2[video2_now%%videos2.length].controls = false;\n",
        "          videos2[video2_now%%videos2.length].load();\n",
        "          //audios[audio_now%%audios.length].autoplay = true;\n",
        "        }\n",
        "        //audios[audio_now%%audios.length].play();\n",
        "      }\n",
        "      //audio_now++;\n",
        " \n",
        "    }\n",
        "    async function displayVideo2(videodata){//},photoWidth=512,photoHeight=512) {\n",
        "      videos2[video2_now%%videos2.length].play();\n",
        "      videos2[video2_now%%videos2.length].style.display = \"block\";\n",
        "      video2_now++;\n",
        "      videos2[video2_now%%videos2.length].style.display = \"none\";\n",
        "    }\n",
        "    async function displayVideo10(videodata,photoWidth=512,photoHeight=512,video_type='video/mp4') {\n",
        "       //if(videos2[video2_now%%videos2.length].width != photoWidth)\n",
        "       //{\n",
        "       //  videos2[video2_now%%videos2.length].width = photoWidth;\n",
        "       //}\n",
        "       //if(videos2[video2_now%%videos2.length].height != photoHeight) \n",
        "       //{\n",
        "       //  videos2[video2_now%%videos2.length].height = photoHeight;\n",
        "       //}\n",
        "       //console.log(\"canvases[video2_now%%videos2.length]:\", canvases[video2_now%%videos2.length]);\n",
        "       //if(canvases[video2_now%%videos2.length].width != photoWidth)\n",
        "       //{\n",
        "       //  canvases[video2_now%%videos2.length].width = photoWidth;\n",
        "       //}\n",
        "       //if(canvases[video2_now%%videos2.length].height != photoHeight) \n",
        "       //{\n",
        "       //  canvases[video2_now%%videos2.length].height = photoHeight;\n",
        "       //}\n",
        "       \n",
        "       if(canvases[0].width != photoWidth)\n",
        "       {\n",
        "         canvases[0].width = photoWidth;\n",
        "       }\n",
        "       if(canvases[0].height != photoHeight) \n",
        "       {\n",
        "         canvases[0].height = photoHeight;\n",
        "       }\n",
        "      //for(let i = 0; i < audios.length; ++i)\n",
        "      {\n",
        "        //addSourceToVideo(videos2[video2_now%%videos2.length], videodata, 'video/mp4');\n",
        "       var arrayBufferView = new Uint8Array( videodata );\n",
        "       var blob = new Blob( [ arrayBufferView ], { type: video_type } );\n",
        "       var urlCreator = window.URL || window.webkitURL;\n",
        "       var videoUrl = urlCreator.createObjectURL( blob );\n",
        "       //console.log(\"videoUrl:\", videoUrl);\n",
        "       //images[image_now%%images.length].src = photodata;\n",
        "\n",
        "        videos2[video2_now%%videos2.length].src = videoUrl;\n",
        "        videos2[video2_now%%videos2.length].type = video_type;\n",
        "        //videos2[video2_now%%videos2.length].type = 'video/webm';\n",
        "//        videos2[video2_now%%videos2.length].type = 'video/mp4';\n",
        "        //if(audio_now==0)\n",
        "        {\n",
        "          videos2[video2_now%%videos2.length].controls = true;\n",
        "          //videos2[video2_now%%videos2.length].controls = false;\n",
        "          videos2[video2_now%%videos2.length].load();\n",
        "          //audios[audio_now%%audios.length].autoplay = true;\n",
        "        }\n",
        "        //audios[audio_now%%audios.length].play();\n",
        "      }\n",
        "      //audio_now++;\n",
        " \n",
        "    }\n",
        "    async function displayVideo20(videodata){//},photoWidth=512,photoHeight=512) {\n",
        "      videos2[video2_now%%videos2.length].play();\n",
        "      videos2[video2_now%%videos2.length].style.display = \"block\";\n",
        "      video2_now++;\n",
        "      videos2[video2_now%%videos2.length].style.display = \"none\";\n",
        "    }\n",
        "    async function displayVideo40(videodata){//},photoWidth,photoHeight) {\n",
        "      //ctx.drawImage(videos2[video2_now%%videos2.length], 0, 0);\n",
        "      videos2[video2_now%%videos2.length].play();\n",
        "      if((video2_now-1)%%videos2.length>=0)\n",
        "      {\n",
        "        videos2[(video2_now-1)%%videos2.length].pause();\n",
        "        if(videos2[(video2_now-1)%%videos2.length].src)\n",
        "        {\n",
        "          URL.revokeObjectURL(videos2[(video2_now-1)%%videos2.length].src);\n",
        "        }\n",
        "      }\n",
        "      video2_now++;\n",
        "    }\n",
        "\n",
        "    \n",
        "  takePhoto();\n",
        "  data_count=0;\n",
        "  var frame_last=0;\n",
        "\n",
        "async function check_to_send() {\n",
        "  //while(true)\n",
        "  {\n",
        "        //console.log(\"device.bufferednewLines:\", device.bufferednewLines);\n",
        "   if(device.bufferednewLines)\n",
        "   {\n",
        "    //if(this.bufferednewLines>this.data_slice_size)\n",
        "    //  {\n",
        "    //    this.bufferednewLines=this.data_slice_size;\n",
        "    //  }\n",
        "    /*device.time000=Date.now();\n",
        "    if(device.generate_wavegan)\n",
        "    {\n",
        "      device.this_frame_wg=parseInt((device.time000-device.time100)*device.fps_wg);\n",
        "      if(device.this_frame_wg>device.last_frame_wg)\n",
        "      {\n",
        "        device.last_frame_wg=device.this_frame_wg;\n",
        "        device.send_wg=true;\n",
        "      }\n",
        "    }\n",
        "    if(device.generate_stylegan2)\n",
        "    {\n",
        "      device.this_frame_sg2=parseInt((device.time000-device.time100)*device.fps_sg2);\n",
        "      if(device.this_frame_sg2>device.last_frame_sg2)\n",
        "      {\n",
        "        device.last_frame_sg2=device.this_frame_sg2;\n",
        "        device.send_sg2=true;\n",
        "      }\n",
        "    }\n",
        "    if(device.send_wg || device.send_sg2)\n",
        "    //if(this.bufferednewLines>512/(this.fps_sg2))\n",
        "    if(device.ready_to_send_data)*/\n",
        "    {\n",
        "      //this.bufferednewLines=512/this.fps;\n",
        " \n",
        "      //device.ready_to_send_data = true;\n",
        "        device.sendserial();\n",
        "        //device.takeandsendSlice(device.data.count-1-device.bufferednewLines,device.data.count-1);\n",
        "      device.send_wg=false;\n",
        "      device.send_sg2=false;\n",
        "    }\n",
        "    //this.takeSlice(this.data.count-1-this.bufferednewLines,this.data.count-1);\n",
        "    //this.takeandsendSlice(this.data.count-1-this.bufferednewLines,this.data.count-1);\n",
        "    //this.takeandsendSliceBroadcast(this.data.count-1-this.bufferednewLines,this.data.count-1);\n",
        "   }\n",
        "  }\n",
        "  device.time000=Date.now();\n",
        "//  var frame_time=parseInt((1000/device.fps_wg)/12);\n",
        "  var frame_time=parseInt((1000/device.fps));\n",
        "  var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "  setTimeout(check_to_send,next_time);\n",
        "  var frame_now=(device.time000-device.time100)/frame_time;\n",
        "  if(Math.round(frame_now-frame_last)!=1)\n",
        "  {\n",
        "    console.log('f1:'+next_time+','+frame_time+','+frame_now+','+\n",
        "      frame_last+','+(frame_now-frame_last));\n",
        "  }\n",
        "  frame_last=frame_now;\n",
        "}\n",
        "  device.time000=Date.now();\n",
        "  var frame_time=parseInt((1000/device.fps));\n",
        "  var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "  setTimeout(check_to_send,next_time);\n",
        "  //console.log(next_time);\n",
        "//var intervalID = setInterval(check_to_send,(1000/device.fps_wg)/10);\n",
        "//  window.requestAnimationFrame\n",
        " \n",
        "''' % {'xsize':xsize,'ysize':ysize,'generate_stylegan2':generate&gen_stylegan2,'generate_wavegan':generate&gen_wavegan,'generate_heatmap':generate&gen_heatmap,\n",
        "       'fps_sg2':fps_sg2,'fps_wg':fps_wg,'fps_hm':fps_hm,'sfreq':sfreq,'vref':vref,'gain':gain,'data_channels':data_channels,\n",
        "       'generate_game':generate&gen_game,'generate_game_mode1':generate&gen_game_mode1,'generate_game_mode3':generate&gen_game_mode3,\n",
        "       'generate_parallel':generate&gen_parallel})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop"
      ],
      "metadata": {
        "id": "XQvAK2DGPpvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    os.makedirs(FLAGS.output_directory, exist_ok=True)\n",
        "    logging.basicConfig(handlers=[\n",
        "            logging.FileHandler(os.path.join(FLAGS.output_directory, 'eval_log.txt'), 'w'),\n",
        "            logging.StreamHandler()\n",
        "            ], level=logging.INFO, format=\"%(message)s\")\n",
        "\n",
        "    testset = EMGDataset(test=True)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'\n",
        "\n",
        "    models = []\n",
        "    for fname in FLAGS.models:\n",
        "#        state_dict = torch.load(fname, map_location=torch.device('cpu'))\n",
        "        state_dict = torch.load(fname)\n",
        "        n_sess = 1 if FLAGS.no_session_embed else state_dict[\"session_emb.weight\"].size(0)\n",
        "        model = Model(testset.num_features, testset.num_speech_features, len(phoneme_inventory), n_sess).to(device)\n",
        "        model.load_state_dict(state_dict)\n",
        "        models.append(model)\n",
        "    ensemble = EnsembleModel(models)\n",
        "\n",
        "    _, _, confusion = test(ensemble, testset, device)\n",
        "    print_confusion(confusion)\n",
        "\n",
        "    for i1, datapoint in enumerate(testset):\n",
        "     #if i == 0:\n",
        "#        save_output(ensemble, datapoint, os.path.join(FLAGS.output_directory, f'example_output_{i}.wav'), device)\n",
        "      model = ensemble\n",
        "      #datapoint\n",
        "      filename = os.path.join(FLAGS.output_directory, f'example_output_{i1}.wav')\n",
        "      #device\n",
        "      gold_mfcc=False\n",
        "\n",
        "      model.eval()\n",
        "      if gold_mfcc:\n",
        "          y = datapoint['audio_features']\n",
        "      else:\n",
        "        with torch.no_grad():\n",
        "            sess = torch.tensor(datapoint['session_ids'], device=device).unsqueeze(0)\n",
        "            X1 = torch.tensor(datapoint['emg'], dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            X_raw = torch.tensor(datapoint['raw_emg'], dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "            pred, _ = model(X1, X_raw, sess)\n",
        "            pred = pred.squeeze(0)\n",
        "\n",
        "            y = pred.cpu().detach().numpy()\n",
        "\n",
        "      wavenet_model = WavenetModel(y.shape[1]).to(device)\n",
        "      assert FLAGS.pretrained_wavenet_model is not None\n",
        "      wavenet_model.load_state_dict(torch.load(FLAGS.pretrained_wavenet_model))\n",
        "#      wavenet_model.load_state_dict(torch.load(FLAGS.pretrained_wavenet_model, map_location=torch.device('cpu')))\n",
        "\n",
        "      #save_wavenet_output(wavenet_model, y, filename, device)\n",
        "      \n",
        "      #wavenet_model\n",
        "      input_data = y\n",
        "      #filename\n",
        "      #device\n",
        "      \n",
        "      wavenet_model.eval()\n",
        "\n",
        "      assert len(input_data.shape) == 2\n",
        "      X = torch.tensor(input_data, dtype=torch.float32).to(device).unsqueeze(0)\n",
        "\n",
        "      wavenet = wavenet_model.wavenet\n",
        "      inference_wavenet = NVWaveNet(**wavenet.export_weights())\n",
        "  #    inference_wavenet = nv_wavenet.NVWaveNet(**wavenet.export_weights())\n",
        "      cond_input = wavenet_model.pre_wavenet_processing(X)\n",
        "\n",
        "      chunk_len = 400\n",
        "      overlap = 1\n",
        "      audio_chunks = []\n",
        "      for i in range(0, cond_input.size(2), chunk_len-overlap):\n",
        "          if cond_input.size(2)-i < overlap:\n",
        "              break # don't make segment at end that doesn't go past overlapped part\n",
        "          cond_chunk = cond_input[:,:,i:i+chunk_len]\n",
        "          wavenet_cond_input = wavenet.get_cond_input(cond_chunk)\n",
        "          audio_data = inference_wavenet.infer(wavenet_cond_input, nv_wavenet.Impl.SINGLE_BLOCK)\n",
        "          audio_chunk = librosa.core.mu_expand(audio_data.squeeze(0).cpu().numpy()-128, 255, True)\n",
        "          audio_chunks.append(audio_chunk)\n",
        "\n",
        "      audio_out = splice_audio(audio_chunks, overlap*160)\n",
        "  \n",
        "      sf.write(filename, audio_out, 16000)\n"
      ],
      "metadata": {
        "id": "CLql-dWMW68Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#asr.py\n",
        "import os\n",
        "import logging\n",
        "\n",
        "import deepspeech\n",
        "import jiwer\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from unidecode import unidecode\n",
        "\n",
        "def evaluate(testset, audio_directory):\n",
        "    model = deepspeech.Model('deepspeech-0.7.0-models.pbmm')\n",
        "    model.enableExternalScorer('deepspeech-0.7.0-models.scorer')\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    for i, datapoint in enumerate(testset):\n",
        "      #if i == 0:\n",
        "        audio, rate = sf.read(os.path.join(audio_directory,f'example_output_{i}.wav'))\n",
        "        assert rate == model.sampleRate(), 'wrong sample rate'\n",
        "        audio_int16 = (audio*(2**15)).astype(np.int16)\n",
        "        text = model.stt(audio_int16)\n",
        "        predictions.append(text)\n",
        "        target_text = unidecode(datapoint['text'])\n",
        "        targets.append(target_text)\n",
        "    transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])\n",
        "    targets = transformation(targets)\n",
        "    predictions = transformation(predictions)\n",
        "    logging.info(f'targets: {targets}')\n",
        "    logging.info(f'predictions: {predictions}')\n",
        "    logging.info(f'wer: {jiwer.wer(targets, predictions)}')\n"
      ],
      "metadata": {
        "id": "tB35a5PF5joA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(testset, FLAGS.output_directory)\n"
      ],
      "metadata": {
        "id": "hjBrlsJX5jhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "for i, datapoint in enumerate(testset):\n",
        " if i == 0:\n",
        "   IPython.display.Audio(os.path.join(FLAGS.output_directory,f'example_output_{i}.wav'))"
      ],
      "metadata": {
        "id": "UXkAL4n35jWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio('evaluation_output/example_output_0.wav')"
      ],
      "metadata": {
        "id": "1VIZdWO-5jHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio('evaluation_output/example_output_1.wav')"
      ],
      "metadata": {
        "id": "GUGJiAR6G5um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio('evaluation_output/example_output_2.wav')"
      ],
      "metadata": {
        "id": "6blrMRO2G5lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio('evaluation_output/example_output_3.wav')"
      ],
      "metadata": {
        "id": "nQj99UGfG5iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio('evaluation_output/example_output_4.wav')"
      ],
      "metadata": {
        "id": "UXt6z5IMG5fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio('evaluation_output/example_output_5.wav')"
      ],
      "metadata": {
        "id": "fgm4Ent8G5cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio('evaluation_output/example_output_6.wav')"
      ],
      "metadata": {
        "id": "iq3NlBlXG5W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio('evaluation_output/example_output_7.wav')"
      ],
      "metadata": {
        "id": "KHSfDPlbG5T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6dDttq92IsX"
      },
      "outputs": [],
      "source": [
        "stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcm-N9yAvtwq"
      },
      "outputs": [],
      "source": [
        "sys.argv = \" --models ./models/transduction_model/model_07.pt --pretrained_wavenet_model ./models/wavenet_model/wavenet_model_50.pt --output_directory evaluation_output\".split(\" \")\n",
        "\n",
        "app.run(main)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6di7H0SSyXgL"
      },
      "outputs": [],
      "source": [
        "testset = EMGDataset(test=True)\n",
        "testset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxDXaj-YzcAc"
      },
      "outputs": [],
      "source": [
        "testset_silent_subset = testset.silent_subset()\n",
        "\n",
        "testset___len__ = testset.__len__()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkXhjDNHz4JW"
      },
      "outputs": [],
      "source": [
        "testset_silent_subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9zuu54Ez1HS"
      },
      "outputs": [],
      "source": [
        "testset___len__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6-tBYA9zzTj"
      },
      "outputs": [],
      "source": [
        "i=0\n",
        "testset___getitem___0 = testset.__getitem__(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XmU40rE0BuB"
      },
      "outputs": [],
      "source": [
        "testset___getitem___0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bku6JTIj1UHe"
      },
      "outputs": [],
      "source": [
        "testset___getitem___0['raw_emg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYjgaYFI1lgf"
      },
      "outputs": [],
      "source": [
        "len(testset___getitem___0['raw_emg'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59xBWppM1ayi"
      },
      "outputs": [],
      "source": [
        "len(testset___getitem___0['raw_emg'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY4rkdRkx-HF"
      },
      "outputs": [],
      "source": [
        "#record_reading.py\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import textwrap\n",
        "import curses\n",
        "import soundfile as sf\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from read_book import Book\n",
        "from record_data import Recorder\n",
        "\n",
        "from absl import flags\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_boolean('debug', False, 'debug')\n",
        "flags.DEFINE_string('output_directory', None, 'where to save outputs')\n",
        "flags.DEFINE_string('book_file', None, 'text to read')\n",
        "flags.mark_flag_as_required('output_directory')\n",
        "flags.mark_flag_as_required('book_file')\n",
        "\n",
        "def display_sentence(sentence, win):\n",
        "    height, width = win.getmaxyx()\n",
        "    win.clear()\n",
        "    wrapped_sentence = textwrap.wrap(sentence, width)\n",
        "    for i, text in enumerate(wrapped_sentence):\n",
        "        if i >= height:\n",
        "            break\n",
        "        win.addstr(i, 0, text)\n",
        "    win.refresh()\n",
        "\n",
        "def save_data(output_idx, data, book):\n",
        "    emg, audio, button, chunk_info = data\n",
        "    emg_file = os.path.join(FLAGS.output_directory, f'{output_idx}_emg.npy')\n",
        "    audio_file = os.path.join(FLAGS.output_directory, f'{output_idx}_audio.flac')\n",
        "    button_file = os.path.join(FLAGS.output_directory, f'{output_idx}_button.npy')\n",
        "    info_file = os.path.join(FLAGS.output_directory, f'{output_idx}_info.json')\n",
        "    assert not os.path.exists(emg_file), 'trying to overwrite existing file'\n",
        "    np.save(emg_file, emg)\n",
        "    sf.write(audio_file, audio, 16000)\n",
        "    np.save(button_file, button)\n",
        "\n",
        "    if book is None:\n",
        "        # special silence segment\n",
        "        bf = ''\n",
        "        bi = -1\n",
        "        t = ''\n",
        "    else:\n",
        "        bf = book.file\n",
        "        bi = book.current_index\n",
        "        t = book.current_sentence()\n",
        "\n",
        "    with open(info_file, 'w') as f:\n",
        "        json.dump({'book':bf, 'sentence_index':bi, 'text':t, 'chunks':chunk_info}, f)\n",
        "\n",
        "\n",
        "def get_ends(data):\n",
        "    emg, audio, button, chunk_info = data\n",
        "    emg_start = emg[:500,:]\n",
        "    emg_end = emg[-500:,:]\n",
        "    dummy_audio = np.zeros(8000)\n",
        "    dummy_button = np.zeros(500, dtype=np.bool)\n",
        "    chunk_info = [(500,8000,500)]\n",
        "    return (emg_start, dummy_audio, dummy_button, chunk_info), (emg_end, dummy_audio, dummy_button, chunk_info)\n",
        "\n",
        "def main(stdscr):\n",
        "    os.makedirs(FLAGS.output_directory, exist_ok=False)\n",
        "    output_idx = 0\n",
        "\n",
        "    curses.curs_set(False)\n",
        "    stdscr.nodelay(True)\n",
        "\n",
        "    text_win = curses.newwin(curses.LINES-1, curses.COLS, 0, 0)\n",
        "\n",
        "    recording = False\n",
        "\n",
        "    with Recorder(debug=FLAGS.debug) as r, Book(FLAGS.book_file) as book:\n",
        "        stdscr.clear()\n",
        "        stdscr.addstr(0,0,'<Press any key to begin.>')\n",
        "        stdscr.refresh()\n",
        "\n",
        "        while True:\n",
        "            r.update()\n",
        "            if not recording:\n",
        "                c = stdscr.getch()\n",
        "                if c >= 0:\n",
        "                    # keypress\n",
        "                    recording = True\n",
        "                    r.get_data() # clear data\n",
        "                    stdscr.addstr(curses.LINES-1, 0, \"Type 'q' to quit, 'n' or ' ' for next, 'r' to restart segment\")\n",
        "                    display_sentence('<silence>', text_win)\n",
        "                    stdscr.refresh()\n",
        "            else:\n",
        "                c = stdscr.getch()\n",
        "                if c < 0:\n",
        "                    # no keypress\n",
        "                    pass\n",
        "                elif c == ord('q'):\n",
        "                    start_data, end_data = get_ends(r.get_data())\n",
        "                    save_data(output_idx, start_data, None)\n",
        "                    break\n",
        "                elif c == ord('n') or c == ord(' '):\n",
        "                    data = r.get_data()\n",
        "\n",
        "                    if output_idx == 0:\n",
        "                        save_data(output_idx, data, None)\n",
        "                    else:\n",
        "                        save_data(output_idx, data, book)\n",
        "                        book.next()\n",
        "\n",
        "                    output_idx += 1\n",
        "                    display_sentence(book.current_sentence(), text_win)\n",
        "                elif c == ord('r'):\n",
        "                    if output_idx == 0:\n",
        "                        r.get_data() # clear data\n",
        "                    else:\n",
        "                        start_data, end_data = get_ends(r.get_data())\n",
        "                        save_data(output_idx, start_data, None)\n",
        "                        output_idx += 1\n",
        "                        save_data(output_idx, end_data, None)\n",
        "                        output_idx += 1\n",
        "\n",
        "\n",
        "FLAGS(sys.argv)\n",
        "curses.wrapper(main)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgUFkujq9j80"
      },
      "outputs": [],
      "source": [
        "stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_QhlT31nJ2s"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vGZCHycmmt0"
      },
      "outputs": [],
      "source": [
        "!python3 ./data_collection/record_data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXYeK1cfb9qB"
      },
      "outputs": [],
      "source": [
        "board.stop_stream()\n",
        "board.release_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwhsStNzSHeW"
      },
      "outputs": [],
      "source": [
        "game_out=0\n",
        "game_out_user_add = 1 << 0\n",
        "game_out_user_attack = 1 << 1\n",
        "game_out_enemy_add = 1 << 2\n",
        "game_out_enemy_attack = 1 << 3\n",
        "game_out_user_killed = 1 << 4\n",
        "game_out_enemy_killed = 1 << 5\n",
        "game_out_user_restored = 1 << 6\n",
        "\n",
        "game_text=False\n",
        "#game_text=True\n",
        "\n",
        "game=0\n",
        "game_easy=10\n",
        "game_stddev_add_user_card=0.1#1\n",
        "game_boss_enemy_cards=5#2\n",
        "\n",
        "game_user_add_card = 1 << 0\n",
        "game_enemy_add_card = 1 << 1\n",
        "game_user_attack_enemy = 1 << 2\n",
        "game_enemy_attack_user = 1 << 3\n",
        "\n",
        "game_max_user_cards=7\n",
        "game_num_user_cards=0\n",
        "#game_user_cards=np.random.rand(game_max_user_cards, dim_sg2)\n",
        "#game_user_cards_life=np.random.rand(game_max_user_cards, dim_sg2)\n",
        "game_user_cards=np.zeros((game_max_user_cards, dim_sg2))\n",
        "game_user_cards_life=np.zeros((game_max_user_cards, dim_sg2))\n",
        "game_killed_user_cards=0\n",
        "\n",
        "game_max_enemy_cards=7\n",
        "game_num_enemy_cards=0\n",
        "#game_enemy_cards=np.random.rand(game_max_enemy_cards, dim_sg2)\n",
        "#game_enemy_cards_life=np.random.rand(game_max_enemy_cards, dim_sg2)\n",
        "game_enemy_cards=np.zeros((game_max_enemy_cards, dim_sg2))\n",
        "game_enemy_cards_life=np.zeros((game_max_enemy_cards, dim_sg2))\n",
        "game_killed_enemy_cards=0\n",
        "\n",
        "game_max_possible_cards=3\n",
        "game_cur_possible_cards=0\n",
        "game_num_possible_cards=0\n",
        "#game_last_possible_cards=np.random.rand(game_max_possible_cards, dim_sg2)\n",
        "game_last_possible_cards=np.zeros((game_max_possible_cards, dim_sg2))\n",
        "#game_compare_with_possible_cards=np.random.rand(game_max_possible_cards+1, dim_sg2)\n",
        "game_compare_with_possible_cards=np.zeros((game_max_possible_cards+1, dim_sg2))\n",
        "\n",
        "game_user_attack_enemy_cards=np.zeros((game_max_user_cards, game_max_enemy_cards, dim_sg2))\n",
        "game_enemy_attack_user_cards=np.zeros((game_max_enemy_cards, game_max_user_cards, dim_sg2))\n",
        "\n",
        "game_user_attack_enemy_cards_possible=np.zeros((game_max_user_cards, game_max_enemy_cards, dim_sg2))\n",
        "game_user_stddev_compare_with_possible_cards=np.zeros((game_max_user_cards, dim_sg2))\n",
        "\n",
        "#ch_types=['eeg']*len(ch_names)\n",
        "#info = mne.create_info(ch_names = ch_names, sfreq = sfreq, ch_types=ch_types)\n",
        "ch_types_wg=['eeg']*len(ch_names_wg)\n",
        "info_wg = mne.create_info(ch_names = ch_names_wg, sfreq = sfreq, ch_types=ch_types_wg)\n",
        "ch_types_wg_l=['eeg']*len(ch_names_wg_l)\n",
        "info_wg_l = mne.create_info(ch_names = ch_names_wg_l, sfreq = sfreq, ch_types=ch_types_wg_l)\n",
        "ch_types_wg_r=['eeg']*len(ch_names_wg_r)\n",
        "info_wg_r = mne.create_info(ch_names = ch_names_wg_r, sfreq = sfreq, ch_types=ch_types_wg_r)\n",
        "ch_types_sg2=['eeg']*len(ch_names_sg2)\n",
        "info_sg2 = mne.create_info(ch_names = ch_names_sg2, sfreq = sfreq, ch_types=ch_types_sg2)\n",
        " \n",
        "#label_names = ch_names\n",
        "#no_names = [''] * len(label_names)\n",
        " \n",
        "from IPython.display import Javascript\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from IPython.display import Javascript\n",
        "import json\n",
        "data_read=False\n",
        " \n",
        "import base64\n",
        "from io import BytesIO\n",
        " \n",
        "data_read=None\n",
        "data_buffer=None\n",
        "data_analyse=None\n",
        "\n",
        "from time import perf_counter\n",
        "time100=perf_counter()\n",
        "time_dir=f'{(time100*1000):9.0f}'\n",
        "#%mkdir '/content/gdrive/MyDrive/EEG-GAN-audio-video/out/{time_dir}'\n",
        "\n",
        "if generate&gen_stylegan2:#_ada or generate_stylegan2_ext:\n",
        "  from IPython.display import Image\n",
        "  import matplotlib.figure\n",
        "  import imageio\n",
        "  #fps=3\n",
        "  #video_out = imageio.get_writer('/content/gdrive/MyDrive/EEG-GAN-audio-video/out/'+time_dir+'/output.mp4', mode='I', fps=fps_sg2, codec='libx264', bitrate='16M')\n",
        " \n",
        "  js_image = \"\"\n",
        "  from PIL import ImageFont, ImageDraw\n",
        " \n",
        "  if generate&gen_sg2_rosasalberto_tf2:\n",
        "    __z = None\n",
        "    dlatents = None\n",
        "  if generate&gen_sg2_moono_tf2:\n",
        "    seed = 6600\n",
        "    rnd = np.random.RandomState(seed)\n",
        "    latents = rnd.randn(1, g_clone.z_dim)\n",
        "    labels = rnd.randn(1, g_clone.labels_dim)\n",
        "    latents = latents.astype(np.float32)\n",
        "    labels = labels.astype(np.float32)\n",
        "    #print([latents, labels])\n",
        "    #image_out = g_clone([latents, labels], training=False, truncation_psi=0.5)\n",
        "    #Gs_kwargs.randomize_noise = False\n",
        "    #image_out = postprocess_images(image_out)\n",
        "    #image_out = image_out.numpy()\n",
        "    #print(image_out)\n",
        "  if generate&gen_sg2_nagolinc_pt:\n",
        "    __z1 = None\n",
        "  if generate&gen_sg2_nvlabs_ada_pt:\n",
        "    ws=None\n",
        "  if generate&gen_sg2_shawwn:\n",
        "    Gs_kwargs = dnnlib.EasyDict()\n",
        "    Gs_kwargs.randomize_noise = False\n",
        "    _z1 = None\n",
        "  elif generate&gen_sg2_shawwn_tpu:\n",
        "    Gs_kwargs = dnnlib.EasyDict()\n",
        "    Gs_kwargs.randomize_noise = False\n",
        "    _z1 = None\n",
        "  elif generate&gen_sg2_aydao_surgery_model_release:\n",
        "#    Gs_kwargs = dnnlib.EasyDict()\n",
        "#    Gs_kwargs.randomize_noise = False\n",
        "    _z1 = None\n",
        "  elif generate&gen_tf1:\n",
        "   Gs_kwargs = dnnlib.EasyDict()\n",
        "   #Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=False)\n",
        "   Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "   Gs_kwargs.randomize_noise = False\n",
        "   _z1 = None\n",
        " \n",
        "if generate&gen_wavegan:\n",
        "  _G_z = None\n",
        "  _G_z_full = None\n",
        "  _G_z_full2 = None\n",
        "  _z = None\n",
        "\n",
        "\n",
        "samples=0\n",
        " \n",
        "last_frame_wg=-1\n",
        "last_frame_sg2=-1\n",
        "time111=perf_counter()\n",
        " \n",
        "last_time=0\n",
        "images=None\n",
        "msg_buffers = None\n",
        "last_sample=0\n",
        "msg_data_buffer = None\n",
        "datas= None\n",
        "epochs=None\n",
        "psd_array_sg2=None\n",
        "def target_func1(comm, msg):\n",
        "  global psd_array_sg2\n",
        "  global device\n",
        "\n",
        "  global game_out\n",
        "  global game_out_user_add\n",
        "  global game_out_user_attack\n",
        "  global game_out_enemy_add\n",
        "  global game_out_enemy_attack\n",
        "  global game_out_user_killed\n",
        "  global game_out_enemy_killed\n",
        "  global game_out_user_restored\n",
        "\n",
        "  global game_text\n",
        "\n",
        "  global game\n",
        "  global game_easy\n",
        "  global game_stddev_add_user_card\n",
        "  global game_boss_enemy_cards\n",
        "  \n",
        "  global game_user_add_card\n",
        "  global game_enemy_add_card\n",
        "  global game_user_attack_enemy\n",
        "  global game_enemy_attack_user\n",
        "\n",
        "  global game_max_user_cards\n",
        "  global game_num_user_cards\n",
        "  global game_user_cards\n",
        "  global game_user_cards_life\n",
        "  global game_killed_user_cards\n",
        "\n",
        "  global game_max_enemy_cards\n",
        "  global game_num_enemy_cards\n",
        "  global game_enemy_cards\n",
        "  global game_enemy_cards_life\n",
        "  global game_killed_enemy_cards\n",
        "\n",
        "  global game_max_possible_cards\n",
        "  global game_cur_possible_cards\n",
        "  global game_num_possible_cards\n",
        "  global game_last_possible_cards\n",
        "\n",
        "  global datas,epochs\n",
        "  #print(comm, msg)\n",
        "  global debug\n",
        "  global images, fps2_sg2\n",
        "  time000=perf_counter()\n",
        "  global msg_buffers\n",
        "  msg_buffers=msg['buffers']\n",
        "#  if generate&gen_game:\n",
        "#    game = 0\n",
        "#    if game_num_user_cards==0:\n",
        "#      game = game | game_user_add_card\n",
        "##    if len(game_user_cards)==1:\n",
        "##      game = game | game_user_add25\n",
        "##    if len(game_user_cards)==2:\n",
        "##      game = game | game_user_addcoh\n",
        "#    if game_num_user_cards>=1:\n",
        "#      if game_num_enemy_cards==0:\n",
        "#         game = game | game_enemy_add_card\n",
        "#      if game_num_enemy_cards>=1:\n",
        "#        game = game | game_user_attack_enemy\n",
        "#        game = game | game_enemy_attack_user\n",
        "\n",
        "  #print(msg['buffers'])\n",
        "  #import dnnlib \n",
        "  #import dnnlib.tflib as tflib \n",
        "  #import PIL.Image \n",
        "  #from tqdm import tqdm\n",
        "  ##global _G, _D, Gs\n",
        "  #tflib.init_tf()\n",
        "  #with dnnlib.util.open_url(network_pkl) as fp:\n",
        "  #   _G, _D, Gs = pickle.load(fp)\n",
        " \n",
        "  # Only send the response if it's the data we are expecting.\n",
        "  #if msg['content']['data'] == 'the data':\n",
        "  #  comm.send({\n",
        "  #        'response': 'got comm open!',\n",
        "  #      }, None, msg['buffers']);\n",
        "  #  print(msg['buffers'])\n",
        "  #else:\n",
        "    #print(msg['content']['data'])\n",
        "    #data_read=True\n",
        "  global last_time\n",
        "  global time100\n",
        "  global samples\n",
        "  #array_to_receive_as_json = '[[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0],[0]]'\n",
        "  array_to_receive_as_json = msg['content']['data']\n",
        "  #global data_read\n",
        "  #data_read = json.loads(array_to_receive_as_json)\n",
        "  \n",
        "  msg_bytes=msg_buffers[0].tobytes()\n",
        "  global msg_data_buffer, last_sample\n",
        "  if msg_data_buffer is None:\n",
        "    msg_data_buffer=msg_bytes\n",
        "  else:\n",
        "    #print(len(msg_data_buffer),len(msg_bytes))\n",
        "    #print(type(msg_data_buffer),type(msg_bytes))\n",
        "    msg_data_buffer = msg_data_buffer + msg_bytes\n",
        "    #print(shape(msg_data_buffer))\n",
        "\n",
        "  #msg_data_channels=128\n",
        "  #msg_data_channels=32\n",
        "  global data_channels\n",
        "  msg_data_channels=data_channels\n",
        "  #msg_data_length=int(len(msg_bytes)/(msg_data_channels*4))\n",
        "  msg_data_slice_len=1+1+msg_data_channels*3+2*3+1;\n",
        "  msg_data_buffer_len=len(msg_data_buffer)\n",
        "  #print(msg_data_buffer_len)\n",
        "  get_data=False\n",
        "  msg_data_shift=0\n",
        "  while not(get_data):\n",
        "    if len(msg_data_buffer)+msg_data_shift>=msg_data_slice_len:\n",
        "      if msg_data_buffer[msg_data_shift]==0xA0:\n",
        "        if msg_data_buffer[msg_data_shift+msg_data_slice_len-1]==0xC0:\n",
        "          get_data=True\n",
        "    else:\n",
        "      #print('shift, no data:',msg_data_shift)\n",
        "      comm.send({\n",
        "          'response': 'close',\n",
        "          #'response': 'no data',\n",
        "        }, None, [memoryview([])]);\n",
        "    if not(get_data):\n",
        "      msg_data_shift=msg_data_shift+1\n",
        "\n",
        "  #print('shift:',msg_data_shift)\n",
        "  msg_data_buffer=msg_data_buffer[msg_data_shift:msg_data_buffer_len-msg_data_shift]\n",
        "  #print('shift ok')\n",
        "\n",
        "  msg_data_length=int(len(msg_data_buffer)/(msg_data_slice_len))\n",
        "  #print(msg_data_length)\n",
        "  #print(len(msg_bytes)%(1+1+msg_data_channels*3+2*3+1))\n",
        "\n",
        "  data_read=np.zeros((msg_data_channels,msg_data_length), np.int32)\n",
        "  for j in range(msg_data_length):\n",
        "    new_sample_read=msg_data_buffer[j*(msg_data_slice_len)+1]\n",
        "    new_sample=(last_sample+1)%256\n",
        "    #print('samples:',new_sample_read,new_sample)\n",
        "    if not(new_sample_read==new_sample):\n",
        "      print('skip samples:',(new_sample_read-new_sample+256)%256,'=',new_sample_read,'-',new_sample)\n",
        "    last_sample=new_sample_read\n",
        "    for i in range(msg_data_channels):\n",
        "      data_read[i][j]=int.from_bytes(msg_data_buffer[j*(msg_data_slice_len)+1+1+i*3:\n",
        "                                               j*(msg_data_slice_len)+1+1+i*3+2], \n",
        "                                               byteorder='big', signed=True)\n",
        " \n",
        "  msg_data_buffer_cut=len(msg_data_buffer)%(msg_data_slice_len)\n",
        "  msg_data_buffer_len=len(msg_data_buffer)\n",
        "  msg_data_buffer=msg_data_buffer[msg_data_buffer_len-msg_data_buffer_cut:msg_data_buffer_len]\n",
        "\n",
        "  #print('cut:',msg_data_buffer_cut)\n",
        "  #print(data_read)\n",
        "\n",
        "  #for i in range(msg_data_channels):\n",
        "  #  for j in range(msg_data_length):\n",
        "  #    data_read[i][j]=int.from_bytes(msg_bytes[i*msg_data_length+j:i*msg_data_length+j+3], byteorder='big', signed=True)\n",
        "  #print(msg_data_array)\n",
        "\n",
        "  #print('got data')\n",
        "  #print(data_read)\n",
        "  new_data_len=len(data_read[0])\n",
        "  #print(new_data_len)\n",
        "  samples=samples+new_data_len#len(data_analyse[0])\n",
        "  time110=perf_counter()\n",
        "  out_text=f'{(samples):7.0f}'+' sam, '\n",
        "  out_text=out_text+f'{(new_data_len):4.0f}'+\" sam/pac, \"\n",
        "  out_text=out_text+f'{(time110-time100):7.2f}'+\" sec, \"\n",
        "  out_text=out_text+f'{(samples/(time110-time100)):7.2f}'+' sam/sec'\n",
        "  out_text=out_text+f'{(time110-time100)-last_time:7.2f}'+\" sec, \"\n",
        "  out_text=out_text+f'{(new_data_len/((time110-time100)-last_time)):7.2f}'+' sam/sec'\n",
        "  last_time=(time110-time100)\n",
        "  #comm.send({\n",
        "  #        'response': out_text,\n",
        "  #      }, None, msg['buffers']);\n",
        "  if debug:\n",
        "    print(out_text)\n",
        "  global ch_locations,bands,duration,overlap,methods,uVperStep,sfreq,dim_sg2,dim_wg\n",
        "  global ch_names_wg, info_wg, ch_names_wg_l, info_wg_l, ch_names_wg_r, info_wg_r, ch_names_sg2, info_sg2\n",
        "  #global label_names,ch_locations,info,bands,duration,overlap,methods,uVperStep,sfreq,dim_sg2,dim_wg\n",
        "  #print(duration)\n",
        "  send_wg=False\n",
        "  send_sg2=False\n",
        "  \n",
        "  if not (data_read is None):\n",
        "      #print('read_data-data_read')\n",
        "      #print(len(data_read[0]))\n",
        "      global data_buffer, data_analyse\n",
        "      if data_buffer is None:\n",
        "        data_buffer=data_read\n",
        "      else:\n",
        "        data_buffer=np.concatenate((data_buffer,data_read),axis=1)\n",
        "      data_read = None\n",
        " \n",
        "      data_buffer_len=len(data_buffer[0])\n",
        "      #print(data_buffer_len)\n",
        "      #if (int(duration*sfreq)+1)>(int(sfreq/fps)):\n",
        "      if True:\n",
        "        data_slice_len=int(duration*sfreq)+1\n",
        "      #else:\n",
        "      #  data_slice_len=int(sfreq/fps)+1\n",
        "      #print(data_slice_len)\n",
        "      if data_buffer_len>data_slice_len:\n",
        "        data_buffer=data_buffer[0:len(data_buffer),data_buffer_len-data_slice_len:data_buffer_len]\n",
        "      #print(len(data_buffer[0]))\n",
        "      #print(data_slice_len)\n",
        "      data_analyse=None\n",
        "      if len(data_buffer[0])==data_slice_len:\n",
        "       global last_frame_sg2, fps_sg2\n",
        "       global last_frame_wg, fps_wg\n",
        "       if generate&gen_wavegan: \n",
        "#        this_frame_wg=int((time000-time100)*fps_wg)#+1/(2*fps_sg2/fps_wg))\n",
        "        this_frame_wg=int(((time000-time100)*fps_wg)+1/(2*fps_sg2/fps_wg))\n",
        "        if this_frame_wg>last_frame_wg:\n",
        "          data_analyse=data_buffer\n",
        "          last_frame_wg=this_frame_wg\n",
        "          send_wg=True\n",
        "          #print(f'{(time000-time100)*fps_wg:7.2f}'+\" frame wavegan\")\n",
        "       if generate&gen_stylegan2:   \n",
        "        #global last_frame_sg2, fps_sg2\n",
        "        this_frame_sg2=int((time000-time100)*fps_sg2)\n",
        "        if this_frame_sg2>last_frame_sg2:\n",
        "          #if not send_wg:\n",
        "          data_analyse=data_buffer\n",
        "          last_frame_sg2=this_frame_sg2\n",
        "          send_sg2=True\n",
        "          #print(f'{(time000-time100)*fps_sg2:7.2f}'+\" frame stylegan2\")\n",
        "      #data_buffer=data_analyse\n",
        "      #print('read_data-data_analyse')\n",
        "      #print(len(data_analyse[0]))\n",
        "  \n",
        "  #if False: \n",
        "  #if True: \n",
        "  if not(data_analyse is None):\n",
        "    #if (time111-time000)>=(1/fps):\n",
        "    time0111=perf_counter()\n",
        "    parts=1\n",
        "    if (generate&gen_wg_stereo):\n",
        "      if (send_wg):\n",
        "        parts=2\n",
        "    for part in range(parts):\n",
        "      #print(parts)\n",
        "      time111=perf_counter()\n",
        "      if send_wg:\n",
        "        ch_names=ch_names_wg\n",
        "        ch_locations=ch_locations_wg\n",
        "        info=info_wg\n",
        "        if generate&gen_wg_stereo:\n",
        "          #print(part)\n",
        "          if part==0:\n",
        "            ch_names=ch_names_wg_l\n",
        "            ch_locations=ch_locations_wg_l\n",
        "            info=info_wg_l\n",
        "            #print('l')\n",
        "          if part==1:\n",
        "            ch_names=ch_names_wg_r\n",
        "            ch_locations=ch_locations_wg_r\n",
        "            info=info_wg_r\n",
        "            #print('r')\n",
        "      elif send_sg2:\n",
        "        ch_names=ch_names_sg2\n",
        "        ch_locations=ch_locations_sg2\n",
        "        info=info_sg2\n",
        "      data_uv = [0]*len(ch_names)\n",
        "      for j in range(len(ch_names)):\n",
        "        data_uv[j]=[0]*len(data_analyse[ch_locations[j]])\n",
        "        for i in range(len(data_analyse[ch_locations[j]])):\n",
        "          data_uv[j][i] = data_analyse[ch_locations[j]][i] * uVperStep * 2\n",
        "      if (part==parts-1):\n",
        "        data_analyse=None\n",
        "  \n",
        "      time002=perf_counter()\n",
        "      #print (f'002: {(time002-time000):.1f}s')\n",
        "\n",
        "      raw = mne.io.RawArray(data_uv, info, verbose=50)\n",
        "      \n",
        "      time003=perf_counter()\n",
        "      #print (f'003: {(time003-time000):.1f}s')\n",
        "      datas=[]\n",
        "      for band in range(len(bands)):\n",
        "        datas.append(raw)\n",
        "      time004=perf_counter()\n",
        "      epochs = []\n",
        "      #print (f'004: {(time004-time000):.1f}s')\n",
        "      for band in range(len(bands)):\n",
        "        epochs.append(mne.make_fixed_length_epochs(datas[band], \n",
        "                                                   duration=duration/2, preload=True, \n",
        "#                                                   overlap=duration/2-0.1, verbose=50))\n",
        "#                                                   overlap=duration-0.1))#, verbose=50))\n",
        "                                                   overlap=overlap, verbose=50))\n",
        "      time005=perf_counter()\n",
        "      #print (f'005: {(time005-time000):.1f}s')\n",
        "      n_generate=int((float(len(data_uv[0]))/float(sfreq))/duration)\n",
        "      #n_generate=1\n",
        "      n_generate=int(fps2_sg2/fps_sg2)\n",
        "      #n_generate=1\n",
        "      part_len = int(fps2_sg2/fps_sg2)\n",
        "#      part_len = 1\n",
        "      #dim = 512\n",
        "      #part_len = 1\n",
        " \n",
        "      n_parts = n_generate//part_len\n",
        "      if n_generate%part_len>0:\n",
        "        n_parts=n_parts+1\n",
        "      #n_parts=1\n",
        "      global vol\n",
        "    \n",
        "      if generate&gen_wavegan:\n",
        "        psd_array_wg=np.zeros([part_len, dim_wg]) \n",
        "      if generate&gen_stylegan2:\n",
        "        if generate&gen_game:\n",
        "          if generate&gen_game_mode1:\n",
        "            psd_array_sg2=np.zeros([part_len, dim_sg2]) \n",
        "          if generate&gen_game_mode3:\n",
        "            psd_array_sg2=np.zeros([part_len*(3+1), dim_sg2]) \n",
        "        else:\n",
        "          psd_array_sg2=np.zeros([part_len, dim_sg2]) \n",
        "      time006=perf_counter()\n",
        "      #print (f'006: {(time006-time000):.1f}s')\n",
        "    \n",
        "      for j in range(n_parts): # display separate audio for each break\n",
        "        for i in range(part_len): # display separate audio for each break\n",
        "            ji = j * part_len + i\n",
        "            \n",
        "            if (i==0) and (n_generate-ji<part_len):\n",
        "              if generate&gen_wavegan:\n",
        "                psd_array_wg=np.zeros([(n_generate-ji), dim_wg]) \n",
        "              if generate&gen_stylegan2:\n",
        "                if generate&gen_game:\n",
        "                  if generate&gen_game_mode1:\n",
        "                    psd_array_sg2=np.zeros([(n_generate-ji), dim_sg2]) \n",
        "                  if generate&gen_game_mode3:\n",
        "                    psd_array_sg2=np.zeros([(n_generate-ji)*(3+1), dim_sg2]) \n",
        "                else:\n",
        "                  psd_array_sg2=np.zeros([(n_generate-ji), dim_sg2]) \n",
        "        \n",
        "            sfreq = raw.info['sfreq']  # the sampling frequency\n",
        "            time0061=perf_counter()\n",
        "            #print (f'0061: {(time0061-time000):.1f}s')\n",
        "            \n",
        "            if generate&gen_wavegan:# and send_wg:\n",
        "              psds_wg=np.zeros(dim_wg)\n",
        "            if generate&gen_stylegan2:\n",
        "              if generate&gen_game:\n",
        "                if generate&gen_game_mode1:\n",
        "                  psds_sg2=np.zeros(dim_sg2)\n",
        "                if generate&gen_game_mode3:\n",
        "                  psds_sg2=np.zeros(dim_sg2)\n",
        "                  psds_sg2_1=np.zeros(dim_sg2)\n",
        "                  psds_sg2_2=np.zeros(dim_sg2)\n",
        "                  psds_sg2_3=np.zeros(dim_sg2)\n",
        "              else:\n",
        "                psds_sg2=np.zeros(dim_sg2)\n",
        "            \n",
        "            for method in range(len(methods)):\n",
        "             for band in range(len(bands)):\n",
        "              fmin=bands[band][0]\n",
        "              fmax=bands[band][1]\n",
        "              time0071=perf_counter()\n",
        "              #print (f'0071: {(time0071-time000):.1f}s')\n",
        "              #if band == 0:\n",
        "              #print(epochs[band])\n",
        "              con, freqs, times, n_epochs, n_tapers = spectral_connectivity(\n",
        "                #epochs[band], method=methods[method], mode='fourier', sfreq=sfreq, fmin=fmin,\n",
        "######                epochs[band][ji:ji+1], method=methods[method], mode='multitaper', sfreq=sfreq, fmin=fmin,\n",
        "                epochs[band], method=methods[method], mode='multitaper', sfreq=sfreq, fmin=fmin,\n",
        "                #epochs[band][ji:ji+1], method=methods[method], mode='multitaper', sfreq=sfreq, fmin=fmin,\n",
        "                #fmax=fmax, faverage=True, mt_adaptive=False, n_jobs=1, verbose=50)\n",
        "                fmax=fmax, faverage=True, mt_adaptive=True, n_jobs=8, verbose=50)\n",
        "#                fmax=fmax, faverage=True, mt_adaptive=True, n_jobs='cuda', verbose=50)\n",
        "              time007=perf_counter()\n",
        "              #print (f'007: {(time007-time000):.1f}s')\n",
        "              coh_len=int(len(ch_names)*(len(ch_names)-1)/2)\n",
        "              psds_shift1=int(round(method*len(bands)+band)*coh_len)\n",
        "              ji1=0\n",
        "              for j1 in range(0,len(ch_names)): # display separate audio for each break\n",
        "                for i1 in range(0,j1): # display separate audio for each break\n",
        "                  #print(ji1)\n",
        "                  if generate&gen_wavegan:\n",
        "                   if ji1+psds_shift1<dim_wg:\n",
        "                    psds_wg[ji1+psds_shift1]=(con[j1][i1][0]-0.5)*1\n",
        "                  if generate&gen_stylegan2:\n",
        "                   for i01 in range(0,int(dim_sg2/coh_len)):\n",
        "                     if ji1+psds_shift1+coh_len*i01<dim_sg2:\n",
        "                      #print(ji1+psds_shift1+coh_len*i01)\n",
        "                      if generate&gen_game:\n",
        "                        if generate&gen_game_mode1:\n",
        "                          psds_sg2[ji1+psds_shift1+coh_len*i01]=(con[j1][i1][0]-0.5)*1\n",
        "                        if generate&gen_game_mode3:\n",
        "                          psds_sg2[ji1+psds_shift1+coh_len*i01]=(con[j1][i1][0]-0.5)*1\n",
        "                          if ch_locations_sg2[j1] in ch_locations_wg_l:\n",
        "                            if ch_locations_sg2[i1] in ch_locations_wg_l:\n",
        "                              psds_sg2_1[ji1+psds_shift1+coh_len*i01]=(con[j1][i1][0]-0.5)*1\n",
        "                            if ch_locations_sg2[i1] in ch_locations_wg_r:\n",
        "                              psds_sg2_2[ji1+psds_shift1+coh_len*i01]=(con[j1][i1][0]-0.5)*1\n",
        "                          if ch_locations_sg2[j1] in ch_locations_wg_r:\n",
        "                            if ch_locations_sg2[i1] in ch_locations_wg_l:\n",
        "                              psds_sg2_2[ji1+psds_shift1+coh_len*i01]=(con[j1][i1][0]-0.5)*1\n",
        "                            if ch_locations_sg2[i1] in ch_locations_wg_r:\n",
        "                              psds_sg2_3[ji1+psds_shift1+coh_len*i01]=(con[j1][i1][0]-0.5)*1\n",
        "                      else:\n",
        "                        psds_sg2[ji1+psds_shift1+coh_len*i01]=(con[j1][i1][0]-0.5)*1\n",
        "\n",
        "                  ji1 = ji1+1\n",
        "\n",
        "              if generate&gen_game:\n",
        "                game_out=0\n",
        "                #print('con:',con[0])\n",
        "                #print('game_in')\n",
        "                #print('game_last_possible_cards:',game_last_possible_cards)\n",
        "                for j1 in range(0,dim_sg2):  \n",
        "                  #print('j1:',j1)\n",
        "                  game_last_possible_cards[game_cur_possible_cards][j1]=psds_sg2[j1]+0.5\n",
        "                game_cur_possible_cards=game_cur_possible_cards+1\n",
        "                \n",
        "                if game_cur_possible_cards>0:\n",
        "                  for i1 in range(game_cur_possible_cards):\n",
        "                    np.copyto(game_compare_with_possible_cards[i1],game_last_possible_cards[i1])\n",
        "                  for i2 in range(game_num_user_cards):\n",
        "                    np.copyto(game_compare_with_possible_cards[game_cur_possible_cards],game_user_cards[i2])\n",
        "                    game_user_stddev_compare_with_possible_cards[i2]=np.std(game_compare_with_possible_cards[:game_cur_possible_cards+1], axis=0)\n",
        "                    for j2 in range(game_num_enemy_cards):\n",
        "                      for j1 in range(0,dim_sg2):\n",
        "                        game_user_attack_enemy_cards_possible[i2][j2][j1]=((game_user_cards[i2][j1]/(game_enemy_cards[j2][j1]+1))*(1))/game_num_enemy_cards\n",
        "                        game_user_attack_enemy_cards[i2][j2][j1]=((game_user_cards[i2][j1]/(game_enemy_cards[j2][j1]+1))*(1-game_user_stddev_compare_with_possible_cards[i2][j1]))/game_num_enemy_cards\n",
        "                \n",
        "                if game_cur_possible_cards==game_max_possible_cards:\n",
        "                  game_cur_possible_cards=0\n",
        "                game_num_possible_cards=game_num_possible_cards+1\n",
        "                if game_num_possible_cards>game_max_possible_cards:\n",
        "                  game_num_possible_cards=game_max_possible_cards\n",
        "                #print('game_cur_possible_cards:',game_cur_possible_cards)\n",
        "\n",
        "                if game_cur_possible_cards==0:\n",
        "                  for i1 in range(game_max_possible_cards):\n",
        "                    np.copyto(game_compare_with_possible_cards[i1],game_last_possible_cards[i1])\n",
        "                  if game&game_user_attack_enemy:\n",
        "                    if game_text:\n",
        "                      print('game_user_attack_enemy')\n",
        "                    game_out = game_out | game_out_user_attack\n",
        "                    for i2 in range(game_num_user_cards):\n",
        "                      np.copyto(game_compare_with_possible_cards[game_num_possible_cards],game_user_cards[i2])\n",
        "                      game_stddev_compare_with_possible_cards=np.std(game_compare_with_possible_cards, axis=0)\n",
        "                      #print(np.array2string(((game_stddev_compare_with_possible_cards)*10).astype(int),separator='',max_line_width=130))\n",
        "                      #print('game_stddev_last_possible_cards:',game_stddev_last_possible_cards)\n",
        "                      #print('max(game_stddev_last_possible_cards):',max(game_stddev_last_possible_cards))\n",
        "                      #print('game_stddev_compare_with_possible_cards:',game_stddev_compare_with_possible_cards)\n",
        "                      #if np.max(game_stddev_compare_with_possible_cards)<0.2:\n",
        "                      for j2 in range(game_num_enemy_cards):\n",
        "                        #print(i2,j2)\n",
        "                        for j1 in range(0,dim_sg2):\n",
        "                          game_user_attack_enemy_cards[i2][j2][j1]=((game_user_cards[i2][j1]/(game_enemy_cards[j2][j1]+1))*(1-game_stddev_compare_with_possible_cards[j1]))/game_num_enemy_cards\n",
        "                          game_enemy_cards_life[j2][j1]=game_enemy_cards_life[j2][j1]-game_user_attack_enemy_cards[i2][j2][j1]\n",
        "                            #if game_enemy_cards_life[j2][j1]<0:\n",
        "                            #  game_enemy_cards_life[j2][j1]=0\n",
        "                            #coh[j1]=game_enemy_cards_life[j2][j1]\n",
        "                    #print('np.max(game_enemy_cards_life):',np.max(game_enemy_cards_life))\n",
        "                  #print('game_enemy_cards_life:',game_enemy_cards_life)\n",
        "                  #print('np.sum(game_enemy_cards_life,axis=0):',np.sum(game_enemy_cards_life,axis=0))\n",
        "                  if np.max(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0))<=0:\n",
        "                  #if np.max(game_enemy_cards_life)<=0:\n",
        "                    game = game_enemy_add_card\n",
        "                    if game_num_enemy_cards>0:\n",
        "                      game_killed_enemy_cards=game_killed_enemy_cards+1\n",
        "                      #game_killed_enemy_cards=game_killed_enemy_cards+game_num_enemy_cards\n",
        "                      game_num_enemy_cards=0\n",
        "                      if game_text:\n",
        "                        print('game_killed_enemy_cards:',game_killed_enemy_cards)\n",
        "                      game_out = game_out | game_out_enemy_killed\n",
        "                      if game_killed_enemy_cards%game_boss_enemy_cards==(game_boss_enemy_cards-1):\n",
        "                        game = game | game_user_add_card\n",
        "                      if game_killed_enemy_cards%game_boss_enemy_cards==0:\n",
        "                        if game_text:\n",
        "                          print('user_cards_life_restored')\n",
        "                        game_out = game_out | game_out_user_restored\n",
        "                        for i1 in range(game_num_user_cards):\n",
        "                          for j1 in range(0,dim_sg2):  \n",
        "                            game_user_cards_life[i1][j1]=game_user_cards[i1][j1]\n",
        "\n",
        "                  if game&game_enemy_attack_user:\n",
        "                    if game_text:\n",
        "                      print('game_enemy_attack_user')\n",
        "                    game_out = game_out | game_out_user_attack\n",
        "                    for j2 in range(game_num_user_cards):\n",
        "                      np.copyto(game_compare_with_possible_cards[game_num_possible_cards],game_user_cards[j2])\n",
        "                      game_stddev_compare_with_possible_cards=np.std(game_compare_with_possible_cards, axis=0)\n",
        "                      #print('game_stddev_last_possible_cards:',game_stddev_last_possible_cards)\n",
        "                      #print('max(game_stddev_last_possible_cards):',max(game_stddev_last_possible_cards))\n",
        "                      #print('game_stddev_compare_with_possible_cards:',game_stddev_compare_with_possible_cards)\n",
        "                      #if np.max(game_stddev_compare_with_possible_cards)>=0.2:\n",
        "                      for i2 in range(game_num_enemy_cards):\n",
        "                        #print(j2,i2)\n",
        "                        for j1 in range(0,dim_sg2):\n",
        "                          game_enemy_attack_user_cards[j2][i2][j1]=((game_enemy_cards[i2][j1]/(game_user_cards[j2][j1]+1))/(1-game_stddev_compare_with_possible_cards[j1]))/game_num_user_cards\n",
        "                          game_user_cards_life[j2][j1]=game_user_cards_life[j2][j1]-game_enemy_attack_user_cards[j2][i2][j1]\n",
        "                  if np.max(np.sum(game_user_cards_life[:game_num_user_cards],axis=0))<=0:\n",
        "                  #if np.max(game_user_cards_life)<=0:\n",
        "                    if game_num_user_cards>0:\n",
        "                      game_killed_user_cards=game_killed_user_cards+game_num_user_cards\n",
        "                      if game_text:\n",
        "                        print('game_killed_user_cards:',game_killed_user_cards)\n",
        "                      game_out = game_out | game_out_user_killed\n",
        "                    game_num_user_cards=0\n",
        "                    game_num_enemy_cards=0\n",
        "                    game = game_user_add_card | game_enemy_add_card\n",
        "                    game_killed_user_cards=0\n",
        "                    game_killed_enemy_cards=0\n",
        "                  if game&game_user_add_card:\n",
        "                    if game_num_user_cards<game_max_user_cards:\n",
        "                    #if game_num_possible_cards==game_max_possible_cards:\n",
        "                      #print('game_last_possible_cards:',game_last_possible_cards)\n",
        "                      game_stddev_last_possible_cards=np.std(game_last_possible_cards, axis=0)\n",
        "                      #print('game_stddev_last_possible_cards:',game_stddev_last_possible_cards)\n",
        "                      #print('max(game_stddev_last_possible_cards):',max(game_stddev_last_possible_cards))\n",
        "                      if np.max(game_stddev_last_possible_cards)<game_stddev_add_user_card:\n",
        "                        for j1 in range(0,dim_sg2):  \n",
        "                          #con[j1]=con[j1]\n",
        "                          #game_user_cards[game_num_user_cards][j1]=np.avg(game_last_possible_cards[:game_num_possible_cards][j1])\n",
        "                          game_user_cards[game_num_user_cards][j1]=0\n",
        "                          #for j2 in range(game_num_possible_cards):\n",
        "                          #  game_user_cards[game_num_user_cards][j1]=game_user_cards[game_num_user_cards][j1]+game_last_possible_cards[j2][j1]\n",
        "                          game_user_cards[game_num_user_cards][j1]=game_user_cards[game_num_user_cards][j1]+game_last_possible_cards[game_num_possible_cards-1][j1]\n",
        "                          #game_user_cards[game_num_user_cards][j1]=game_user_cards[game_num_user_cards][j1]/game_num_possible_cards\n",
        "                          game_user_cards_life[game_num_user_cards][j1]=game_user_cards[game_num_user_cards][j1]\n",
        "                          #game_user_cards[game_num_user_cards][j1]=psds_sg2[j1]+0.5\n",
        "                          #game_user_cards_life[game_num_user_cards][j1]=psds_sg2[j1]+0.5\n",
        "                        #print('game_user_cards:',game_user_cards)\n",
        "                        game_num_user_cards=game_num_user_cards+1\n",
        "                        if game_text:\n",
        "                          print('game_num_user_cards+1')\n",
        "                        game_out = game_out | game_out_user_add\n",
        "                  if game&game_enemy_add_card:\n",
        "                    if game_num_user_cards>0:\n",
        "                      for i1 in range(game_killed_enemy_cards//game_boss_enemy_cards+1):\n",
        "                        if game_num_enemy_cards<game_max_enemy_cards:\n",
        "                          for j1 in range(0,dim_sg2):  \n",
        "                            #con[j1]=1-con[j1]\n",
        "                            if game_killed_enemy_cards%game_boss_enemy_cards==(game_boss_enemy_cards-1):\n",
        "                              #game_enemy_cards[game_num_enemy_cards][j1]=1-(1-np.avg(game_user_cards[:game_num_user_cards][j1]))*2/3\n",
        "                              game_enemy_cards[game_num_enemy_cards][j1]=0\n",
        "                              #for j2 in range(game_num_user_cards):\n",
        "                              #  game_enemy_cards[game_num_enemy_cards][j1]=game_enemy_cards[game_num_enemy_cards][j1]+1-(1-game_user_cards[j2][j1])*(1-1/game_easy)\n",
        "                              game_enemy_cards[game_num_enemy_cards][j1]=game_enemy_cards[game_num_enemy_cards][j1]+1-(1-game_user_cards[game_num_user_cards-1][j1])*(1-1/game_easy)\n",
        "                              #game_enemy_cards[game_num_enemy_cards][j1]=game_enemy_cards[game_num_enemy_cards][j1]/game_num_user_cards\n",
        "                              game_enemy_cards_life[game_num_enemy_cards][j1]=game_enemy_cards[game_num_enemy_cards][j1]\n",
        "                              #game_enemy_cards[game_num_enemy_cards][j1]=1-(1-(psds_sg2[j1]+0.5))*2/3\n",
        "                              #game_enemy_cards_life[game_num_enemy_cards][j1]=1-(1-(psds_sg2[j1]+0.5))*2/3\n",
        "                            else:\n",
        "                              #game_enemy_cards[game_num_enemy_cards][j1]=np.avg(game_user_cards[:game_num_user_cards][j1])*2/3\n",
        "                              game_enemy_cards[game_num_enemy_cards][j1]=0\n",
        "                              #for j2 in range(game_num_user_cards):\n",
        "                              #  game_enemy_cards[game_num_enemy_cards][j1]=game_enemy_cards[game_num_enemy_cards][j1]+game_user_cards[j2][j1]*1/game_easy\n",
        "                              game_enemy_cards[game_num_enemy_cards][j1]=game_enemy_cards[game_num_enemy_cards][j1]+game_user_cards[game_num_user_cards-1][j1]*1/game_easy\n",
        "                              #game_enemy_cards[game_num_enemy_cards][j1]=game_enemy_cards[game_num_enemy_cards][j1]/game_num_user_cards\n",
        "                              game_enemy_cards_life[game_num_enemy_cards][j1]=game_enemy_cards[game_num_enemy_cards][j1]\n",
        "                              #game_enemy_cards[game_num_enemy_cards][j1]=(psds_sg2[j1]+0.5)/3\n",
        "                              #game_enemy_cards_life[game_num_enemy_cards][j1]=(psds_sg2[j1]+0.5)/3\n",
        "                          #print('game_enemy_cards:',game_enemy_cards)\n",
        "                          game_num_enemy_cards=game_num_enemy_cards+1\n",
        "                          game = game_user_attack_enemy | game_enemy_attack_user\n",
        "                          if game_text:\n",
        "                            print('game_num_enemy_cards+1')\n",
        "                          game_out = game_out | game_out_enemy_add\n",
        "\n",
        "              \n",
        "                sum_user_pos_life=0\n",
        "                sum_enemy_pos_life=0\n",
        "                if (game_num_user_cards>0) and (game_num_enemy_cards>0):\n",
        "                  user_cards_array=(np.sum(game_user_cards_life[:game_num_user_cards],axis=0)+np.abs(np.sum(game_user_cards_life[:game_num_user_cards],axis=0)))/2\n",
        "                  user_life_array=(np.sum(game_user_cards[:game_num_user_cards],axis=0)+np.abs(np.sum(game_user_cards[:game_num_user_cards],axis=0)))/2\n",
        "                  enemy_cards_array=(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)+np.abs(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)))/2\n",
        "                  enemy_life_array=(np.sum(game_enemy_cards[:game_num_enemy_cards],axis=0)+np.abs(np.sum(game_enemy_cards[:game_num_enemy_cards],axis=0)))/2\n",
        "                  sum_user_pos_life=np.sum(user_life_array)/dim_sg2\n",
        "                  sum_enemy_pos_life=np.sum(enemy_life_array)/dim_sg2\n",
        "                out_text_game=f'{(np.max(np.std(game_last_possible_cards, axis=0))):7.2f}'+' max_std_possible, '\n",
        "                out_text_game=out_text_game+f'{(np.average(game_user_cards_life[:game_num_user_cards])):7.2f}'+\" average_user, \"\n",
        "                out_text_game=out_text_game+f'{(np.average(game_enemy_cards_life[:game_num_enemy_cards])):7.2f}'+' average_enemy, '\n",
        "                out_text_game=out_text_game+f'{sum_user_pos_life:7.2f}'+\" sum_pos_user, \"\n",
        "                out_text_game=out_text_game+f'{sum_enemy_pos_life:7.2f}'+' sum_pos_enemy, '\n",
        "                #out_text_game=out_text_game+f'{(np.sum(np.sum(game_user_cards_life[:game_num_user_cards],axis=0)+np.abs(np.sum(game_user_cards_life[:game_num_user_cards],axis=0)))/2)/dim_sg2:7.2f}'+\" sum_pos_user, \"\n",
        "                #out_text_game=out_text_game+f'{(np.sum(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)+np.abs(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)))/2)/dim_sg2:7.2f}'+' sum_pos_enemy, '\n",
        "                #out_text_game=out_text_game+f'{(np.sum(game_user_cards_life[:game_num_user_cards]+np.abs(game_user_cards_life[:game_num_user_cards]),axis=0)/2)/dim_sg2:7.2f}'+\" sum_pos_user, \"\n",
        "                #out_text_game=out_text_game+f'{(np.sum(game_enemy_cards_life[:game_num_enemy_cards]+np.abs(game_enemy_cards_life[:game_num_enemy_cards]),axis=0)/2)/dim_sg2:7.2f}'+' sum_pos_enemy, '\n",
        "                #out_text_game=out_text_game+f'{(np.sum(game_user_cards_life[:game_num_user_cards]+np.abs(game_user_cards_life[:game_num_user_cards]))/2)/dim_sg2:7.2f}'+\" sum_pos_user, \"\n",
        "                #out_text_game=out_text_game+f'{(np.sum(game_enemy_cards_life[:game_num_enemy_cards]+np.abs(game_enemy_cards_life[:game_num_enemy_cards]))/2)/dim_sg2:7.2f}'+' sum_pos_enemy, '\n",
        "                out_text_game=out_text_game+f'{(game_num_user_cards):1.0f}'+\" user, \"\n",
        "                out_text_game=out_text_game+f'{(game_num_enemy_cards):1.0f}'+' enemy'\n",
        "                #if debug:\n",
        "                if game_text:\n",
        "                  print(out_text_game)\n",
        "                #print('game_out')\n",
        "                #if game_num_user_cards>0:\n",
        "                #  print(np.array2string(((((np.sum(game_user_cards_life[:game_num_user_cards],axis=0)+np.abs(np.sum(game_user_cards_life[:game_num_user_cards],axis=0)))/2)/game_num_user_cards)*10).astype(int),separator='',max_line_width=130))\n",
        "                #if game_num_enemy_cards>0:\n",
        "                #  print(np.array2string(((((np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)+np.abs(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)))/2)/game_num_enemy_cards)*10).astype(int),separator='',max_line_width=130))\n",
        "                #print(np.array2string((((game_user_cards_life[:game_num_user_cards]+np.abs(game_user_cards_life[:game_num_user_cards]))/2)*10).astype(int),separator='',max_line_width=130))\n",
        "                #print(np.array2string((((game_enemy_cards_life[:game_num_enemy_cards]+np.abs(game_enemy_cards_life[:game_num_enemy_cards]))/2)*10).astype(int),separator='',max_line_width=130))\n",
        "                #print(np.array2string(((psds_sg2+0.5)*10).astype(int),separator='',max_line_width=130))\n",
        "\n",
        "            if generate&gen_wavegan:\n",
        "              psd_array_wg[i]=psds_wg\n",
        "            if generate&gen_stylegan2:\n",
        "              if generate&gen_game:\n",
        "                if generate&gen_game_mode1:\n",
        "                  psd_array_sg2[i]=psds_sg2\n",
        "                if generate&gen_game_mode3:\n",
        "                  psd_array_sg2[part_len*0 + i]=psds_sg2\n",
        "                  psd_array_sg2[part_len*1 + i]=psds_sg2_1\n",
        "                  psd_array_sg2[part_len*2 + i]=psds_sg2_2\n",
        "                  psd_array_sg2[part_len*3 + i]=psds_sg2_3\n",
        "              else:\n",
        "                psd_array_sg2[i]=psds_sg2\n",
        "            if (i==part_len-1) or (ji==n_generate-1) :\n",
        "             encodeds=[]\n",
        "             #print('encodeds=[]')\n",
        "             if generate&gen_wavegan:\n",
        "               if send_wg:\n",
        "                global G_z, z, _G_z, _G_z_full, _G_z_full2#, _z\n",
        "                _z = psd_array_wg * vol\n",
        "                #print('>>sess.run')\n",
        "                _G_z = sess.run(G_z, {z: _z})[:,:,0]\n",
        "                #print('<<sess.run')\n",
        "                if j==0:\n",
        "                  _G_z_full=_G_z\n",
        "                else:\n",
        "                  _G_z_full=np.append(_G_z_full,_G_z)\n",
        "                #if _G_z_full2 is None:\n",
        "                #  _G_z_full2=_G_z_full\n",
        "                #else:\n",
        "                #  _G_z_full2=np.append(_G_z_full2,_G_z_full)\n",
        "                                    \n",
        "                if (ji==n_generate-1):\n",
        "                  if (part==0) and not (part==parts-1):\n",
        "                    #print(_G_z_full)\n",
        "                    _G_z_full_l=_G_z_full.flatten()\n",
        "                    #print(_G_z_full_l)\n",
        "                  if (part==parts-1):\n",
        "                    if (part==0):\n",
        "                      _G_z_full=_G_z_full.flatten()\n",
        "                    if (part==1):\n",
        "                      #_G_z_full_lr=_G_z_full.flatten()\n",
        "                      #_G_z_full_lr=_G_z_full\n",
        "                      _G_z_full_r=_G_z_full.flatten()\n",
        "                      #print(_G_z_full_r)\n",
        "                      #_G_z_full=_G_z_full_l\n",
        "                      if generate&gen_wg_st_swap:\n",
        "                        _G_z_full=np.append([_G_z_full_l],[_G_z_full_r], axis=0)\n",
        "                      else:\n",
        "                        _G_z_full=np.append([_G_z_full_r],[_G_z_full_l], axis=0)\n",
        "                      #print(_G_z_full)\n",
        "                      if _G_z_full.ndim > 1 and _G_z_full.shape[0] == 2:\n",
        "                        _G_z_full = _G_z_full.T\n",
        "\n",
        "                    buffer = BytesIO()\n",
        "                    if generate&gen_mp3:\n",
        "                      buffer_wav = BytesIO()\n",
        "                      scipy.io.wavfile.write(buffer_wav, hz, _G_z_full) # change rate for different tempo\n",
        "                      AudioSegment.from_wav(buffer_wav).export(buffer, format=\"mp3\")\n",
        "                    if generate&gen_wav:\n",
        "                      scipy.io.wavfile.write(buffer, hz, _G_z_full) # change rate for different tempo\n",
        "                    if False:  \n",
        "                     if generate&gen_webm:\n",
        "                      buffer_wav = BytesIO()\n",
        "                      scipy.io.wavfile.write(buffer_wav, hz, _G_z_full) # change rate for different tempo\n",
        "                      #binary = b64decode(data.split(',')[1])\n",
        "                      binary = buffer_wav\n",
        "                      binary.name='output.mp4'  \n",
        "                      process = (ffmpeg\n",
        "                       .input('pipe:0', format='wav')\n",
        "                       .output('pipe:1', format='webm')\n",
        "                       .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "                      )\n",
        "                      output, err = process.communicate(input=binary)\n",
        "                      with open(buffer, 'wb') as f:\n",
        "                        f.write(output)\n",
        "                    \n",
        "                    #wave.open(buffer, mode='wb')\n",
        "                    #AudioSegment.from_wav(buffer).export('/content/gdrive/MyDrive/EEG-GAN-audio-video/out/'+\n",
        "                    #               f'{(time100*1000):9.0f}'+'/'+f'{(time000*1000):9.0f}'+'.mp3', format=\"mp3\")\n",
        " \n",
        "                    buffer.seek(0)\n",
        "                    mysound = buffer.getvalue()\n",
        "                    if generate&gen_mp3:\n",
        "                      encoded= \"data:audio/mp3;base64,\"+base64.b64encode(mysound).decode()\n",
        "                    if generate&gen_wav:\n",
        "                      encoded= \"data:audio/wav;base64,\"+base64.b64encode(mysound).decode()\n",
        "                    if False:\n",
        "                     if generate&gen_webm:\n",
        "                      encoded= \"data:video/webm;base64,\"+base64.b64encode(mysound).decode()\n",
        "                    #print('audio encoded')\n",
        " \n",
        "                    time110=perf_counter()\n",
        "                    #print (f'110: {(time110-time000):.1f}s')\n",
        "                    #js_image=js\n",
        "                    #encodeds.append(encoded)\n",
        "                    comm.send({\n",
        "                      'response': encoded,\n",
        "                      }, None, msg['buffers']);\n",
        "                    #break\n",
        "    \n",
        "                  #encoded = base64.b64encode(image_asarray).decode('ascii')\n",
        "                  #js='''this.photo.src = \"data:image/png;base64,{0}\"'''.format(encoded)\n",
        " \n",
        "                  \n",
        "                  #js='''senderChannel.postMessage(\"{0}\")'''.format(encoded)\n",
        "                    #js='''playAudio1(\"{0}\",{1},{2})'''.format(encoded,xsize,ysize)\n",
        "                    #js_image=js\n",
        "             if generate&gen_stylegan2:\n",
        "              if send_sg2:#_ada or generate_stylegan2_ext:\n",
        "               images=[]\n",
        "               if generate&gen_sg2_rosasalberto_tf2:\n",
        "                global generator, dlatents, __z\n",
        "                #global images\n",
        " \n",
        "                #seed = 6600\n",
        "                # creating random latent vector\n",
        "                #rnd = np.random.RandomState(seed)\n",
        "                #__z = rnd.randn(1, 512).astype('float32')\n",
        "                # running mapping network\n",
        "                time1101=perf_counter()\n",
        "                #print (f'1101: {(time1101-time000):.1f}s')\n",
        "                #dlatents = generator.mapping_network(__z)  \n",
        "                time1102=perf_counter()\n",
        "                #print (f'1102: {(time1102-time000):.1f}s')\n",
        " \n",
        "                __z = psd_array_sg2 * vol\n",
        "                dlatents = generator.mapping_network(__z)\n",
        "                time1103=perf_counter()\n",
        "                #print (f'1103: {(time1103-time000):.1f}s')\n",
        "                image_out = generator.synthesis_network(dlatents)\n",
        "                time1104=perf_counter()\n",
        "                #print (f'1104: {(time1104-time000):.1f}s')\n",
        "                #converting image/s to uint8\n",
        "                img = convert_images_to_uint8(image_out, nchw_to_nhwc=True, uint8_cast=True)\n",
        "                #plotting images\n",
        "                #ax[i].axis('off')\n",
        "                #img_plot = ax[i].imshow(img.numpy()[0])   \n",
        "                time1105=perf_counter()\n",
        "                #print (f'1105: {(time1105-time000):.1f}s')\n",
        "                #images=[img.numpy()[0]]\n",
        "                images=img.numpy()\n",
        "               elif generate&gen_sg2_moono_tf2:\n",
        "                 global g_clone, latents, labels\n",
        "                 #global images\n",
        "                 latents = psd_array_sg2 * vol\n",
        "                 image_out = g_clone([latents, labels], training=False, truncation_psi=0.5)\n",
        "                 image_out = postprocess_images(image_out)\n",
        "                 image = image_out.numpy()\n",
        "               elif generate&gen_sg2_nagolinc_pt:\n",
        "                global device, __z1, g, latent_avg\n",
        "                #global images\n",
        "                __z1 = psd_array_sg2 * vol\n",
        "                with torch.no_grad():\n",
        "                  img_pt, _ = g(\n",
        "                    [torch.from_numpy(np.float32(__z1)).to(device)],\n",
        "                    truncation=0.5,\n",
        "                    truncation_latent=latent_avg.to(device),\n",
        "                    randomize_noise=False,\n",
        "                  )\n",
        "                images=img_pt.cpu().numpy()\n",
        "               elif generate&gen_tf1:\n",
        "                global _G, _D, Gs, Gs_kwargs, _z1\n",
        "                #global images\n",
        "                _z1 = psd_array_sg2 * vol\n",
        "                #print(_z1)\n",
        "                time1101=perf_counter()\n",
        "                #print (f'1101: {(time1101-time000):.1f}s')\n",
        "                #print('>>Gs.run')\n",
        "                images = Gs.run(_z1, None, **Gs_kwargs) # [minibatch, height, width, channel]\n",
        "                #print('<<Gs.run')\n",
        "                time1102=perf_counter()\n",
        "                time1105=perf_counter()\n",
        "                #print (f'1102: {(time1102-time000):.1f}s')\n",
        "                #if generate&gen_sg2_shawwn:\n",
        "                #  n_sample=1\n",
        "                #  inputSize=1024\n",
        "                #  z = np.random.randn(n_sample, inputSize).astype(\"float32\")\n",
        "                #  images = Gs.run(z, None, **Gs_kwargs) # [minibatch, height, width, channel]\n",
        "               #elif generate&gen_sg2_nvlabs_ada_pt:\n",
        "               if generate&gen_sg2_nvlabs_ada_pt:\n",
        "                global G, ws#, device\n",
        "                #global images\n",
        "                #device = torch.device('cuda:0')\n",
        "#                device = torch.device('cuda')\n",
        " \n",
        "                if game_num_enemy_cards>0:\n",
        "                  psd_array_sg2[0] = game_enemy_cards[0]-0.5\n",
        "                  #psd_array_sg2[0] = game_user_cards[0]-0.5\n",
        "                z_samples = psd_array_sg2 * vol\n",
        "                w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]\n",
        "                w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)       # [N, 1, C]\n",
        "                w_avg = np.mean(w_samples, axis=0, keepdims=True)      # [1, 1, C]\n",
        "                w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True) # pylint: disable=not-callable\n",
        "                ws = (w_opt).repeat([1, G.mapping.num_ws, 1])\n",
        " \n",
        "                synth_images = G.synthesis(ws, noise_mode='const')\n",
        "                synth_images = (synth_images + 1) * (255/2)\n",
        "                synth_images = synth_images.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "                #out.append_data(synth_images)\n",
        "                if generate&gen_game:\n",
        "                  #print(images.shape)\n",
        "                  #print(synth_images.shape)\n",
        "                  image_pil=PIL.Image.fromarray(synth_images, 'RGB')\n",
        "                  img=image_pil.resize((512,512),PIL.Image.ANTIALIAS)\n",
        "                  #img=image_pil.resize((xsize,ysize),PIL.Image.ANTIALIAS)\n",
        "                  image_asarray=np.asarray(img)\n",
        "                  #print(image_asarray.shape)\n",
        "                  images=np.append(images,[image_asarray],axis=0)\n",
        "                  #images.append([image_asarray])\n",
        "                  #print(images.shape)\n",
        "                else:\n",
        "                  images=[synth_images]\n",
        "               global mp4_codec\n",
        "               if True:\n",
        "                if generate&gen_mp4_h264_nvenc:\n",
        "                 if generate&gen_mp4:\n",
        "                    mp4_fps = fps2_sg2\n",
        "                    mp4_codec = 'h264_nvenc'\n",
        "                    #mp4_bitrate = 1000000\n",
        "                    #mp4_file = buffer\n",
        "                    #!rm -f buffer_*\n",
        "                    img_index=str(int(time1102*10)).zfill(9)\n",
        "                    img_counter=0\n",
        "                    img_files = 'buffer_%09d.jpg'\n",
        "                    mp4_file = 'buffer_'+img_index+'.mp4'\n",
        "                    #stream.bit_rate = mp4_bitrate\n",
        "                    #stream.pix_fmt = 'yuv420p'\n",
        "                if generate&gen_mp4_pyav:\n",
        "                 if generate&gen_mp4:\n",
        "                    mp4_fps = fps2_sg2\n",
        "                    #mp4_fps = fps_sg2*fps2_sg2\n",
        "                    #mp4_fps = fps_sg2\n",
        "                    #mp4_codec = 'mpeg4'\n",
        "                    #mp4_codec = 'h264'\n",
        "                    #mp4_codec = 'h264_cuvid'\n",
        "                    mp4_codec = 'libx264'\n",
        "                    #mp4_bitrate = 1000000\n",
        "                    #mp4_file = buffer\n",
        "                    mp4_file = 'buffer.mp4'\n",
        "                    output = av.open(mp4_file, 'w')\n",
        "                    stream = output.add_stream(mp4_codec, mp4_fps)\n",
        "                    #stream.bit_rate = mp4_bitrate\n",
        "                    stream.pix_fmt = 'yuv420p'\n",
        "                if generate&gen_mp4_imageio:\n",
        "                  if generate&gen_mp4:\n",
        "                    duration_sec = 1/fps_sg2\n",
        "                    smoothing_sec = 2.0 # 1.0\n",
        "                    mp4_fps = fps2_sg2\n",
        "                    #mp4_fps = fps_sg2*fps2_sg2\n",
        "                    #mp4_fps = fps_sg2\n",
        "                    mp4_codec = 'libx264'\n",
        "                    mp4_bitrate = '4M'\n",
        "                    #mp4_file = buffer\n",
        "                    mp4_file = 'buffer.mp4'\n",
        "                    video_out = imageio.get_writer(mp4_file, format='FFMPEG', mode='I', fps=mp4_fps, codec=mp4_codec, bitrate=mp4_bitrate)\n",
        "                buffers=[]\n",
        "                for image in images:\n",
        "                  time1110=perf_counter()\n",
        "                  #print (f'1110: {(time1110-time000):.1f}s')\n",
        "                  #print(image)\n",
        "                  if generate&gen_sg2_nagolinc_pt:\n",
        "                    image = ((image+1)/2*256).clip(0,255).astype(np.uint8).transpose(1,2,0)\n",
        "                  elif generate&gen_sg2_shawwn:\n",
        "                    image = ((image+1)/2*256).clip(0,255).astype(np.uint8).transpose(1,2,0)\n",
        "                  elif generate&gen_sg2_shawwn_tpu:\n",
        "                    image = ((image+1)/2*256).clip(0,255).astype(np.uint8).transpose(1,2,0)\n",
        "                  #elif generate&gen_sg2_aydao_surgery_model_release:\n",
        "                  #  image = ((image+1)/2*256).clip(0,255).astype(np.uint8).transpose(1,2,0)\n",
        "                    \n",
        "                  #if generate_stylega2_ada_pytorch:\n",
        " \n",
        "                  image_pil=PIL.Image.fromarray(image, 'RGB')\n",
        "                  #if generate&gen_sg2_shawwn:\n",
        "                  #  display(image_pil)\n",
        "                  #print(image_pil)\n",
        "                  #image_asarray=np.asarray(image_pil)\n",
        "                  #print(image_asarray)\n",
        "                  time1111=perf_counter()\n",
        "                  #print (f'1111: {(time1111-time000):.1f}s')\n",
        "                  #global video_out\n",
        "                  #video_out.append_data(image_asarray)\n",
        "                  time1112=perf_counter()\n",
        "                  #print (f'1112: {(time1112-time000):.1f}s')\n",
        "                  img=image_pil.resize((xsize,ysize),PIL.Image.ANTIALIAS)\n",
        "                  #print(img)\n",
        "                  time1113=perf_counter()\n",
        "                  #print (f'1113: {(time1113-time000):.1f}s')\n",
        "                  buffer = BytesIO()\n",
        "                  buffers.append(buffer)\n",
        "                  if generate&gen_jpeg:\n",
        "                    img.save(buffer,format=\"JPEG\")                  #Enregistre l'image dans le buffer\n",
        "                  if generate&gen_png:\n",
        "                    img.save(buffer,format=\"PNG\")                  #Enregistre l'image dans le buffer\n",
        "                  if generate&gen_mp4_h264_nvenc:\n",
        "                   if generate&gen_mp4:\n",
        "                    img.save('buffer_'+img_index+\n",
        "                             '_'+str(int(img_counter)).zfill(4)+'.jpg',format=\"JPEG\")                  #Enregistre l'image dans le buffer\n",
        "                    img_counter=img_counter+1\n",
        "                  if generate&gen_mp4_pyav:\n",
        "                   if generate&gen_mp4:\n",
        "                    stream.height = ysize\n",
        "                    stream.width = xsize\n",
        "                    img_asarray=np.asarray(img)\n",
        "                    frame = av.VideoFrame.from_ndarray(img_asarray, format='rgb24')\n",
        "                    packet = stream.encode(frame)\n",
        "                    output.mux(packet)\n",
        "                  if generate&gen_mp4_imageio:\n",
        "                    if generate&gen_mp4:\n",
        "                      img_asarray=np.asarray(img)\n",
        "                      video_out.append_data(img_asarray)\n",
        "                    if generate&gen_webm:\n",
        "                      img_asarray=np.asarray(img)\n",
        "                      video_out.append_data(img_asarray)\n",
        "\n",
        "                if generate&gen_mp4_h264_nvenc:\n",
        "                 if generate&gen_mp4:\n",
        "                  !ffmpeg -v quiet -vsync 0 -hwaccel cuvid -c:v mjpeg_cuvid -framerate {mp4_fps} -i buffer_{img_index}_%04d.jpg -c:v {mp4_codec} {mp4_file} -y\n",
        "                  with open(mp4_file, 'rb') as fh:\n",
        "                    buffer = BytesIO(fh.read())\n",
        "                if generate&gen_mp4_pyav:\n",
        "                 if generate&gen_mp4:\n",
        "                  for packet in stream.encode():\n",
        "                    output.mux(packet)\n",
        "                  output.close()\n",
        "                  with open(mp4_file, 'rb') as fh:\n",
        "                    buffer = BytesIO(fh.read())\n",
        "                if generate&gen_mp4_imageio:\n",
        "                  if generate&gen_mp4:\n",
        "                    video_out.close()\n",
        "                    with open(mp4_file, 'rb') as fh:\n",
        "                      buffer = BytesIO(fh.read())\n",
        "                  if generate&gen_webm:\n",
        "                    video_out.close()\n",
        "                    with open(mp4_file, 'rb') as fh:\n",
        "                      buffer = BytesIO(fh.read())\n",
        "\n",
        "                if generate&gen_mp4_moviepy:\n",
        "                  #img.save('/content/gdrive/MyDrive/EEG-GAN-audio-video/out/'+\n",
        "                  #          f'{(time100*1000):9.0f}'+'/'+f'{(time000*1000):9.0f}'+'.png',format=\"PNG\")\n",
        "                  if generate&gen_webm:\n",
        "                    duration_sec = 1/fps_sg2\n",
        "                    smoothing_sec = 2.0 # 1.0\n",
        "                    mp4_fps = fps2_sg2\n",
        "                    #mp4_fps = fps_sg2*fps2_sg2\n",
        "                    mp4_codec = 'libvpx'\n",
        "                    mp4_bitrate = '1M'\n",
        "                    #mp4_codec = 'libx264'\n",
        "                    #mp4_bitrate = '4M'\n",
        "                    random_seed = np.random.randint(0, 999)#405\n",
        "                    #mp4_file = buffer\n",
        "                    mp4_file = 'buffer.webm'\n",
        "                    #mp4_file = 'random_grid_%s.webm' % random_seed\n",
        "                    #minibatch_size = 16\n",
        "                    num_frames = int(np.rint(duration_sec * mp4_fps))\n",
        "                     # Frame generation func for moviepy.\n",
        "                    def make_frame(t):\n",
        "                        frame_idx = int(np.clip(np.round(t * mp4_fps), 0, num_frames - 1))\n",
        "                        global images\n",
        "                        image=images[frame_idx]\n",
        "                        return image\n",
        "                    # Generate video.\n",
        "                    video_clip = moviepy.editor.VideoClip(make_frame, duration=duration_sec)\n",
        "                    video_clip.write_videofile(mp4_file, fps=mp4_fps, codec=mp4_codec, bitrate=mp4_bitrate, \n",
        "                                               verbose=False, progress_bar=False)\n",
        "                    with open(mp4_file, 'rb') as fh:\n",
        "                      buffer = BytesIO(fh.read())\n",
        "                  if generate&gen_mp4:\n",
        "                    duration_sec = 1/fps_sg2\n",
        "                    smoothing_sec = 2.0 # 1.0\n",
        "                    mp4_fps = fps2_sg2\n",
        "                    #mp4_fps = fps_sg2*fps2_sg2\n",
        "                    #mp4_fps = fps_sg2\n",
        "                    mp4_codec = 'libx264'\n",
        "                    mp4_bitrate = '4M'\n",
        "                    random_seed = np.random.randint(0, 999)#405\n",
        "                    #mp4_file = buffer\n",
        "                    mp4_file = 'buffer.mp4'\n",
        "                    #mp4_file = 'random_grid_%s.mp4' % random_seed\n",
        "                    #minibatch_size = 16\n",
        "                    num_frames = int(np.rint(duration_sec * mp4_fps))\n",
        "                     # Frame generation func for moviepy.\n",
        "                    def make_frame(t):\n",
        "                        frame_idx = int(np.clip(np.round(t * mp4_fps), 0, num_frames - 1))\n",
        "                        global images\n",
        "                        #print(frame_idx)\n",
        "                        #print(images)\n",
        "                        #print(images[frame_idx])\n",
        "                        image=images[frame_idx]\n",
        "                        return image\n",
        "                    # Generate video.\n",
        "                    #%del mp4_file\n",
        "                    video_clip = moviepy.editor.VideoClip(make_frame, duration=duration_sec)\n",
        "                    video_clip.write_videofile(mp4_file, fps=mp4_fps, codec=mp4_codec, \n",
        "                                               bitrate=mp4_bitrate, verbose=False, progress_bar=False)#, logger=None)\n",
        "                    with open(mp4_file, 'rb') as fh:\n",
        "                      buffer = BytesIO(fh.read())\n",
        "\n",
        "                if True:\n",
        "                  buffer.seek(0)\n",
        "                  time1114=perf_counter()\n",
        "                  #print (f'1114: {(time1114-time000):.1f}s')\n",
        "                  myimage = buffer.getvalue()   \n",
        "                  #encoded=myimage\n",
        "                  if generate&gen_jpeg:\n",
        "                    #encoded= \"data:image/jpeg;base64,\"+base64.b64encode(myimage).decode()\n",
        "                    encoded= \"binary:data:image/jpeg\"\n",
        "                  if generate&gen_png:\n",
        "                    #encoded= \"data:image/png;base64,\"+base64.b64encode(myimage).decode()\n",
        "                    encoded= \"binary:data:image/png\"\n",
        "                  if generate&gen_webm:\n",
        "                    #encoded= \"data:video/webm;base64,\"+base64.b64encode(myimage).decode()\n",
        "                    encoded= \"binary:data:video/webm\"\n",
        "                  if generate&gen_mp4:\n",
        "                    #encoded= \"data:video/mp4;base64,\"+base64.b64encode(myimage).decode()\n",
        "                    encoded= \"binary:data:video/mp4\"\n",
        "                  #print(encoded)\n",
        "                 #js='''displayPhoto1(\"{0}\",{1},{2})'''.format(encoded,xsize,ysize)\n",
        "                if (ji==n_generate-1) :\n",
        "                    time110=perf_counter()\n",
        "                    #print (f'110: {(time110-time000):.1f}s')\n",
        "                    #js_image=js\n",
        "                    #print('>>encodeds.append(encoded)')\n",
        "                    #encodeds.append(encoded)\n",
        "                    #print('<<encodeds.append(encoded)')\n",
        "                    #print(myimage)\n",
        "                    if generate&gen_game:\n",
        "                      buffers[0].seek(0)\n",
        "                      myimage0 = buffers[0].getvalue()   \n",
        "                      if generate&gen_game_mode3:\n",
        "                        buffers[1].seek(0)\n",
        "                        myimage1 = buffers[1].getvalue()   \n",
        "                        buffers[2].seek(0)\n",
        "                        myimage2 = buffers[2].getvalue()   \n",
        "                        buffers[3].seek(0)\n",
        "                        myimage3 = buffers[3].getvalue()   \n",
        "                        encoded=encoded+';mode:3'\n",
        "                      \n",
        "                      if False:\n",
        "                       if (game_num_user_cards>0) and (game_num_enemy_cards>0):\n",
        "                        #user_cards_array=(np.sum(game_user_cards_life[:game_num_user_cards],axis=0)+np.abs(np.sum(game_user_cards_life[:game_num_user_cards],axis=0)))/2\n",
        "                        #user_life_array=(np.sum(game_user_cards[:game_num_user_cards],axis=0)+np.abs(np.sum(game_user_cards[:game_num_user_cards],axis=0)))/2\n",
        "                        #enemy_cards_array=(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)+np.abs(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)))/2\n",
        "                        #enemy_life_array=(np.sum(game_enemy_cards[:game_num_enemy_cards],axis=0)+np.abs(np.sum(game_enemy_cards[:game_num_enemy_cards],axis=0)))/2\n",
        "                        plt.figure()\n",
        "                        plt.fill_between(range(dim_sg2),user_cards_array,user_life_array)\n",
        "                        plt.fill_between(range(dim_sg2),user_cards_array,np.zeros(dim_sg2))\n",
        "                        plt.title(\"user_cards_life\")\n",
        "                        plt.ylim(0, game_num_user_cards)\n",
        "                        buf1 = io.BytesIO()\n",
        "                        #buf10 = io.BytesIO()\n",
        "                        #if generate&gen_jpeg:\n",
        "                        #  buf_jpg = io.BytesIO()\n",
        "                        #  plt.savefig(buf_jpg, format='png')\n",
        "                        #  buf_jpg.seek(0)\n",
        "                        #  Image.open(buf_jpg).save(buf,'JPEG')\n",
        "                        #if generate&gen_png:\n",
        "                        #  plt.savefig(buf, format='png')\n",
        "                        buf1.seek(0)\n",
        "                        plt.savefig(buf1, format='png')\n",
        "                        #PIL.Image.open(buf1).save(buf10,'PNG')\n",
        "                        #im = PIL.Image.open(buf)\n",
        "                        #im.show()\n",
        "                        #buf10.seek(0)\n",
        "                        image_user_cards_life = buf1.getvalue()\n",
        "                        plt.close()\n",
        "                        plt.figure()\n",
        "                        plt.fill_between(range(dim_sg2),enemy_cards_array,enemy_life_array)\n",
        "                        plt.fill_between(range(dim_sg2),enemy_cards_array,np.zeros(dim_sg2))\n",
        "                        plt.title(\"enemy_cards_life\")\n",
        "                        plt.ylim(0, game_num_enemy_cards)\n",
        "                        buf2 = io.BytesIO()\n",
        "                        #buf20 = io.BytesIO()\n",
        "                        #if generate&gen_jpeg:\n",
        "                        #  buf_jpg = io.BytesIO()\n",
        "                        #  plt.savefig(buf_jpg, format='png')\n",
        "                        #  buf_jpg.seek(0)\n",
        "                        #  PIL.Image.open(buf_jpg).save(buf,'JPEG')\n",
        "                        #if generate&gen_png:\n",
        "                        #  plt.savefig(buf, format='png')\n",
        "                        buf2.seek(0)\n",
        "                        plt.savefig(buf2, format='png')\n",
        "                        #PIL.Image.open(buf2).save(buf20,'PNG')\n",
        "                        #buf20.seek(0)\n",
        "                        #im = PIL.Image.open(buf)\n",
        "                        #im.show()\n",
        "                        #buf.seek(0)\n",
        "                        image_enemy_cards_life = buf2.getvalue()   \n",
        "                        plt.close()\n",
        "                        encoded=encoded+';user:cards_life'\n",
        "                        encoded=encoded+';enemy:cards_life'\n",
        "\n",
        "                        #if (game_num_user_cards>0) and (game_num_enemy_cards>0):\n",
        "                        if True:\n",
        "                        #if game_out&game_out_user_attack:\n",
        "                          image_user_attack_enemy_cards_life=[]\n",
        "                          plt.figure()\n",
        "                          fig, axs = plt.subplots(3, game_max_user_cards)\n",
        "                          for i in range(game_num_user_cards):\n",
        "                            #for j in range(game_num_enemy_cards):\n",
        "                             # print(i,j)\n",
        "                             # if j==0:\n",
        "                             #   plt.fill_between(range(dim_sg2),game_user_attack_enemy_cards[i][j],np.zeros(dim_sg2))\n",
        "                             # else:\n",
        "                             #   plt.fill_between(range(dim_sg2),np.sum(game_user_attack_enemy_cards[i][:j],axis=0),np.sum(game_user_attack_enemy_cards[i][:j-1],axis=0))\n",
        "                            #plt.fill_between(range(dim_sg2),np.sum(np.sum(game_user_attack_enemy_cards,axis=0),axis=0),np.zeros(dim_sg2))\n",
        "\n",
        "                            axs[0, i].fill_between(range(dim_sg2),game_user_stddev_compare_with_possible_cards[i],np.zeros(dim_sg2))\n",
        "                            axs[1, i].fill_between(range(dim_sg2),np.sum(game_user_attack_enemy_cards_possible[i],axis=0),np.zeros(dim_sg2))\n",
        "                            axs[2, i].fill_between(range(dim_sg2),np.sum(game_user_attack_enemy_cards[i],axis=0),np.zeros(dim_sg2))\n",
        "                            #game_user_attack_enemy_cards_possible[i2][j2][j1]=((game_user_cards[i2][j1]/(game_enemy_cards[j2][j1]+1))*(1))/game_num_enemy_cards\n",
        "                            #game_user_attack_enemy_cards[i2][j2][j1]=((game_user_cards[i2][j1]/(game_enemy_cards[j2][j1]+1))*(1-game_user_stddev_compare_with_possible_cards[i2]))/game_num_enemy_cards\n",
        "\n",
        "                            #axs[0, i].set_title(\"user_attack_enemy_cards_life_\"+str(i))\n",
        "                            #plt.ylim(0, 1)\n",
        "                            #plt.ylim(0, game_num_enemy_cards)\n",
        "                            axs[0, i].set_ylim(0, np.max(game_user_stddev_compare_with_possible_cards[:game_num_user_cards]))\n",
        "                            axs[1, i].set_ylim(0, 1)\n",
        "                            axs[2, i].set_ylim(0, 1)\n",
        "                            #axs[0, i].label_outer()\n",
        "                            #axs[1, i].label_outer()\n",
        "                            #axs[2, i].label_outer()\n",
        "                            axs[0, i].set(ylabel='stddev')\n",
        "                            axs[1, i].set(ylabel='possible')\n",
        "                            axs[2, i].set(ylabel='possible&stddev')\n",
        "                          for ax in axs.flat:\n",
        "                            ax.label_outer()\n",
        "                          buf2 = io.BytesIO()\n",
        "                          buf2.seek(0)\n",
        "                          plt.savefig(buf2, format='png')\n",
        "                          #image_user_attack_enemy_cards_life.append(buf2.getvalue())\n",
        "                          #if i==0:\n",
        "                          image_user_attack_enemy_cards_life0=buf2.getvalue()\n",
        "                          plt.close()\n",
        "                          encoded=encoded+';user_attack_enemy:cards_life'\n",
        "                          if generate&gen_game_mode1:\n",
        "                            msg['buffers']=[memoryview(myimage0),memoryview(myimage),memoryview(image_user_cards_life),memoryview(image_enemy_cards_life),memoryview(image_user_attack_enemy_cards_life0)]\n",
        "                          if generate&gen_game_mode3:\n",
        "                            msg['buffers']=[memoryview(myimage0),memoryview(myimage),memoryview(myimage1),memoryview(myimage2),memoryview(myimage3),memoryview(image_user_cards_life),memoryview(image_enemy_cards_life),memoryview(image_user_attack_enemy_cards_life0)]\n",
        "                          #msg['buffers']=[memoryview(myimage0),memoryview(myimage),memoryview(image_user_cards_life),memoryview(image_enemy_cards_life),memoryview(image_user_attack_enemy_cards_life)]\n",
        "                        else:\n",
        "                          if generate&gen_game_mode1:\n",
        "                            msg['buffers']=[memoryview(myimage0),memoryview(myimage),memoryview(image_user_cards_life),memoryview(image_enemy_cards_life)]\n",
        "                          if generate&gen_game_mode3:\n",
        "                            msg['buffers']=[memoryview(myimage0),memoryview(myimage),memoryview(myimage1),memoryview(myimage2),memoryview(myimage3),memoryview(image_user_cards_life),memoryview(image_enemy_cards_life)]\n",
        "\n",
        "                        buf1.close()\n",
        "                        #buf10.close()\n",
        "                        buf2.close()\n",
        "                        #buf20.close()\n",
        "                      else:\n",
        "                        if generate&gen_game_mode1:\n",
        "                          msg['buffers']=[memoryview(myimage0),memoryview(myimage)]\n",
        "                        if generate&gen_game_mode3:\n",
        "                          msg['buffers']=[memoryview(myimage0),memoryview(myimage),memoryview(myimage1),memoryview(myimage2),memoryview(myimage3)]\n",
        "\n",
        "                    else:\n",
        "                      msg['buffers']=[memoryview(myimage)]\n",
        "                    #print(msg['buffers'])\n",
        "                    #encoded=''\n",
        "                    if debug:\n",
        "                     #print('image sent',f'110: {(time110-time000):.1f}s, ')\n",
        "                     if False:\n",
        "                     #if True:\n",
        "                      print('image sent',f'110: {(time110-time000):.2f}s, '\n",
        "                                        ,f'1101-0111: {(time1101-time0111):.2f}s, '\n",
        "                                        ,f'1105-1101: {(time1105-time1101):.2f}s, '\n",
        "                                        ,f'110-1105: {(time110-time1105):.2f}s, ')\n",
        "                    if generate&gen_game:\n",
        "                      #encoded=encoded+';user:add'\n",
        "                      if game_out&game_out_user_add:\n",
        "                        encoded=encoded+';user:add'\n",
        "                      if game_out&game_out_user_attack:\n",
        "                        encoded=encoded+';user:attack'\n",
        "                      if game_out&game_out_enemy_add:\n",
        "                        encoded=encoded+';enemy:add'\n",
        "                      if game_out&game_out_enemy_attack:\n",
        "                        encoded=encoded+';enemy:attack'\n",
        "                      if game_out&game_out_user_killed:\n",
        "                        encoded=encoded+';user:killed'\n",
        "                      if game_out&game_out_enemy_killed:\n",
        "                        encoded=encoded+';enemy:killed'\n",
        "                      if game_out&game_out_user_restored:\n",
        "                        encoded=encoded+';user:restored'\n",
        "                      #user_cards=((np.sum(np.sum(game_user_cards_life[:game_num_user_cards],axis=0)+np.abs(np.sum(game_user_cards_life[:game_num_user_cards],axis=0)))/2)/dim_sg2)\n",
        "                      #user_life=((np.sum(np.sum(game_user_cards[:game_num_user_cards],axis=0)+np.abs(np.sum(game_user_cards[:game_num_user_cards],axis=0)))/2)/dim_sg2)\n",
        "                      #enemy_cards=((np.sum(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)+np.abs(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)))/2)/dim_sg2)\n",
        "                      #enemy_life=((np.sum(np.sum(game_enemy_cards[:game_num_enemy_cards],axis=0)+np.abs(np.sum(game_enemy_cards[:game_num_enemy_cards],axis=0)))/2)/dim_sg2)\n",
        "                      #encoded=encoded+';user_cards:'+f'{:.2f}'\n",
        "                      #encoded=encoded+';user_life:'+f'{:.2f}'\n",
        "                      #encoded=encoded+';enemy_cards:'+f'{:.2f}'\n",
        "                      #encoded=encoded+';enemy_life:'+f'{:.2f}'\n",
        "                    \n",
        "                    #  if game&game_user_add25:\n",
        "                    #    encoded=encoded+';user:25'\n",
        "                    #  if game&game_user_add50:\n",
        "                    #    encoded=encoded+';user:50'\n",
        "                    #  if game&game_user_add75:\n",
        "                    #    encoded=encoded+';user:75'\n",
        "                    #  if game&game_user_add100:\n",
        "                    #    encoded=encoded+';user:100'\n",
        "                    #  if game&game_user_addcoh:\n",
        "                    #    encoded=encoded+';user:coh'\n",
        "                    #  if game&game_enemy_addcoh:\n",
        "                    #    encoded=encoded+';enemy:coh'\n",
        "\n",
        "                    comm.send({\n",
        "                      'response': encoded,\n",
        "                      }, None, msg['buffers']);\n",
        "                    #break\n",
        "              #print('audio&image send')\n",
        "              #comm.send({\n",
        "              #  'response': 'close',\n",
        "                #'response': json.dumps(encodeds),\n",
        "              #}, None, msg['buffers']);\n",
        "              #break\n",
        " \n",
        "    #if not(js_image==\"\"):\n",
        "    #  js=js_image\n",
        "    #  js_image=\"\"\n",
        "    #  eval_js(js)\n",
        "#      if True:\n",
        "      if False:\n",
        "                    time111=perf_counter()\n",
        "                    print (f'000: {(time111-time000):.1f}s, '+\n",
        "                      f'001: {(time111-time001):.1f}s, '+\n",
        "                      f'110: {(time111-time110):.1f}s, '+\n",
        "                      f'001-000: {(time001-time000):.1f}s, {(time001-time000)*100/(time111-time000):.0f}%, '+\n",
        "                      f'110-001: {(time110-time001):.1f}s, {(time110-time001)*100/(time111-time000):.0f}%. '+\n",
        "                      f'111-110: {(time111-time110):.1f}s, {(time111-time110)*100/(time111-time000):.0f}%')\n",
        " \n",
        "  #if False:\n",
        "  if True:\n",
        "    comm.send({\n",
        "      'response': 'close',\n",
        "      #'response': json.dumps(encodeds),\n",
        "      }, None, msg['buffers']);\n",
        "  #print('again')\n",
        "  #comm.send({\n",
        "  #    'response': 'close',\n",
        "  #    #'response': json.dumps(encodeds),\n",
        "  #    }, None, [memoryview([])]);\n",
        " \n",
        "data_read=False\n",
        "get_ipython().kernel.comm_manager.register_target('comm_target1', target_func1)\n",
        "#target_func1('{1}','{1}')\n",
        "#for i in range(100):\n",
        "#  get_ipython().kernel.comm_manager.register_target(str(i), target_func1)\n",
        " \n",
        "Javascript('''\n",
        "//Joshua Brewster, GPL (copyleft)\n",
        " \n",
        "//import 'regenerator-runtime/runtime' //For async functions on node\\\\\n",
        " \n",
        " class eeg32 { //Contains structs and necessary functions/API calls to analyze serial data for the FreeEEG32\n",
        " \n",
        "    constructor(\n",
        "        onDecodedCallback = this.onDecodedCallback,\n",
        "        onConnectedCallback = this.onConnectedCallback,\n",
        "        onDisconnectedCallback = this.onDisconnectedCallback,\n",
        "        CustomDecoder = this.decode,\n",
        "        //baudrate = 1500000//115200\n",
        "        baudrate = 921600//115200\n",
        "        ) {\n",
        " \n",
        "        this.onDecodedCallback = onDecodedCallback;\n",
        "        this.onConnectedCallback = onConnectedCallback;\n",
        "        this.onDisconnectedCallback = onDisconnectedCallback;\n",
        "        this.decode = CustomDecoder;\n",
        "        //Free EEG 32 data structure:\n",
        "        \n",
        "        //    [stop byte, start byte, counter byte, 32x3 channel data bytes (24 bit), 3x2 accelerometer data bytes, stop byte, start byte...] Gyroscope not enabled yet but would be printed after the accelerometer..\n",
        "        //    Total = 105 bytes/line\n",
        "        \n",
        "        this.connected = false;\n",
        "        this.subscribed = false;\n",
        "        this.buffer = [];\n",
        "        this.startByte = 160; // Start byte value\n",
        "        this.stopByte = 192; // Stop byte value\n",
        "        this.searchString = new Uint8Array([this.stopByte,this.startByte]); //Byte search string\n",
        "        this.readRate = 16.666667; //Throttle EEG read speed. (1.953ms/sample min @103 bytes/line)\n",
        "        this.nChannels=%(data_channels)d;\n",
        "        if(this.nChannels==128)\n",
        "        {\n",
        "          this.readBufferSize = 8000; //Serial read buffer size, increase for slower read speeds (~1030bytes every 20ms) to keep up with the stream (or it will crash)\n",
        "        }\n",
        "        else\n",
        "        {\n",
        "        this.readBufferSize = 4000; //Serial read buffer size, increase for slower read speeds (~1030bytes every 20ms) to keep up with the stream (or it will crash)\n",
        "          //this.readBufferSize = 1000; //Serial read buffer size, increase for slower read speeds (~1030bytes every 20ms) to keep up with the stream (or it will crash)\n",
        "          //this.readBufferSize = 2000; //Serial read buffer size, increase for slower read speeds (~1030bytes every 20ms) to keep up with the stream (or it will crash)\n",
        "        }\n",
        "\n",
        "        //this.sps = 512; // Sample rate\n",
        "        //this.sps = 250; // Sample rate\n",
        "        this.sps=%(sfreq)d;\n",
        "        //this.sps=%(sfreq)f;\n",
        "        //this.nChannels = 128;\n",
        "        //this.nChannels = 32;\n",
        "        this.generate_game=%(generate_game)d;\n",
        "        this.generate_game_mode1=%(generate_game_mode1)d;\n",
        "        this.generate_game_mode3=%(generate_game_mode3)d;\n",
        "        \n",
        "        this.nPeripheralChannels = 6; // accelerometer and gyroscope (2 bytes * 3 coordinates each)\n",
        "        this.updateMs = 1000/this.sps; //even spacing\n",
        "        this.stepSize = 1/Math.pow(2,24);\n",
        "        //this.vref = 2.50; //2.5V voltage ref +/- 250nV\n",
        "        //this.gain = 8;\n",
        "        //this.vref = 1.25; //2.5V voltage ref +/- 250nV\n",
        "        //this.gain = 32;\n",
        "        this.vref =%(vref)f; //2.5V voltage ref +/- 250nV\n",
        "        this.gain = %(gain)d;\n",
        " \n",
        "        this.vscale = (this.vref/this.gain)*this.stepSize; //volts per step.\n",
        "        this.uVperStep = 1000000 * ((this.vref/this.gain)*this.stepSize); //uV per step.\n",
        "        this.scalar = 1/(1000000 / ((this.vref/this.gain)*this.stepSize)); //steps per uV.\n",
        " \n",
        "        this.maxBufferedSamples = this.sps*60*2; //max samples in buffer this.sps*60*nMinutes = max minutes of data\n",
        "        \n",
        "        this.data = { //Data object to keep our head from exploding. Get current data with e.g. this.data.A0[this.data.count-1]\n",
        "            count: 0,\n",
        "            startms: 0,\n",
        "            ms: [],\n",
        "            'A0': [],'A1': [],'A2': [],'A3': [],'A4': [],'A5': [],'A6': [],'A7': [], //ADC 0\n",
        "            'A8': [],'A9': [],'A10': [],'A11': [],'A12': [],'A13': [],'A14': [],'A15': [], //ADC 1\n",
        "            'A16': [],'A17': [],'A18': [],'A19': [],'A20': [],'A21': [],'A22': [],'A23': [], //ADC 2\n",
        "            'A24': [],'A25': [],'A26': [],'A27': [],'A28': [],'A29': [],'A30': [],'A31': [], //ADC 3\n",
        "            //'A0': [],'A1': [],'A2': [],'A3': [],'A4': [],'A5': [],'A6': [],'A7': [], //ADC 0\n",
        "            //'A8': [],'A9': [],'A10': [],'A11': [],'A12': [],'A13': [],'A14': [],'A15': [], //ADC 1\n",
        "            //'A16': [],'A17': [],'A18': [],'A19': [],'A20': [],'A21': [],'A22': [],'A23': [], //ADC 2\n",
        "            //'A24': [],'A25': [],'A26': [],'A27': [],'A28': [],'A29': [],'A30': [],'A31': [], //ADC 3\n",
        "            //'A0': [],'A1': [],'A2': [],'A3': [],'A4': [],'A5': [],'A6': [],'A7': [], //ADC 0\n",
        "            //'A8': [],'A9': [],'A10': [],'A11': [],'A12': [],'A13': [],'A14': [],'A15': [], //ADC 1\n",
        "            //'A16': [],'A17': [],'A18': [],'A19': [],'A20': [],'A21': [],'A22': [],'A23': [], //ADC 2\n",
        "            //'A24': [],'A25': [],'A26': [],'A27': [],'A28': [],'A29': [],'A30': [],'A31': [], //ADC 3\n",
        "            //'A0': [],'A1': [],'A2': [],'A3': [],'A4': [],'A5': [],'A6': [],'A7': [], //ADC 0\n",
        "            //'A8': [],'A9': [],'A10': [],'A11': [],'A12': [],'A13': [],'A14': [],'A15': [], //ADC 1\n",
        "            //'A16': [],'A17': [],'A18': [],'A19': [],'A20': [],'A21': [],'A22': [],'A23': [], //ADC 2\n",
        "            //'A24': [],'A25': [],'A26': [],'A27': [],'A28': [],'A29': [],'A30': [],'A31': [], //ADC 3\n",
        "            'Ax': [], 'Ay': [], 'Az': [], 'Gx': [], 'Gy': [], 'Gz': []  //Peripheral data (accelerometer, gyroscope)\n",
        "        };\n",
        " \n",
        "    this.bufferednewLines = 0;\n",
        "    this.data_slice=[];\n",
        "    this.data_slice_size=this.sps*(5*1/8+0.1);\n",
        "    this.ready_to_send_data = false;\n",
        "    this.data_send_count=0;\n",
        "\n",
        "    this.generate_parallel=%(generate_parallel)d;\n",
        "    \n",
        "    this.xsize=%(xsize)d;\n",
        "    this.ysize=%(ysize)d;\n",
        "\n",
        "    this.generate_stylegan2=true;\n",
        "    this.generate_stylegan2=%(generate_stylegan2)d;\n",
        "    \n",
        "    //this.generate_stylegan2=false;\n",
        "    this.generate_wavegan=true;\n",
        "    this.generate_wavegan=%(generate_wavegan)d;\n",
        "    this.generate_heatmap=true;\n",
        "    this.generate_heatmap=%(generate_heatmap)d;\n",
        "    //data:audio/wav;base64,\n",
        "    //data:image/jpeg;base64,\n",
        "    this.time100=Date.now();\n",
        "    this.time000=Date.now();\n",
        "    this.this_frame_wg=-1;\n",
        "    this.last_frame_wg=-1;\n",
        "    this.send_wg=false;\n",
        "    this.this_frame_sg2=-1;\n",
        "    this.last_frame_sg2=-1;\n",
        "    this.send_sg2=false;\n",
        "\n",
        "    this.frame_last=0;\n",
        "\n",
        "    if(this.generate_stylegan2)\n",
        "    {\n",
        "      this.fps_sg2=1;\n",
        "    }\n",
        "    if(this.generate_wavegan)\n",
        "    {\n",
        "      this.hz=44100;\n",
        "      this.fps_wg=this.hz/(32768*2);\n",
        "      //this.fps=this.hz/(32768);\n",
        "    }\n",
        "      this.fps_sg2=this.fps_wg;\n",
        "      this.fps_wg=%(fps_wg)f;\n",
        "      this.fps_sg2=%(fps_sg2)f;\n",
        "      this.fps_hm=%(fps_hm)f;\n",
        "      this.fps=Math.max(this.fps_wg,this.fps_sg2,this.fps_hm)*4;\n",
        "      //this.fps=Math.max(this.fps_wg,this.fps_sg2,this.fps_hm)*5;\n",
        "      //this.fps=10;\n",
        "      \n",
        "    this.samples_count=0;\n",
        "    //this.channel=None;\n",
        " \n",
        "        this.resetDataBuffers();\n",
        " \n",
        "        //navigator.serial utils\n",
        "        if(!navigator.serial){\n",
        "            console.error(\"`navigator.serial not found! Enable #enable-experimental-web-platform-features in chrome://flags (search 'experimental')\")\n",
        "        }\n",
        "        this.port = null;\n",
        "        this.reader = null;\n",
        "        this.baudrate = baudrate;\n",
        " \n",
        "    }\n",
        "    \n",
        "    resetDataBuffers(){\n",
        "        this.data.count = 0;\n",
        "        this.data.startms = 0;\n",
        "        for(const prop in this.data) {\n",
        "            if(typeof this.data[prop] === \"object\"){\n",
        "                this.data[prop] = new Array(this.maxBufferedSamples).fill(0);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        " \n",
        "    setScalar(gain=24,stepSize=1/(Math.pow(2,23)-1),vref=4.50) {\n",
        "        this.stepSize = stepSize;\n",
        "        this.vref = vref; //2.5V voltage ref +/- 250nV\n",
        "        this.gain = gain;\n",
        " \n",
        "        this.vscale = (this.vref/this.gain)*this.stepSize; //volts per step.\n",
        "        this.uVperStep = 1000000 * ((this.vref/this.gain)*this.stepSize); //uV per step.\n",
        "        this.scalar = 1/(1000000 / ((this.vref/this.gain)*this.stepSize)); //steps per uV.\n",
        "    }\n",
        " \n",
        "    getLatestData(channel=\"A0\",count=1) { //Return slice of specified size of the latest data from the specified channel\n",
        "        let ct = count;\n",
        "        if(ct <= 1) {\n",
        "            return [this.data[channel][this.data.count-1]];\n",
        "        }\n",
        "        else {\n",
        "            if(ct > this.data.count) {\n",
        "                ct = this.data.count;\n",
        "            }\n",
        "            return this.data[channel].slice(this.data.count-ct,this.data.count);\n",
        "        }\n",
        "    }\n",
        " \n",
        "    bytesToInt16(x0,x1){\n",
        "        return x0 * 256 + x1;\n",
        "    }\n",
        " \n",
        "    int16ToBytes(y){ //Turns a 24 bit int into a 3 byte sequence\n",
        "        return [y & 0xFF , (y >> 8) & 0xFF];\n",
        "    }\n",
        " \n",
        "    bytesToInt24(x0,x1,x2){ //Turns a 3 byte sequence into a 24 bit int\n",
        "        return x0 * 65536 + x1 * 256 + x2;\n",
        "    }\n",
        " \n",
        "    int24ToBytes(y){ //Turns a 24 bit int into a 3 byte sequence\n",
        "        return [y & 0xFF , (y >> 8) & 0xFF , (y >> 16) & 0xFF];\n",
        "    }\n",
        " \n",
        "    decode(buffer = this.buffer) { //returns true if successful, returns false if not\n",
        " \n",
        "        var needle = this.searchString\n",
        "        var haystack = buffer;\n",
        "        var search = this.boyerMoore(needle);\n",
        "        var skip = search.byteLength;\n",
        "        var indices = [];\n",
        "        let newLines = 0;\n",
        "    \n",
        "        for (var i = search(haystack); i !== -1; i = search(haystack, i + skip)) {\n",
        "            indices.push(i);\n",
        "        }\n",
        "        //console.log(indices);\n",
        "        if(indices.length >= 2){\n",
        "            for(let k = 1; k < indices.length; k++) {\n",
        "                if(indices[k] - indices[k-1] !== 105) {\n",
        "                    \n",
        "                } //This is not a valid sequence going by size, drop sequence and return\n",
        "                else {\n",
        "                    var line = buffer.slice(indices[k-1],indices[k]+1); //Splice out this line to be decoded\n",
        "                    \n",
        "                    // line[0] = stop byte, line[1] = start byte, line[2] = counter, line[3:99] = ADC data 32x3 bytes, line[100-104] = Accelerometer data 3x2 bytes\n",
        " \n",
        "                    //line found, decode.\n",
        "                    if(this.data.count < this.maxBufferedSamples){\n",
        "                        this.data.count++;\n",
        "                    }\n",
        " \n",
        "                    if(this.data.count-1 === 0) {this.data.ms[this.data.count-1]= Date.now(); this.data.startms = this.data.ms[0];}\n",
        "                    else {\n",
        "                        this.data.ms[this.data.count-1]=this.data.ms[this.data.count-2]+this.updateMs;\n",
        "                        \n",
        "                        if(this.data.count >= this.maxBufferedSamples) {\n",
        "                            this.data.ms.splice(0,5120);\n",
        "                            this.data.ms.push(new Array(5120).fill(0));\n",
        "                        }\n",
        "                    }//Assume no dropped samples\n",
        "                  var sample_count = line[2];\n",
        "                  var sample_count_diff = sample_count-this.samples_count;\n",
        "          if(sample_count_diff<0){\n",
        "            sample_count_diff+=256;\n",
        "          }\n",
        "          if(sample_count_diff!=1)\n",
        "          {\n",
        "            console.error(\"dropped samples:\"+sample_count_diff.toString());\n",
        "          }\n",
        "          this.samples_count=sample_count;\n",
        " \n",
        "                    for(var i = 3; i < 99; i+=3) {\n",
        "                        var channel = \"A\"+(i-3)/3;\n",
        "                        this.data[channel][this.data.count-1]=this.bytesToInt24(line[i],line[i+1],line[i+2]);\n",
        "                        if(this.data.count >= this.maxBufferedSamples) { \n",
        "                            this.data[channel].splice(0,5120);\n",
        "                            this.data[channel].push(new Array(5120).fill(0));//shave off the last 10 seconds of data if buffer full (don't use shift())\n",
        "                        }\n",
        "                            //console.log(this.data[channel][this.data.count-1],indices[k], channel)\n",
        "                    }\n",
        " \n",
        "                    this.data[\"Ax\"][this.data.count-1]=this.bytesToInt16(line[99],line[100]);\n",
        "                    this.data[\"Ay\"][this.data.count-1]=this.bytesToInt16(line[101],line[102]);\n",
        "                    this.data[\"Az\"][this.data.count-1]=this.bytesToInt16(line[103],line[104]);\n",
        " \n",
        "                    \n",
        "                    if(this.data.count >= this.maxBufferedSamples) { \n",
        "                        this.data[\"Ax\"].splice(0,5120);\n",
        "                        this.data[\"Ay\"].splice(0,5120);\n",
        "                        this.data[\"Az\"].splice(0,5120);\n",
        "                        this.data[\"Ax\"].push(new Array(5120).fill(0))\n",
        "                        this.data[\"Ay\"].push(new Array(5120).fill(0))\n",
        "                        this.data[\"Az\"].push(new Array(5120).fill(0))\n",
        "                        this.data.count -= 5120;\n",
        "                    }\n",
        "                    //console.log(this.data)\n",
        "                    newLines++;\n",
        "                    //console.log(indices[k-1],indices[k])\n",
        "                    //console.log(buffer[indices[k-1],buffer[indices[k]]])\n",
        "                    //indices.shift();\n",
        "                }\n",
        "                \n",
        "            }\n",
        "            if(newLines > 0) buffer.splice(0,indices[indices.length-1]);\n",
        "   \n",
        "            return newLines;\n",
        "            //Continue\n",
        "        }\n",
        "        //else {this.buffer = []; return false;}\n",
        "    }\n",
        "    //Callbacks\n",
        "    onDecodedCallback(newLinesInt){\n",
        "        //console.log(\"new samples:\", newLinesInt);\n",
        "        this.bufferednewLines=this.bufferednewLines+newLinesInt;\n",
        "    }\n",
        " \n",
        "    onConnectedCallback() {\n",
        "        console.log(\"port connected!\");\n",
        "    }\n",
        " \n",
        "    onDisconnectedCallback() {\n",
        "        console.log(\"port disconnected!\");\n",
        "    }\n",
        " \n",
        "    onReceive(value){\n",
        "        this.buffer.push(...value);\n",
        "        let newLines=this.buffer.length;\n",
        "        this.onDecodedCallback(newLines);\n",
        "        \n",
        "        if(this.ready_to_send_data)\n",
        "        {\n",
        "          //this.sendserial(this.buffer);\n",
        "          //console.log(this.buffer.length)\n",
        "          //this.buffer=[];\n",
        "        }\n",
        "        //console.log(value.length)\n",
        "\n",
        " \n",
        "        //let newLines = this.decode(this.buffer);\n",
        "        //console.log(this.data)\n",
        "        //console.log(\"decoding... \", this.buffer.length)\n",
        "        //if(newLines !== false && newLines !== 0 && !isNaN(newLines) ) this.onDecodedCallback(newLines);\n",
        "    }\n",
        " \n",
        "    async onPortSelected(port,baud=this.baudrate) {\n",
        "        try{\n",
        "            try {\n",
        "                await port.open({ baudRate: baud, bufferSize: this.readBufferSize });\n",
        "                this.onConnectedCallback();\n",
        "                this.connected = true;\n",
        "                this.subscribed = true;\n",
        "                this.subscribe(port);//this.subscribeSafe(port);\n",
        "        \n",
        "            } //API inconsistency in syntax between linux and windows\n",
        "            catch {\n",
        "                await port.open({ baudrate: baud, buffersize: this.readBufferSize });\n",
        "                this.onConnectedCallback();\n",
        "                this.connected = true;\n",
        "                this.subscribed = true;\n",
        "                this.subscribe(port);//this.subscribeSafe(port);\n",
        "            }\n",
        "        }\n",
        "        catch(err){\n",
        "            console.log(err);\n",
        "            this.connected = false;\n",
        "        }\n",
        "    }\n",
        " \n",
        "    async subscribe(port){\n",
        "        if (this.port.readable && this.subscribed === true) {\n",
        "            this.reader = port.readable.getReader();\n",
        "            const streamData = async () => {\n",
        "                try {\n",
        "                    const { value, done } = await this.reader.read();\n",
        "                    if (done || this.subscribed === false) {\n",
        "                        // Allow the serial port to be closed later.\n",
        "                        await this.reader.releaseLock();\n",
        "                        \n",
        "                    }\n",
        "                    if (value) {\n",
        "                        //console.log(value.length);\n",
        "                        try{\n",
        "                            this.onReceive(value);\n",
        "                        }\n",
        "                        catch (err) {console.log(err)}\n",
        "                        //console.log(\"new Read\");\n",
        "                        //console.log(this.decoder.decode(value));\n",
        "                    }\n",
        "                    if(this.subscribed === true) {\n",
        "                        setTimeout(()=>{streamData();}, this.readRate);//Throttled read 1/512sps = 1.953ms/sample @ 103 bytes / line or 1030bytes every 20ms\n",
        "                    }\n",
        "                } catch (error) {\n",
        "                    console.log(error);// TODO: Handle non-fatal read error.\n",
        "                    if(error.message.includes('framing') || error.message.includes('overflow') || error.message.includes('Overflow') || error.message.includes('break')) {\n",
        "                        this.subscribed = false;\n",
        "                        setTimeout(async ()=>{\n",
        "                            try{\n",
        "                            if (this.reader) {\n",
        "                                await this.reader.releaseLock();\n",
        "                                this.reader = null;\n",
        "                            }\n",
        "                            } catch (er){ console.error(er);}\n",
        "                            this.subscribed = true; \n",
        "                            this.subscribe(port);\n",
        "                            //if that fails then close port and reopen it\n",
        "                        },30); //try to resubscribe \n",
        "                    } else if (error.message.includes('parity') || error.message.includes('Parity') || error.message.includes('overrun') ) {\n",
        "                        if(this.port){\n",
        "                            this.subscribed = false;\n",
        "                            setTimeout(async () => {\n",
        "                                try{\n",
        "                                if (this.reader) {\n",
        "                                    await this.reader.releaseLock();\n",
        "                                    this.reader = null;\n",
        "                                }\n",
        "                                await port.close();\n",
        "                                } catch (er){ console.error(er);}\n",
        "                                //this.port = null;\n",
        "                                this.connected = false;\n",
        "                                setTimeout(()=>{this.onPortSelected(this.port)},100); //close the port and reopen\n",
        "                            }, 50);\n",
        "                        }\n",
        "                    }\n",
        "                     else {\n",
        "                        this.closePort();   \n",
        "                    }   \n",
        "                }\n",
        "            }\n",
        "            streamData();\n",
        "        }\n",
        "    }\n",
        " \n",
        "    //Unfinished\n",
        "    async subscribeSafe(port) { //Using promises instead of async/await to cure hangs when the serial update does not meet tick requirements\n",
        "        var readable = new Promise((resolve,reject) => {\n",
        "            while(this.port.readable && this.subscribed === true){\n",
        "                this.reader = port.readable.getReader();\n",
        "                var looper = true;\n",
        "                var prom1 = new Promise((resolve,reject) => {\n",
        "                    return this.reader.read();\n",
        "                });\n",
        " \n",
        "                var prom2 = new Promise((resolve,reject) => {\n",
        "                    setTimeout(resolve,100,\"readfail\");\n",
        "                });\n",
        "                while(looper === true ) {\n",
        "                    //console.log(\"reading...\");\n",
        "                    Promise.race([prom1,prom2]).then((result) => {\n",
        "                        console.log(\"newpromise\")\n",
        "                        if(result === \"readfail\"){\n",
        "                            console.log(result);\n",
        "                        }\n",
        "                        else{\n",
        "                            const {value, done} = result;\n",
        "                            if(done === true || this.subscribed === true) { var donezo = new Promise((resolve,reject) => {\n",
        "                                resolve(this.reader.releaseLock())}).then(() => {\n",
        "                                    looper = false;\n",
        "                                    return;\n",
        "                                });\n",
        "                            }\n",
        "                            else{\n",
        "                                this.onReceive(value);\n",
        "                            }\n",
        "                        }\n",
        "                    });\n",
        "                }\n",
        "            }\n",
        "            resolve(\"not readable\");\n",
        "        });\n",
        "    }\n",
        " \n",
        "    async closePort(port=this.port) {\n",
        "        //if(this.reader) {this.reader.releaseLock();}\n",
        "        if(this.port){\n",
        "            this.subscribed = false;\n",
        "            setTimeout(async () => {\n",
        "                if (this.reader) {\n",
        "                    await this.reader.releaseLock();\n",
        "                    this.reader = null;\n",
        "                }\n",
        "                await port.close();\n",
        "                this.port = null;\n",
        "                this.connected = false;\n",
        "                this.onDisconnectedCallback();\n",
        "            }, 100);\n",
        "        }\n",
        "    }\n",
        " \n",
        "    async setupSerialAsync(baudrate=this.baudrate) { //You can specify baudrate just in case\n",
        " \n",
        "        const filters = [\n",
        "            { usbVendorId: 0x10c4, usbProductId: 0x0043 } //CP2102 filter (e.g. for UART via ESP32)\n",
        "        ];\n",
        " \n",
        "        this.port = await navigator.serial.requestPort();\n",
        "        navigator.serial.addEventListener(\"disconnect\",(e) => {\n",
        "            this.closePort(this.port);\n",
        "        });\n",
        "        this.onPortSelected(this.port,baudrate);\n",
        " \n",
        "        //navigator.serial.addEventListener(\"onReceive\", (e) => {console.log(e)});//this.onReceive(e));\n",
        " \n",
        "    }\n",
        " \n",
        " \n",
        "    //Boyer Moore fast byte search method copied from https://codereview.stackexchange.com/questions/20136/uint8array-indexof-method-that-allows-to-search-for-byte-sequences\n",
        "    asUint8Array(input) {\n",
        "        if (input instanceof Uint8Array) {\n",
        "            return input;\n",
        "        } else if (typeof(input) === 'string') {\n",
        "            // This naive transform only supports ASCII patterns. UTF-8 support\n",
        "            // not necessary for the intended use case here.\n",
        "            var arr = new Uint8Array(input.length);\n",
        "            for (var i = 0; i < input.length; i++) {\n",
        "            var c = input.charCodeAt(i);\n",
        "            if (c > 127) {\n",
        "                throw new TypeError(\"Only ASCII patterns are supported\");\n",
        "            }\n",
        "            arr[i] = c;\n",
        "            }\n",
        "            return arr;\n",
        "        } else {\n",
        "            // Assume that it's already something that can be coerced.\n",
        "            return new Uint8Array(input);\n",
        "        }\n",
        "    }\n",
        " \n",
        "    boyerMoore(patternBuffer) {\n",
        "        // Implementation of Boyer-Moore substring search ported from page 772 of\n",
        "        // Algorithms Fourth Edition (Sedgewick, Wayne)\n",
        "        // http://algs4.cs.princeton.edu/53substring/BoyerMoore.java.html\n",
        "        \n",
        "//      USAGE:\n",
        "            // needle should be ASCII string, ArrayBuffer, or Uint8Array\n",
        "            // haystack should be an ArrayBuffer or Uint8Array\n",
        "//          var search = boyerMoore(needle);\n",
        "//          var skip = search.byteLength;\n",
        "//          var indices = [];\n",
        "//          for (var i = search(haystack); i !== -1; i = search(haystack, i + skip)) {\n",
        "//              indices.push(i);\n",
        "//          }\n",
        "        \n",
        "        var pattern = this.asUint8Array(patternBuffer);\n",
        "        var M = pattern.length;\n",
        "        if (M === 0) {\n",
        "            throw new TypeError(\"patternBuffer must be at least 1 byte long\");\n",
        "        }\n",
        "        // radix\n",
        "        var R = 256;\n",
        "        var rightmost_positions = new Int32Array(R);\n",
        "        // position of the rightmost occurrence of the byte c in the pattern\n",
        "        for (var c = 0; c < R; c++) {\n",
        "            // -1 for bytes not in pattern\n",
        "            rightmost_positions[c] = -1;\n",
        "        }\n",
        "        for (var j = 0; j < M; j++) {\n",
        "            // rightmost position for bytes in pattern\n",
        "            rightmost_positions[pattern[j]] = j;\n",
        "        }\n",
        "        var boyerMooreSearch = (txtBuffer, start, end) => {\n",
        "            // Return offset of first match, -1 if no match.\n",
        "            var txt = this.asUint8Array(txtBuffer);\n",
        "            if (start === undefined) start = 0;\n",
        "            if (end === undefined) end = txt.length;\n",
        "            var pat = pattern;\n",
        "            var right = rightmost_positions;\n",
        "            var lastIndex = end - pat.length;\n",
        "            var lastPatIndex = pat.length - 1;\n",
        "            var skip;\n",
        "            for (var i = start; i <= lastIndex; i += skip) {\n",
        "                skip = 0;\n",
        "                for (var j = lastPatIndex; j >= 0; j--) {\n",
        "                var c = txt[i + j];\n",
        "                if (pat[j] !== c) {\n",
        "                    skip = Math.max(1, j - right[c]);\n",
        "                    break;\n",
        "                }\n",
        "                }\n",
        "                if (skip === 0) {\n",
        "                return i;\n",
        "                }\n",
        "            }\n",
        "            return -1;\n",
        "        };\n",
        "        boyerMooreSearch.byteLength = pattern.byteLength;\n",
        "        return boyerMooreSearch;\n",
        "    }\n",
        "    //---------------------end copy/pasted solution------------------------\n",
        " \n",
        "     async  sendserial() {\n",
        "    //console.log('sending sendserial');\n",
        "     if(this.ready_to_send_data&&this.bufferednewLines)//&&this.data_slice[0].length)\n",
        "     {\n",
        "        if(!this.generate_parallel)\n",
        "       {\n",
        "         this.ready_to_send_data=false;\n",
        "       }\n",
        "  this.data_send_count++;\n",
        "  var array_to_send_as_json='';\n",
        "    var value = new Uint8Array(this.buffer);\n",
        "    this.bufferednewLines=0;\n",
        "    this.buffer=[];\n",
        "    //console.log('sending buffer');\n",
        "            //document.body.appendChild(document.createTextNode('sending buffer'));\n",
        "\n",
        "  const channel = await google.colab.kernel.comms.open('comm_target1', array_to_send_as_json, [value.buffer]);\n",
        "  let success = false;\n",
        "  for await (const message of channel.messages) {\n",
        "       //this.ready_to_send_data=true;\n",
        "    if (message.data.response == 'close') {\n",
        "    //if (message.data.response == 'got comm open!') {\n",
        "      //const responseBuffer = new Uint8Array(message.buffers[0]);\n",
        "      //for (let i = 0; i < buffer.length; ++i) {\n",
        "      //  if (responseBuffer[i] != buffer[i]) {\n",
        "      //    console.error('comm buffer different at ' + i);\n",
        "      //    document.body.appendChild(document.createTextNode('comm buffer different at2 ' + i));\n",
        "      //    return;\n",
        "      //  }\n",
        "      //}\n",
        "      // Close the channel once the expected message is received. This should\n",
        "      // cause the messages iterator to complete and for the for-await loop to\n",
        "      // end.\n",
        "      //console.error('comm buffer same ' + responseBuffer);\n",
        "      //document.body.appendChild(document.createTextNode('comm buffer same2 ' + responseBuffer));\n",
        "      channel.close();\n",
        "    }\n",
        "    //console.log('audio&image received');\n",
        "    //var message_parsed=JSON.parse(message.data.response);\n",
        "    //console.log('audio&image decoded');\n",
        "    //for(let i = 0; i < message_parsed.length; ++i)\n",
        "    {\n",
        "      \n",
        "      if (this.generate_wavegan)\n",
        "      {\n",
        "        //if((typeof message_parsed[i]) === 'string')\n",
        "        {\n",
        "          if(message.data.response.startsWith('data:audio'))\n",
        "          //if(message_parsed[i].startsWith('data:audio'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('audio decoded'));\n",
        "            //await \n",
        "            //playAudio1(message_parsed[i]);\n",
        "            //await \n",
        "            //playAudio1(message.buffers[0]);\n",
        "            playAudio1(message.data.response);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_wg));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(playAudio2,next_time);\n",
        "          }\n",
        "          if(0)\n",
        "          if(message.data.response.startsWith('data:video'))\n",
        "          //if(message_parsed[i].startsWith('data:audio'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('audio decoded'));\n",
        "            //await \n",
        "            //playAudio1(message_parsed[i]);\n",
        "            //await \n",
        "            playvideo1(message.data.response);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_wg));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(playVideo2,next_time);\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      if (this.generate_stylegan2)\n",
        "      {\n",
        "        //if((typeof message_parsed[i]) === 'string')\n",
        "          //console.log(message.data.response);\n",
        "        {\n",
        "          //console.log(message.buffers);\n",
        "          //if(message.buffers[0].length>0)\n",
        "          if(message.data.response.startsWith('binary:data:image'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('image decoded'));\n",
        "\n",
        "            var image_type='';\n",
        "            if(message.data.response.startsWith('binary:data:image/png'))\n",
        "            {\n",
        "              image_type='image/png';\n",
        "            }\n",
        "            if(message.data.response.startsWith('binary:data:image/jpeg'))\n",
        "            {\n",
        "              image_type='image/jpeg';\n",
        "            }\n",
        "            if(message.data.response.includes('user:killed'))\n",
        "            {\n",
        "              console.log('user:killed');\n",
        "              avic01.clear();\n",
        "              avic02.clear();\n",
        "            }\n",
        "            if(message.data.response.includes('enemy:killed'))\n",
        "            {\n",
        "              console.log('enemy:killed');\n",
        "              avic02.clear();\n",
        "            }\n",
        "            if(message.data.response.includes('user:add'))\n",
        "            {\n",
        "              console.log('user:add');\n",
        "              avic01.displayPhoto1(message.buffers[0],this.xsize,this.ysize,image_type);\n",
        "              //displayPhoto1001(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic01.displayPhoto4();},next_time);\n",
        "            }\n",
        "            if(message.data.response.includes('user:attack'))\n",
        "            if(0)\n",
        "            {\n",
        "              console.log('user:attack');\n",
        "              displayPhoto10011(message.buffers[0],this.xsize,this.ysize,image_type);\n",
        "              //displayPhoto1001(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(displayPhoto40011,next_time);\n",
        "            }\n",
        "            if(message.data.response.includes('enemy:add'))\n",
        "            {\n",
        "              console.log('enemy:add');\n",
        "              avic02.displayPhoto1(message.buffers[1],this.xsize,this.ysize,image_type);\n",
        "              //displayPhoto1001(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic02.displayPhoto4();},next_time);\n",
        "            }\n",
        "            if(message.data.response.includes('enemy:attack'))\n",
        "            if(0)\n",
        "            {\n",
        "              console.log('enemy:attack');\n",
        "              displayPhoto10021(message.buffers[1],this.xsize,this.ysize,image_type);\n",
        "              //displayPhoto1001(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(displayPhoto40021,next_time);\n",
        "            }\n",
        "            if(message.data.response.includes('user:restored'))\n",
        "            if(0)\n",
        "            {\n",
        "              console.log('user:restored');\n",
        "            }\n",
        "            var image_buffer_shift=0;\n",
        "            if(message.data.response.includes('mode:3'))\n",
        "            {\n",
        "              var image_buffer_shift=3;\n",
        "              avic1.displayPhoto1(message.buffers[2],this.xsize,this.ysize,image_type);\n",
        "              //displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic1.displayPhoto4();},next_time);\n",
        "              avic2.displayPhoto1(message.buffers[3],device.xsize,device.ysize,image_type);\n",
        "              //displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic2.displayPhoto4();},next_time);\n",
        "              avic3.displayPhoto1(message.buffers[4],this.xsize,this.ysize,image_type);\n",
        "              //displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic3.displayPhoto4();},next_time);\n",
        "            }\n",
        "            if(message.data.response.includes('user:cards_life'))\n",
        "            {\n",
        "              console.log('user:cards_life');\n",
        "              avic002.displayPhoto1(message.buffers[2+image_buffer_shift],this.xsize*2,this.ysize,'image/png');\n",
        "              //displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic002.displayPhoto4();},next_time);\n",
        "            }\n",
        "            if(message.data.response.includes('enemy:cards_life'))\n",
        "            {\n",
        "              console.log('enemy:cards_life');\n",
        "              avic001.displayPhoto1(message.buffers[3+image_buffer_shift],this.xsize*2,this.ysize,'image/png');\n",
        "              //displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic001.displayPhoto4();},next_time);\n",
        "            }\n",
        "            if(message.data.response.includes('user_attack_enemy:cards_life'))\n",
        "            {\n",
        "              console.log('user_attack_enemy:cards_life');\n",
        "              avic.displayPhoto1(message.buffers[4+image_buffer_shift],this.xsize*7,this.ysize*3,'image/png');\n",
        "              //avic.displayPhoto1(message.buffers[4],128,128,'image/png');\n",
        "              //displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "              this.time000=Date.now();\n",
        "              var frame_time=parseInt((1000/this.fps_sg2));\n",
        "              var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "              setTimeout(function(){avic.displayPhoto4();},next_time);\n",
        "            }\n",
        "            if(this.generate_game)\n",
        "            {\n",
        "              if(this.generate_game_mode1)\n",
        "              {\n",
        "                avic00.displayPhoto1(message.buffers[0],this.xsize,this.ysize,image_type);\n",
        "              }\n",
        "            }\n",
        "            else\n",
        "            {\n",
        "              avic00.displayPhoto1(message.buffers[0],this.xsize,this.ysize,image_type);\n",
        "              //avic00.displayPhoto1(message.buffers[0],512,512,image_type);\n",
        "            }\n",
        "            //displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "            this.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/this.fps_sg2));\n",
        "            var next_time=frame_time-((this.time000-this.time100)%%frame_time);\n",
        "            setTimeout(function(){avic00.displayPhoto4();},next_time);\n",
        "            //displayPhoto10(message.buffers[0],128,128,image_type);\n",
        "            ////displayPhoto10(message.buffers[0],device.xsize,device.ysize,image_type);\n",
        "            //device.time000=Date.now();\n",
        "            //var frame_time=parseInt((1000/device.fps_sg2));\n",
        "            //var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            //setTimeout(displayPhoto40,next_time);\n",
        "            var frame_now=(this.time000-this.time100)/frame_time;\n",
        "            if(Math.round(frame_now-this.frame_last)!=1)\n",
        "            {\n",
        "              console.log('f2:'+next_time+','+frame_time+','+\n",
        "                frame_now+','+this.frame_last+','+\n",
        "                (frame_now-this.frame_last));\n",
        "            }\n",
        "            this.frame_last=frame_now;\n",
        "            //console.log('show');\n",
        "          }\n",
        "          if(0)\n",
        "          if(message.data.response.startsWith('binary:data:video'))\n",
        "          //if(message_parsed[i].startsWith('data:image'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('image decoded'));\n",
        "            //await \n",
        "            //displayPhoto1(message_parsed[i]);\n",
        "            //await \n",
        "            var video_type='';\n",
        "            if(message.data.response.startsWith('binary:data:video/webm'))\n",
        "            {\n",
        "              video_type='video/webm';\n",
        "            }\n",
        "            if(message.data.response.startsWith('binary:data:video/mp4'))\n",
        "            {\n",
        "              video_type='video/mp4';\n",
        "            }\n",
        "            displayVideo10(message.buffers[0],device.xsize,device.ysize,video_type);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_sg2));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(displayVideo40,next_time);\n",
        "            var frame_now=(device.time000-device.time100)/frame_time;\n",
        "            if(Math.round(frame_now-this.frame_last)!=1)\n",
        "            {\n",
        "              console.log('f2:'+next_time+','+frame_time+','+\n",
        "                frame_now+','+this.frame_last+','+\n",
        "                (frame_now-this.frame_last));\n",
        "            }\n",
        "            this.frame_last=frame_now;\n",
        "          }\n",
        "          //console.log(\"image string\");\n",
        "          if(0)\n",
        "          if(message.data.response.startsWith('data:image'))\n",
        "          //if(message_parsed[i].startsWith('data:image'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('image decoded'));\n",
        "            //await \n",
        "            //displayPhoto1(message_parsed[i]);\n",
        "            //await \n",
        "            displayPhoto1(message.data.response,device.xsize,device.ysize);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_sg2));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(displayPhoto4,next_time);\n",
        "            var frame_now=(device.time000-device.time100)/frame_time;\n",
        "            if(Math.round(frame_now-this.frame_last)!=1)\n",
        "            {\n",
        "              console.log('f2:'+next_time+','+frame_time+','+\n",
        "                frame_now+','+this.frame_last+','+\n",
        "                (frame_now-this.frame_last));\n",
        "            }\n",
        "            this.frame_last=frame_now;\n",
        "          }\n",
        "          if(0)\n",
        "          if(message.data.response.startsWith('data:video'))\n",
        "          //if(message_parsed[i].startsWith('data:image'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('image decoded'));\n",
        "            //await \n",
        "            //displayPhoto1(message_parsed[i]);\n",
        "            //await \n",
        "            var video_type='';\n",
        "            if(message.data.response.startsWith('data:video/webm'))\n",
        "            {\n",
        "              video_type='video/webm';\n",
        "            }\n",
        "            if(message.data.response.startsWith('data:video/mp4'))\n",
        "            {\n",
        "              video_type='video/mp4';\n",
        "            }\n",
        "            displayVideo1(message.data.response,device.xsize,device.ysize,video_type);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_sg2));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(displayVideo2,next_time);\n",
        "            var frame_now=(device.time000-device.time100)/frame_time;\n",
        "            if(Math.round(frame_now-this.frame_last)!=1)\n",
        "            {\n",
        "              console.log('f2:'+next_time+','+frame_time+','+\n",
        "                frame_now+','+this.frame_last+','+\n",
        "                (frame_now-this.frame_last));\n",
        "            }\n",
        "            this.frame_last=frame_now;\n",
        "          }\n",
        "        }\n",
        "        //else\n",
        "        {\n",
        "          //console.log(\"image not string\");\n",
        "          //displayPhoto3(message_parsed[i]);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    //console.log(\"close\");\n",
        "    channel.close();\n",
        "  }\n",
        "      this.ready_to_send_data = true;\n",
        "    //console.log(\"ready_to_send_data\");\n",
        "  //document.body.appendChild(document.createTextNode('done2.'));\n",
        "     } \n",
        "     }\n",
        " \n",
        "    async  takeandsendSlice(data_slice_from,data_slice_to) {\n",
        "        //this.lastnewLines=this.bufferednewLines;\n",
        "     //if(this.ready_to_send_data&&this.bufferednewLines){//&&this.data_slice[0].length)\n",
        "     if(this.ready_to_send_data&&this.bufferednewLines)//&&this.data_slice[0].length)\n",
        "     {\n",
        "      //this.ready_to_send_data = false;\n",
        "      this.data_slice = [\n",
        "        //device.data.ms.slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+0].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+1].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+2].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+3].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+4].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+5].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+6].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+7].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+8].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+9].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+10].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+11].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+12].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+13].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+14].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+15].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+16].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+17].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+18].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+19].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+20].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+21].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+22].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+23].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+24].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+25].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+26].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+27].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+28].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+29].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+30].slice(data_slice_from,data_slice_to),\n",
        "        this.data[\"A\"+31].slice(data_slice_from,data_slice_to)\n",
        "        ];\n",
        "  this.bufferednewLines=0;\n",
        "  //const buffer = new Uint8Array(10);\n",
        "  //for (let i = 0; i < buffer.byteLength; ++i) {\n",
        "  //  buffer[i] = i\n",
        "  //}\n",
        "  var data_slice_uint32array=new Uint32Array(this.data_slice.length*this.data_slice[0].length);\n",
        "  for(let i=0;i<this.data_slice.length;i++)\n",
        "  {\n",
        "    for(let j=0;j<this.data_slice[i].length;j++)\n",
        "    {\n",
        "      data_slice_uint32array[i*this.data_slice[i].length + j]=this.data_slice[i][j];\n",
        "    }\n",
        "  }\n",
        "  //var data_slice_uint8array = new Int8Array(this.data_slice_array.buffer);\n",
        "\n",
        "  //const buffer = new Uint8Array(this.data_slice.byteLength);\n",
        "  //for (let i = 0; i < buffer.byteLength; ++i) {\n",
        "  //  buffer[i] = i\n",
        "  //}\n",
        "  \n",
        "  \n",
        "  //var array_to_send_as_json = JSON.stringify(this.data_slice);\n",
        "  var array_to_send_as_json = JSON.stringify([]);\n",
        "  //document.body.appendChild(document.createTextNode('sending ready'));\n",
        "  this.data_send_count++;\n",
        "  //if(this.channel==None)\n",
        "  //{\n",
        "  //  this.channel = await google.colab.kernel.comms.open('comm_target1', array_to_send_as_json, []);\n",
        "    //this.channel = await google.colab.kernel.comms.open(this.data_send_count.toString(), array_to_send_as_json, []);\n",
        "  //} else \n",
        "  //{\n",
        "    //this.channel.send(this.data_send_count.toString())\n",
        "  //}\n",
        "  //document.body.appendChild(document.createTextNode(array_to_send_as_json));\n",
        "  //const channel = await google.colab.kernel.comms.open('comm_target1', array_to_send_as_json, []);\n",
        "//  const channel = await google.colab.kernel.comms.open('comm_target1', array_to_send_as_json, [this.data_slice.buffer]);\n",
        "  const channel = await google.colab.kernel.comms.open('comm_target1', array_to_send_as_json, [data_slice_uint32array.buffer]);\n",
        "  //const channel = await google.colab.kernel.comms.open('comm_target1', array_to_send_as_json, [buffer.buffer]);\n",
        "  //const channel = await google.colab.kernel.comms.open('comm_target1', array_to_send_as_json, [this.data_slice]);\n",
        "  //const channel = await google.colab.kernel.comms.open('comm_target1', 'the data', [buffer.buffer]);\n",
        "  let success = false;\n",
        "  for await (const message of channel.messages) {\n",
        "    if (message.data.response == 'close') {\n",
        "    //if (message.data.response == 'got comm open!') {\n",
        "      //const responseBuffer = new Uint8Array(message.buffers[0]);\n",
        "      //for (let i = 0; i < buffer.length; ++i) {\n",
        "      //  if (responseBuffer[i] != buffer[i]) {\n",
        "      //    console.error('comm buffer different at ' + i);\n",
        "      //    document.body.appendChild(document.createTextNode('comm buffer different at2 ' + i));\n",
        "      //    return;\n",
        "      //  }\n",
        "      //}\n",
        "      // Close the channel once the expected message is received. This should\n",
        "      // cause the messages iterator to complete and for the for-await loop to\n",
        "      // end.\n",
        "      //console.error('comm buffer same ' + responseBuffer);\n",
        "      //document.body.appendChild(document.createTextNode('comm buffer same2 ' + responseBuffer));\n",
        "      channel.close();\n",
        "    }\n",
        "    //console.log('audio&image received');\n",
        "    //var message_parsed=JSON.parse(message.data.response);\n",
        "    //console.log('audio&image decoded');\n",
        "    //for(let i = 0; i < message_parsed.length; ++i)\n",
        "    {\n",
        "      \n",
        "      if (this.generate_wavegan)\n",
        "      {\n",
        "        //if((typeof message_parsed[i]) === 'string')\n",
        "        {\n",
        "          if(message.data.response.startsWith('data:audio'))\n",
        "          //if(message_parsed[i].startsWith('data:audio'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('audio decoded'));\n",
        "            //await \n",
        "            //playAudio1(message_parsed[i]);\n",
        "            //await \n",
        "            //playAudio1(message.buffers[0]);\n",
        "            playAudio1(message.data.response);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_wg));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(playAudio2,next_time);\n",
        "          }\n",
        "          if(0)\n",
        "          if(message.data.response.startsWith('data:video'))\n",
        "          //if(message_parsed[i].startsWith('data:audio'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('audio decoded'));\n",
        "            //await \n",
        "            //playAudio1(message_parsed[i]);\n",
        "            //await \n",
        "            playvideo1(message.data.response);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_wg));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(playVideo2,next_time);\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      if (this.generate_stylegan2)\n",
        "      {\n",
        "        //if((typeof message_parsed[i]) === 'string')\n",
        "        {\n",
        "          //console.log(message.buffers);\n",
        "          //if(message.buffers[0].length>0)\n",
        "          if(0)\n",
        "          {\n",
        "            displayPhoto10(message.buffers[0],device.xsize,device.ysize);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_sg2));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(displayPhoto40,next_time);\n",
        "            var frame_now=(device.time000-device.time100)/frame_time;\n",
        "            if(Math.round(frame_now-this.frame_last)!=1)\n",
        "            {\n",
        "              console.log('f2:'+next_time+','+frame_time+','+\n",
        "                frame_now+','+this.frame_last+','+\n",
        "                (frame_now-this.frame_last));\n",
        "            }\n",
        "            this.frame_last=frame_now;\n",
        "          }\n",
        "          //console.log(\"image string\");\n",
        "          if(message.data.response.startsWith('data:image'))\n",
        "          //if(message_parsed[i].startsWith('data:image'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('image decoded'));\n",
        "            //await \n",
        "            //displayPhoto1(message_parsed[i]);\n",
        "            //await \n",
        "            \n",
        "            \n",
        "            displayPhoto1(message.data.response,device.xsize,device.ysize);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_sg2));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(displayPhoto4,next_time);\n",
        "            var frame_now=(device.time000-device.time100)/frame_time;\n",
        "            if(Math.round(frame_now-this.frame_last)!=1)\n",
        "            {\n",
        "              console.log('f2:'+next_time+','+frame_time+','+\n",
        "                frame_now+','+this.frame_last+','+\n",
        "                (frame_now-this.frame_last));\n",
        "            }\n",
        "            this.frame_last=frame_now;\n",
        "          }\n",
        "          if(message.data.response.startsWith('data:video'))\n",
        "          //if(message_parsed[i].startsWith('data:image'))\n",
        "          {\n",
        "            //document.body.appendChild(document.createTextNode('image decoded'));\n",
        "            //await \n",
        "            //displayPhoto1(message_parsed[i]);\n",
        "            //await \n",
        "            var video_type='';\n",
        "            if(message.data.response.startsWith('data:video/webm'))\n",
        "            {\n",
        "              video_type='video/webm';\n",
        "            }\n",
        "            if(message.data.response.startsWith('data:video/mp4'))\n",
        "            {\n",
        "              video_type='video/mp4';\n",
        "            }\n",
        "            displayVideo1(message.data.response,device.xsize,device.ysize,video_type);\n",
        "            device.time000=Date.now();\n",
        "            var frame_time=parseInt((1000/device.fps_sg2));\n",
        "            var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "            setTimeout(displayVideo2,next_time);\n",
        "            var frame_now=(device.time000-device.time100)/frame_time;\n",
        "            if(Math.round(frame_now-this.frame_last)!=1)\n",
        "            {\n",
        "              console.log('f2:'+next_time+','+frame_time+','+\n",
        "                frame_now+','+this.frame_last+','+\n",
        "                (frame_now-this.frame_last));\n",
        "            }\n",
        "            this.frame_last=frame_now;\n",
        "          }\n",
        "        }\n",
        "        //else\n",
        "        {\n",
        "          //console.log(\"image not string\");\n",
        "          //displayPhoto3(message_parsed[i]);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "      this.ready_to_send_data = true;\n",
        "  //document.body.appendChild(document.createTextNode('done2.'));\n",
        "     } \n",
        " \n",
        "    }\n",
        " \n",
        "}\n",
        " \n",
        "device = new eeg32();\n",
        " \n",
        "    connect = async () => {\n",
        "        await this.device.setupSerialAsync();\n",
        "    }\n",
        " \n",
        "    disconnect = () => {\n",
        "        if (this.ui) this.ui.deleteNode()\n",
        "        this.device.closePort();\n",
        "    }\n",
        " \n",
        "      //const canvas = document.createElement('canvas');\n",
        "      //const audio = document.createElement('audio');\n",
        "      //const audio1 = document.createElement('audio');\n",
        "      //const audio2 = document.createElement('audio');\n",
        "      var audios = new Array();\n",
        "      var videos1 = new Array();\n",
        "      if(device.generate_wavegan)\n",
        "      {\n",
        "        const audios_length = 5;\n",
        "        for(let i = 0; i < audios_length; i++)\n",
        "        {\n",
        "          audios[i]=document.createElement('audio');\n",
        "        }\n",
        "        const videos1_length = 1;\n",
        "        for(let i = 0; i < videos1_length; i++)\n",
        "        {\n",
        "          videos1[i]=document.createElement('video');\n",
        "        }\n",
        "      }\n",
        "\n",
        " class audio_video_image_canvas { \n",
        " \n",
        "    constructor(\n",
        "        canvases_length = 1,\n",
        "        images_length = 1,\n",
        "        videos_length = 1,\n",
        "        width = 128,\n",
        "        height = 128\n",
        "        ) {\n",
        "          this.canvases = new Array();\n",
        "          this.ctxs = new Array();\n",
        "        this.canvases_length = canvases_length;//images_length;\n",
        "        for(let i = 0; i < this.canvases_length; i++)\n",
        "        {\n",
        "          //var ctx = canvas.getContext(\"2d\");\n",
        "          this.canvases[i]=document.createElement('canvas');\n",
        "          this.ctx = this.canvases[i].getContext(\"2d\");\n",
        "          this.ctxs[i]=this.canvases[i].getContext(\"2d\");\n",
        "          this.canvases[i].width=width;\n",
        "          this.canvases[i].height=height;\n",
        "          this.ctxs[i].clearRect(0, 0, this.canvases[i].width, this.canvases[i].height);\n",
        "        }\n",
        "      this.images = new Array();\n",
        "      this.videos = new Array();\n",
        "      //if(device.generate_stylegan2)\n",
        "      {\n",
        "        this.images_length = images_length;\n",
        "        //const canvases_length = 1;//images_length;\n",
        "        for(let i = 0; i < this.images_length; i++)\n",
        "        {\n",
        "          //images[i]=document.createElement('image');\n",
        "          //canvases[i]=document.createElement('canvas');\n",
        "          //var ctx = canvases[i].getContext(\"2d\");\n",
        "          this.images[i]=new Image();\n",
        "          //images[i].onload = function() {\n",
        "          //  ctx.drawImage(images[i], 0, 0);\n",
        "          //};\n",
        "        }\n",
        "        this.videos_length = videos_length;\n",
        "        for(let i = 0; i < this.videos_length; i++)\n",
        "        {\n",
        "          //videos2[i]=new Video();\n",
        "          this.videos[i]=document.createElement('video');\n",
        "          this.videos[i].addEventListener('play', function() {\n",
        "            var $this = this; //cache\n",
        "            (function loop() {\n",
        "              if (!$this.paused && !$this.ended) {\n",
        "                this.ctx.drawImage($this, 0, 0);\n",
        "                setTimeout(loop, 1000 / 10); // drawing at 30fps\n",
        "              }\n",
        "            })();\n",
        "          }, 0);\n",
        "        }\n",
        "      }\n",
        "\n",
        "          this.div = document.createElement('div');\n",
        "\n",
        "      //if(device.generate_stylegan2)\n",
        "      {\n",
        "        for(let i = 0; i <this.canvases.length; i++)\n",
        "        {\n",
        "          this.div.appendChild(this.canvases[i]);\n",
        "        }\n",
        "        for(let i = 0; i < this.videos.length; i++)\n",
        "        {\n",
        "          //div3.appendChild(videos2[i]);\n",
        "          //videos2[i].controls = true;\n",
        "          //videos2[i].autoplay = true;\n",
        "        }\n",
        "      }\n",
        "\n",
        "    this.image_now=0;\n",
        "    this.canvas_now=0;\n",
        "\n",
        "        }\n",
        "\n",
        "        async  clear() {\n",
        "              for(let i = 0; i < this.canvases.length; i++)\n",
        "              {\n",
        "                this.ctxs[i].clearRect(0, 0, this.canvases[i].width, this.canvases[i].height);\n",
        "              }\n",
        "              this.canvas_now=0;\n",
        "              this.image_now=0;\n",
        "\n",
        "        }\n",
        "\n",
        "    async  displayPhoto1(photodata,photoWidth=512,photoHeight=512,image_type=\"image/jpeg\") {\n",
        "      //if(canvas.width != photoWidth) canvas.width = photoWidth;\n",
        "      //if(canvas.height != photoHeight) canvas.height = photoHeight;\n",
        "      await this.displayPhoto2(photodata,photoWidth,photoHeight,image_type);\n",
        "    }\n",
        "    async  displayPhoto2(photodata,photoWidth,photoHeight,image_type=\"image/jpeg\") {\n",
        "      //if(canvas.width != photoWidth) canvas.width = photoWidth;\n",
        "      //if(canvas.height != photoHeight) canvas.height = photoHeight;\n",
        "     //image.src = photodata;\n",
        "      {\n",
        "       if(this.canvases[this.canvas_now%%this.canvases.length].width != photoWidth)\n",
        "       {\n",
        "         this.canvases[this.canvas_now%%this.canvases.length].width = photoWidth;\n",
        "       }\n",
        "       if(this.canvases[this.canvas_now%%this.canvases.length].height != photoHeight) \n",
        "       {\n",
        "         this.canvases[this.canvas_now%%this.canvases.length].height = photoHeight;\n",
        "       }\n",
        "       //if(canvases[0].width != photoWidth)\n",
        "       //{\n",
        "       //  canvases[0].width = photoWidth;\n",
        "       //}\n",
        "       //if(canvases[0].height != photoHeight) \n",
        "       //{\n",
        "       //  canvases[0].height = photoHeight;\n",
        "       //}\n",
        "       //console.log(photodata);\n",
        "       var arrayBufferView = new Uint8Array( photodata );\n",
        "       var blob = new Blob( [ arrayBufferView ], { type: image_type } );\n",
        "       var urlCreator = window.URL || window.webkitURL;\n",
        "       var imageUrl = urlCreator.createObjectURL( blob );\n",
        "       //images[image_now%%images.length].src = photodata;\n",
        "       //if(images[image_now%%images.length].src)\n",
        "       //{\n",
        "       //  URL.revokeObjectURL(images[image_now%%images.length].src);\n",
        "       //}\n",
        "       this.images[this.image_now%%this.images.length].src = imageUrl;\n",
        "        //audios[audio_now%%audios.length].play();\n",
        "        //console.log(imageUrl);\n",
        "      }\n",
        "      //image_now++;\n",
        "    }\n",
        "    async  displayPhoto4(photodata){//},photoWidth,photoHeight) {\n",
        "      this.ctxs[this.canvas_now%%this.canvases.length].drawImage(this.images[this.image_now%%this.images.length], 0, 0, this.canvases[this.canvas_now%%this.canvases.length].width, this.canvases[this.canvas_now%%this.canvases.length].height);\n",
        "      //ctxs01[image01_now%%images01.length].drawImage(images01[image01_now%%images01.length], 0, 0);\n",
        "      if((this.image_now-1)%%this.images.length>=0)\n",
        "      {\n",
        "        if(this.images[(this.image_now-1)%%this.images.length].src)\n",
        "        {\n",
        "          URL.revokeObjectURL(this.images[(this.image_now-1)%%this.images.length].src);\n",
        "        }\n",
        "      }\n",
        "      this.image_now++;\n",
        "      this.canvas_now++;\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "const div = document.createElement('div');\n",
        "const div01 = document.createElement('div');\n",
        "const div02 = document.createElement('div');\n",
        "const div03 = document.createElement('div');\n",
        "const div2 = document.createElement('div');\n",
        "const div3 = document.createElement('div');\n",
        "const div4 = document.createElement('div');\n",
        "const div5 = document.createElement('div');\n",
        "const div6 = document.createElement('div');\n",
        "const div7 = document.createElement('div');\n",
        "const div8 = document.createElement('div');\n",
        "const btnconnect = document.createElement('button');\n",
        "const btndisconnect = document.createElement('button');\n",
        "const capture = document.createElement('button');\n",
        "\n",
        "if (device.generate_game)\n",
        "{\n",
        "  if (device.generate_game_mode3)\n",
        "  {\n",
        "    avic1 = new audio_video_image_canvas(1,1,0);\n",
        "    avic2 = new audio_video_image_canvas(1,1,0);\n",
        "    avic3 = new audio_video_image_canvas(1,1,0);\n",
        "          div01.appendChild(avic1.div);\n",
        "          div02.appendChild(avic2.div);\n",
        "          div03.appendChild(avic3.div);\n",
        "  }\n",
        "  avic = new audio_video_image_canvas(1,1,0);\n",
        "  avic01 = new audio_video_image_canvas(7,7,0);\n",
        "  avic02 = new audio_video_image_canvas(7,7,0);\n",
        "          div3.appendChild(avic.div);\n",
        "          div4.appendChild(avic01.div);\n",
        "          div5.appendChild(avic02.div);\n",
        "  if (device.generate_game_mode1)\n",
        "  {\n",
        "    avic00 = new audio_video_image_canvas(1,1,0,device.xsize,device.ysize);\n",
        "          div6.appendChild(avic00.div);\n",
        "  }\n",
        "  avic001 = new audio_video_image_canvas(1,1,0,device.xsize*2,device.ysize);\n",
        "  avic002 = new audio_video_image_canvas(1,1,0,device.xsize*2,device.ysize);\n",
        "          div7.appendChild(avic001.div);\n",
        "          div8.appendChild(avic002.div);\n",
        "}\n",
        "else\n",
        "{\n",
        "  avic00 = new audio_video_image_canvas(1,1,0,device.xsize,device.ysize);\n",
        "//  avic00 = new audio_video_image_canvas(1,1,0,512,512);\n",
        "          div6.appendChild(avic00.div);\n",
        "}\n",
        "\n",
        "\n",
        "    async function takePhoto2(quality=1) {\n",
        "      btnconnect.remove();\n",
        "      capture.remove();\n",
        "      device.ready_to_send_data = true;\n",
        "    }\n",
        " \n",
        "    async function takePhoto(quality=1) {\n",
        " \n",
        "      btnconnect.textContent = 'connect';\n",
        "      div.appendChild(btnconnect);\n",
        "      btnconnect.onclick = this.connect;\n",
        "      \n",
        "      btndisconnect.textContent = 'disconnect';\n",
        "      div.appendChild(btndisconnect);\n",
        "      btndisconnect.onclick = this.disconnect;\n",
        "      \n",
        "      capture.textContent = 'Capture';\n",
        "      capture.onclick = takePhoto2;\n",
        "      div.appendChild(capture);\n",
        "     \n",
        "      //div.appendChild(canvas);\n",
        "      //div.appendChild(audio);\n",
        "      //div.appendChild(audio1);\n",
        "      //div.appendChild(audio2);\n",
        "      if(device.generate_wavegan)\n",
        "      {\n",
        "        for(let i = 0; i < audios.length; i++)\n",
        "        {\n",
        "          div2.appendChild(audios[i]);\n",
        "          //audios[i].controls = true;\n",
        "          //audios[i].autoplay = true;\n",
        "        }\n",
        "        for(let i = 0; i < videos1.length; i++)\n",
        "        {\n",
        "          div2.appendChild(videos1[i]);\n",
        "          //videos1[i].controls = true;\n",
        "          //videos1[i].autoplay = true;\n",
        "        }\n",
        "      }\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      if(device.generate_wavegan)\n",
        "      {\n",
        "        document.body.appendChild(div2);\n",
        "      }\n",
        "      if(device.generate_game)\n",
        "      {\n",
        "        document.body.appendChild(div5);\n",
        "        document.body.appendChild(div2);\n",
        "\n",
        "        document.body.appendChild(div7);\n",
        "        div7.style.styleFloat = 'left';\n",
        "        div7.style.cssFloat = 'left';\n",
        "      }\n",
        "      if(device.generate_game_mode3)\n",
        "      {\n",
        "        document.body.appendChild(div01);\n",
        "        div01.style.styleFloat = 'left';\n",
        "        div01.style.cssFloat = 'left';\n",
        "        document.body.appendChild(div02);\n",
        "        div02.style.styleFloat = 'left';\n",
        "        div02.style.cssFloat = 'left';\n",
        "        document.body.appendChild(div03);\n",
        "        div03.style.styleFloat = 'left';\n",
        "        div03.style.cssFloat = 'left';\n",
        "      }\n",
        "      else\n",
        "      {\n",
        "        document.body.appendChild(div6);\n",
        "        div6.style.styleFloat = 'left';\n",
        "        div6.style.cssFloat = 'left';\n",
        "      }\n",
        "      if(device.generate_game)\n",
        "      {\n",
        "        document.body.appendChild(div8);\n",
        "\n",
        "        document.body.appendChild(div4);\n",
        "        document.body.appendChild(div3);\n",
        "      }\n",
        "         await new Promise((resolve) => capture.onclick = resolve);\n",
        " \n",
        "      btnconnect.remove();\n",
        "      capture.remove();\n",
        "      device.ready_to_send_data = true;\n",
        "    }\n",
        "\n",
        "\n",
        "    async function takePhoto1(quality=1) {  \n",
        "      //var data_slice_send=this.device.data_slice;\n",
        "      //var data_slice_send=[this.device.data_slice[0],this.device.data_slice[1]];\n",
        "      //var data_slice_send=[this.device.data_slice[0]];\n",
        "      var data_slice_send=[[this.device.data_slice[0][0]]];\n",
        "        //console.log(\"data_slice_send[0].length:\", data_slice_send[0].length);\n",
        "        //console.log(\"device.bufferednewLines:\", device.bufferednewLines);\n",
        "      device.bufferednewLines=0;\n",
        "      return data_slice_send;      \n",
        "    }\n",
        " \n",
        "    var audio_now=0;\n",
        "    async function playAudio1(audiodata){//},photoWidth=512,photoHeight=512) {\n",
        "      //const canvas = document.createElement('canvas');\n",
        "      //if(canvas.width != photoWidth) canvas.width = photoWidth;\n",
        "      //if(canvas.height != photoHeight) canvas.height = photoHeight;\n",
        "      //audio.controls = true;\n",
        "      //audio.autoplay = true;\n",
        "      //audio1.controls = true;\n",
        "      //audio1.autoplay = true;\n",
        "      //audio2.controls = true;\n",
        "      //audio2.autoplay = true;\n",
        " \n",
        "      //canvas.getContext('2d').drawImage(photodata, 0, 0);\n",
        "      //var canvas = document.getElementById(\"c\");\n",
        "      ///var ctx = canvas.getContext(\"2d\");\n",
        " \n",
        "      ///var image = new Image();\n",
        "      ///image.onload = function() {\n",
        "      ///  ctx.drawImage(image, 0, 0);\n",
        "      ///};\n",
        "      //audio.src = audiodata;\n",
        "      //audio.play()\n",
        "      //if(audio_now%%2==0)\n",
        "      //{\n",
        "      //  audio1.src = audiodata;\n",
        "      //  audio1.play()\n",
        "      //}\n",
        "      //else\n",
        "      //{\n",
        "      //  audio2.src = audiodata;\n",
        "      //  audio2.play()\n",
        "      //}\n",
        "      //for(let i = 0; i < audios.length; ++i)\n",
        "      {\n",
        "        audios[audio_now%%audios.length].src = audiodata;\n",
        "        //if(audio_now==0)\n",
        "        {\n",
        "          audios[audio_now%%audios.length].controls = false;\n",
        "          //audios[audio_now%%audios.length].controls = true;\n",
        "          //audios[audio_now%%audios.length].autoplay = true;\n",
        "        }\n",
        "        //audios[audio_now%%audios.length].play();\n",
        "      }\n",
        "      //audio_now++;\n",
        " \n",
        "    }\n",
        "    async function playAudio2(audiodata){//},photoWidth=512,photoHeight=512) {\n",
        "      audios[audio_now%%audios.length].play();\n",
        "      audio_now++;\n",
        "    }\n",
        "    function addSourceToVideo(element, src, type) {\n",
        "      var source = document.createElement('source');\n",
        "\n",
        "      source.src = src;\n",
        "      source.type = type;\n",
        "\n",
        "      element.appendChild(source);\n",
        "    }\n",
        "\n",
        "    var video1_now=0;\n",
        "    async function playVideo1(videodata){//},photoWidth=512,photoHeight=512) {\n",
        "      //for(let i = 0; i < audios.length; ++i)\n",
        "      {\n",
        "        //addSourceToVideo(videos1[video1_now%%videos1.length], videodata, 'video/mp4');\n",
        "\n",
        "        videos1[video1_now%%videos1.length].src = videodata;\n",
        "        //if(audio_now==0)\n",
        "        {\n",
        "          videos1[video1_now%%videos1.length].controls = false;//true;\n",
        "          //videos1[video1_now%%videos1.length].load();\n",
        "          //audios[audio_now%%audios.length].autoplay = true;\n",
        "        }\n",
        "        //audios[audio_now%%audios.length].play();\n",
        "      }\n",
        "      //audio_now++;\n",
        " \n",
        "    }\n",
        "    async function playVideo2(videodata){//},photoWidth=512,photoHeight=512) {\n",
        "      videos1[video1_now%%videos1.length].play();\n",
        "      video1_now++;\n",
        "    }\n",
        "    var video2_now=0;\n",
        "    async function displayVideo1(videodata,photoWidth=512,photoHeight=512,video_type='video/mp4') {\n",
        "       if(videos2[video2_now%%videos2.length].width != photoWidth)\n",
        "       {\n",
        "         videos2[video2_now%%videos2.length].width = photoWidth;\n",
        "       }\n",
        "       if(videos2[video2_now%%videos2.length].height != photoHeight) \n",
        "       {\n",
        "         videos2[video2_now%%videos2.length].height = photoHeight;\n",
        "       }\n",
        "      //for(let i = 0; i < audios.length; ++i)\n",
        "      {\n",
        "        //addSourceToVideo(videos2[video2_now%%videos2.length], videodata, 'video/mp4');\n",
        "        videos2[video2_now%%videos2.length].src = videodata;\n",
        "        videos2[video2_now%%videos2.length].type = video_type;\n",
        "        //videos2[video2_now%%videos2.length].type = 'video/webm';\n",
        "//        videos2[video2_now%%videos2.length].type = 'video/mp4';\n",
        "        //if(audio_now==0)\n",
        "        {\n",
        "          //videos2[video2_now%%videos2.length].controls = true;\n",
        "          videos2[video2_now%%videos2.length].controls = false;\n",
        "          videos2[video2_now%%videos2.length].load();\n",
        "          //audios[audio_now%%audios.length].autoplay = true;\n",
        "        }\n",
        "        //audios[audio_now%%audios.length].play();\n",
        "      }\n",
        "      //audio_now++;\n",
        " \n",
        "    }\n",
        "    async function displayVideo2(videodata){//},photoWidth=512,photoHeight=512) {\n",
        "      videos2[video2_now%%videos2.length].play();\n",
        "      videos2[video2_now%%videos2.length].style.display = \"block\";\n",
        "      video2_now++;\n",
        "      videos2[video2_now%%videos2.length].style.display = \"none\";\n",
        "    }\n",
        "    async function displayVideo10(videodata,photoWidth=512,photoHeight=512,video_type='video/mp4') {\n",
        "       //if(videos2[video2_now%%videos2.length].width != photoWidth)\n",
        "       //{\n",
        "       //  videos2[video2_now%%videos2.length].width = photoWidth;\n",
        "       //}\n",
        "       //if(videos2[video2_now%%videos2.length].height != photoHeight) \n",
        "       //{\n",
        "       //  videos2[video2_now%%videos2.length].height = photoHeight;\n",
        "       //}\n",
        "       //console.log(\"canvases[video2_now%%videos2.length]:\", canvases[video2_now%%videos2.length]);\n",
        "       //if(canvases[video2_now%%videos2.length].width != photoWidth)\n",
        "       //{\n",
        "       //  canvases[video2_now%%videos2.length].width = photoWidth;\n",
        "       //}\n",
        "       //if(canvases[video2_now%%videos2.length].height != photoHeight) \n",
        "       //{\n",
        "       //  canvases[video2_now%%videos2.length].height = photoHeight;\n",
        "       //}\n",
        "       \n",
        "       if(canvases[0].width != photoWidth)\n",
        "       {\n",
        "         canvases[0].width = photoWidth;\n",
        "       }\n",
        "       if(canvases[0].height != photoHeight) \n",
        "       {\n",
        "         canvases[0].height = photoHeight;\n",
        "       }\n",
        "      //for(let i = 0; i < audios.length; ++i)\n",
        "      {\n",
        "        //addSourceToVideo(videos2[video2_now%%videos2.length], videodata, 'video/mp4');\n",
        "       var arrayBufferView = new Uint8Array( videodata );\n",
        "       var blob = new Blob( [ arrayBufferView ], { type: video_type } );\n",
        "       var urlCreator = window.URL || window.webkitURL;\n",
        "       var videoUrl = urlCreator.createObjectURL( blob );\n",
        "       //console.log(\"videoUrl:\", videoUrl);\n",
        "       //images[image_now%%images.length].src = photodata;\n",
        "\n",
        "        videos2[video2_now%%videos2.length].src = videoUrl;\n",
        "        videos2[video2_now%%videos2.length].type = video_type;\n",
        "        //videos2[video2_now%%videos2.length].type = 'video/webm';\n",
        "//        videos2[video2_now%%videos2.length].type = 'video/mp4';\n",
        "        //if(audio_now==0)\n",
        "        {\n",
        "          videos2[video2_now%%videos2.length].controls = true;\n",
        "          //videos2[video2_now%%videos2.length].controls = false;\n",
        "          videos2[video2_now%%videos2.length].load();\n",
        "          //audios[audio_now%%audios.length].autoplay = true;\n",
        "        }\n",
        "        //audios[audio_now%%audios.length].play();\n",
        "      }\n",
        "      //audio_now++;\n",
        " \n",
        "    }\n",
        "    async function displayVideo20(videodata){//},photoWidth=512,photoHeight=512) {\n",
        "      videos2[video2_now%%videos2.length].play();\n",
        "      videos2[video2_now%%videos2.length].style.display = \"block\";\n",
        "      video2_now++;\n",
        "      videos2[video2_now%%videos2.length].style.display = \"none\";\n",
        "    }\n",
        "    async function displayVideo40(videodata){//},photoWidth,photoHeight) {\n",
        "      //ctx.drawImage(videos2[video2_now%%videos2.length], 0, 0);\n",
        "      videos2[video2_now%%videos2.length].play();\n",
        "      if((video2_now-1)%%videos2.length>=0)\n",
        "      {\n",
        "        videos2[(video2_now-1)%%videos2.length].pause();\n",
        "        if(videos2[(video2_now-1)%%videos2.length].src)\n",
        "        {\n",
        "          URL.revokeObjectURL(videos2[(video2_now-1)%%videos2.length].src);\n",
        "        }\n",
        "      }\n",
        "      video2_now++;\n",
        "    }\n",
        "\n",
        "    \n",
        "  takePhoto();\n",
        "  data_count=0;\n",
        "  var frame_last=0;\n",
        "\n",
        "async function check_to_send() {\n",
        "  //while(true)\n",
        "  {\n",
        "        //console.log(\"device.bufferednewLines:\", device.bufferednewLines);\n",
        "   if(device.bufferednewLines)\n",
        "   {\n",
        "    //if(this.bufferednewLines>this.data_slice_size)\n",
        "    //  {\n",
        "    //    this.bufferednewLines=this.data_slice_size;\n",
        "    //  }\n",
        "    /*device.time000=Date.now();\n",
        "    if(device.generate_wavegan)\n",
        "    {\n",
        "      device.this_frame_wg=parseInt((device.time000-device.time100)*device.fps_wg);\n",
        "      if(device.this_frame_wg>device.last_frame_wg)\n",
        "      {\n",
        "        device.last_frame_wg=device.this_frame_wg;\n",
        "        device.send_wg=true;\n",
        "      }\n",
        "    }\n",
        "    if(device.generate_stylegan2)\n",
        "    {\n",
        "      device.this_frame_sg2=parseInt((device.time000-device.time100)*device.fps_sg2);\n",
        "      if(device.this_frame_sg2>device.last_frame_sg2)\n",
        "      {\n",
        "        device.last_frame_sg2=device.this_frame_sg2;\n",
        "        device.send_sg2=true;\n",
        "      }\n",
        "    }\n",
        "    if(device.send_wg || device.send_sg2)\n",
        "    //if(this.bufferednewLines>512/(this.fps_sg2))\n",
        "    if(device.ready_to_send_data)*/\n",
        "    {\n",
        "      //this.bufferednewLines=512/this.fps;\n",
        " \n",
        "      //device.ready_to_send_data = true;\n",
        "        device.sendserial();\n",
        "        //device.takeandsendSlice(device.data.count-1-device.bufferednewLines,device.data.count-1);\n",
        "      device.send_wg=false;\n",
        "      device.send_sg2=false;\n",
        "    }\n",
        "    //this.takeSlice(this.data.count-1-this.bufferednewLines,this.data.count-1);\n",
        "    //this.takeandsendSlice(this.data.count-1-this.bufferednewLines,this.data.count-1);\n",
        "    //this.takeandsendSliceBroadcast(this.data.count-1-this.bufferednewLines,this.data.count-1);\n",
        "   }\n",
        "  }\n",
        "  device.time000=Date.now();\n",
        "//  var frame_time=parseInt((1000/device.fps_wg)/12);\n",
        "  var frame_time=parseInt((1000/device.fps));\n",
        "  var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "  setTimeout(check_to_send,next_time);\n",
        "  var frame_now=(device.time000-device.time100)/frame_time;\n",
        "  if(Math.round(frame_now-frame_last)!=1)\n",
        "  {\n",
        "    console.log('f1:'+next_time+','+frame_time+','+frame_now+','+\n",
        "      frame_last+','+(frame_now-frame_last));\n",
        "  }\n",
        "  frame_last=frame_now;\n",
        "}\n",
        "  device.time000=Date.now();\n",
        "  var frame_time=parseInt((1000/device.fps));\n",
        "  var next_time=frame_time-((device.time000-device.time100)%%frame_time);\n",
        "  setTimeout(check_to_send,next_time);\n",
        "  //console.log(next_time);\n",
        "//var intervalID = setInterval(check_to_send,(1000/device.fps_wg)/10);\n",
        "//  window.requestAnimationFrame\n",
        " \n",
        "''' % {'xsize':xsize,'ysize':ysize,'generate_stylegan2':generate&gen_stylegan2,'generate_wavegan':generate&gen_wavegan,'generate_heatmap':generate&gen_heatmap,\n",
        "       'fps_sg2':fps_sg2,'fps_wg':fps_wg,'fps_hm':fps_hm,'sfreq':sfreq,'vref':vref,'gain':gain,'data_channels':data_channels,\n",
        "       'generate_game':generate&gen_game,'generate_game_mode1':generate&gen_game_mode1,'generate_game_mode3':generate&gen_game_mode3,\n",
        "       'generate_parallel':generate&gen_parallel})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLhd7pdGj-PY"
      },
      "outputs": [],
      "source": [
        "stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBC4ta4-VyN5"
      },
      "outputs": [],
      "source": [
        "                z_samples = psd_array_sg2 * vol\n",
        "                w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]\n",
        "                w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)       # [N, 1, C]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3PznB3sUSDB"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZiiNucjOiH8"
      },
      "outputs": [],
      "source": [
        "#@title methods { run: \"auto\" }\n",
        "\n",
        "method = \"ppc\" #@param ['coh', 'plv', 'ciplv', 'ppc', 'pli', 'wpli']\n",
        "print('You selected', method)\n",
        "methods = [method]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQSkOcqkztOT"
      },
      "outputs": [],
      "source": [
        "                          plt.figure()\n",
        "                          fig, axs = plt.subplots(2, game_num_user_cards)\n",
        "                          for i in range(game_num_user_cards):\n",
        "                            #for j in range(game_num_enemy_cards):\n",
        "                             # print(i,j)\n",
        "                             # if j==0:\n",
        "                             #   plt.fill_between(range(dim_sg2),game_user_attack_enemy_cards[i][j],np.zeros(dim_sg2))\n",
        "                             # else:\n",
        "                             #   plt.fill_between(range(dim_sg2),np.sum(game_user_attack_enemy_cards[i][:j],axis=0),np.sum(game_user_attack_enemy_cards[i][:j-1],axis=0))\n",
        "                            #plt.fill_between(range(dim_sg2),np.sum(np.sum(game_user_attack_enemy_cards,axis=0),axis=0),np.zeros(dim_sg2))\n",
        "\n",
        "                            axs[0, i].fill_between(range(dim_sg2),np.sum(game_user_stddev_compare_with_possible_cards,axis=0),np.zeros(dim_sg2))\n",
        "                            #game_user_attack_enemy_cards_possible[i2][j2][j1]=((game_user_cards[i2][j1]/(game_enemy_cards[j2][j1]+1))*(1))/game_num_enemy_cards\n",
        "                            #game_user_attack_enemy_cards[i2][j2][j1]=((game_user_cards[i2][j1]/(game_enemy_cards[j2][j1]+1))*(1-game_user_stddev_compare_with_possible_cards[i2]))/game_num_enemy_cards\n",
        "\n",
        "                            axs[0, i].set_title(\"user_attack_enemy_cards_life_\"+str(i))\n",
        "                            #plt.ylim(0, 1)\n",
        "                            #plt.ylim(0, game_num_enemy_cards)\n",
        "                            axs[0, i].set_ylim(0, game_num_user_cards)\n",
        "                          #buf2 = io.BytesIO()\n",
        "                          #buf2.seek(0)\n",
        "                          #plt.savefig(buf2, format='png')\n",
        "                          plt.show()\n",
        "                          #image_user_attack_enemy_cards_life.append(buf2.getvalue())\n",
        "                          #if i==0:\n",
        "                          #image_user_attack_enemy_cards_life0=buf2.getvalue()\n",
        "                          plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYBXaPVHp_0A"
      },
      "outputs": [],
      "source": [
        "#print(images)\n",
        "len(images)\n",
        "images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0dklJqOIG7k"
      },
      "outputs": [],
      "source": [
        "game_enemy_cards[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbzqGzv-MnWM"
      },
      "outputs": [],
      "source": [
        "game_user_cards[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0bvKRKYOy9Q"
      },
      "outputs": [],
      "source": [
        "                      user_cards=((np.sum(np.sum(game_user_cards_life[:game_num_user_cards],axis=0)+np.abs(np.sum(game_user_cards_life[:game_num_user_cards],axis=0)))/2)/dim_sg2)\n",
        "                      user_life=((np.sum(np.sum(game_user_cards[:game_num_user_cards],axis=0)+np.abs(np.sum(game_user_cards[:game_num_user_cards],axis=0)))/2)/dim_sg2)\n",
        "                      enemy_cards=((np.sum(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)+np.abs(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)))/2)/dim_sg2)\n",
        "                      enemy_life=((np.sum(np.sum(game_enemy_cards[:game_num_enemy_cards],axis=0)+np.abs(np.sum(game_enemy_cards[:game_num_enemy_cards],axis=0)))/2)/dim_sg2)\n",
        "                      \n",
        "                      user_cards_array=(np.sum(game_user_cards_life[:game_num_user_cards],axis=0)+np.abs(np.sum(game_user_cards_life[:game_num_user_cards],axis=0)))/2\n",
        "                      user_life_array=(np.sum(game_user_cards[:game_num_user_cards],axis=0)+np.abs(np.sum(game_user_cards[:game_num_user_cards],axis=0)))/2\n",
        "                      enemy_cards_array=(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)+np.abs(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)))/2\n",
        "                      enemy_life_array=(np.sum(game_enemy_cards[:game_num_enemy_cards],axis=0)+np.abs(np.sum(game_enemy_cards[:game_num_enemy_cards],axis=0)))/2\n",
        "                      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZB11q8sb3v-"
      },
      "outputs": [],
      "source": [
        "                      user_cards_array=(np.sum(game_user_cards_life[:game_num_user_cards],axis=0)+np.abs(np.sum(game_user_cards_life[:game_num_user_cards],axis=0)))/2\n",
        "                      user_life_array=(np.sum(game_user_cards[:game_num_user_cards],axis=0)+np.abs(np.sum(game_user_cards[:game_num_user_cards],axis=0)))/2\n",
        "                      enemy_cards_array=(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)+np.abs(np.sum(game_enemy_cards_life[:game_num_enemy_cards],axis=0)))/2\n",
        "                      enemy_life_array=(np.sum(game_enemy_cards[:game_num_enemy_cards],axis=0)+np.abs(np.sum(game_enemy_cards[:game_num_enemy_cards],axis=0)))/2\n",
        "                      plt.figure()\n",
        "                      plt.fill_between(range(dim_sg2),user_cards_array,user_life_array)\n",
        "                      plt.fill_between(range(dim_sg2),user_cards_array,np.zeros([dim_sg2]))\n",
        "                      plt.title(\"user_cards_life\")\n",
        "                      plt.ylim(0, game_num_user_cards)\n",
        "                      buf = io.BytesIO()\n",
        "                      plt.savefig(buf, format='png')\n",
        "                      buf.seek(0)\n",
        "                      im = PIL.Image.open(buf)\n",
        "                      im.show()\n",
        "                      plt.figure()\n",
        "                      plt.fill_between(range(dim_sg2),enemy_cards_array,enemy_life_array)\n",
        "                      plt.fill_between(range(dim_sg2),enemy_cards_array,np.zeros([dim_sg2]))\n",
        "                      plt.title(\"enemy_cards_life\")\n",
        "                      plt.ylim(0, game_num_enemy_cards)\n",
        "                      buf = io.BytesIO()\n",
        "                      plt.savefig(buf, format='png')\n",
        "                      buf.seek(0)\n",
        "                      im = PIL.Image.open(buf)\n",
        "                      im.show()\n",
        "                      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGxbjb7CO0Ww"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "#from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.fill_between(range(dim_sg2),user_cards_array,np.zeros([dim_sg2]))\n",
        "plt.title(\"user_cards\")\n",
        "plt.ylim(0, game_num_user_cards)\n",
        "buf = io.BytesIO()\n",
        "plt.savefig(buf, format='png')\n",
        "buf.seek(0)\n",
        "im = PIL.Image.open(buf)\n",
        "im.show()\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.fill_between(range(dim_sg2),user_cards_array,user_life_array)\n",
        "plt.fill_between(range(dim_sg2),user_cards_array,np.zeros([dim_sg2]))\n",
        "#plt.fill_between(range(dim_sg2),user_cards_array,np.zeros([dim_sg2]))\n",
        "plt.title(\"user_life\")\n",
        "plt.ylim(0, game_num_user_cards)\n",
        "buf = io.BytesIO()\n",
        "plt.savefig(buf, format='png')\n",
        "buf.seek(0)\n",
        "im = PIL.Image.open(buf)\n",
        "im.show()\n",
        "buf.seek(0)\n",
        "image_user_cards_life = buf.getvalue()\n",
        "\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgROISJkZ6wS"
      },
      "outputs": [],
      "source": [
        "user_cards_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNk4yerhAetK"
      },
      "outputs": [],
      "source": [
        "game_easy=3\n",
        "game_stddev_add_user_card=0.5\n",
        "game_boss_enemy_cards=2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yao8BaMUAwOr"
      },
      "outputs": [],
      "source": [
        "#@title game_easy { run: \"auto\" }\n",
        "\n",
        "game_easy = \"2\" #@param [2, 3, 4, 5, 6, 7]\n",
        "print('You selected', game_easy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpzFG9myQWXl"
      },
      "outputs": [],
      "source": [
        "#@title bands { run: \"auto\" }\n",
        "\n",
        "band = \"13.0,29.0\" #@param ['4.0,7.0', '8.0,12.0', '13.0,29.0', '30.0,45.0']\n",
        "print('You selected', band)\n",
        "import json\n",
        "bands = [json.loads('['+band+']')]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gbRSYKA6lBh"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qsGtQ2ecAta"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Puv0gtaP8JWD"
      },
      "outputs": [],
      "source": [
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W00w8_s6oV_"
      },
      "outputs": [],
      "source": [
        "# Get TPU profiling service address. This address will be needed for capturing\n",
        "# profile information with TensorBoard in the following steps.\n",
        "service_addr = tpu.get_master().replace(':8470', ':8466')\n",
        "print(service_addr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JELdpoKxBsg5"
      },
      "outputs": [],
      "source": [
        "!pip install tb-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsmOy3yFFWVq"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboard==1.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tzIBmPrE3do"
      },
      "outputs": [],
      "source": [
        "!pip install cloud-tpu-profiler==1.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x23ItWYkJodZ"
      },
      "outputs": [],
      "source": [
        "!pip uninstall tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RprUkDbJx0S"
      },
      "outputs": [],
      "source": [
        "!pip install tb-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzTV_30cchIA"
      },
      "outputs": [],
      "source": [
        "!pip uninstall tb-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNiRD6WMCMlp"
      },
      "outputs": [],
      "source": [
        "TPU_NAME=service_addr\n",
        "#MODEL_DIR='/content/sg2'\n",
        "MODEL_DIR='gs://train_with_tpu/sg2'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZniEk_YTd6t"
      },
      "outputs": [],
      "source": [
        "!capture_tpu_profile --tpu={TPU_NAME} --logdir={MODEL_DIR} --duration_ms=60000 \\\n",
        "   --monitoring_level=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QBUY33RG6IK"
      },
      "outputs": [],
      "source": [
        "!capture_tpu_profile --service_addr {TPU_NAME} --monitoring_level 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po2vkhMuGkRU"
      },
      "outputs": [],
      "source": [
        "!capture_tpu_profile --tpu={TPU_NAME}   --monitoring_level=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGguVSnTGUMs"
      },
      "outputs": [],
      "source": [
        "!gcloud compute tpus describe {TPU_NAME}\n",
        "# --zone=<zone>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIuoC-9DNl9-"
      },
      "outputs": [],
      "source": [
        "!capture_tpu_profile --helpfull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Intnx2DUDrhi"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "p = subprocess.Popen([\"capture_tpu_profile\",\n",
        "                  \"--logdir=gs://train_with_tpu/sg2\", \n",
        "#                  \"--monitoring_level=2\", \n",
        "#                  \"--duration_ms=600000\", \n",
        "#                  \"--num_queries=10000\",\n",
        "                  \"--gcp_project=encoded-phalanx-326615\",\n",
        "#                  \"--workers_list\",\n",
        "#                  \"--tpu={}\".format(os.environ['COLAB_TPU_ADDR'])])\n",
        "                  \"--service_addr={}\".format(os.environ['COLAB_TPU_ADDR'])])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErDAT0HdQ42k"
      },
      "outputs": [],
      "source": [
        "print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C-mlc29btx1"
      },
      "outputs": [],
      "source": [
        "p.kill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D15fd3etO7wS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import signal\n",
        "import subprocess\n",
        "\n",
        "os.killpg(os.getpgid(p.pid), signal.SIGTERM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uCnXLYfCyXv"
      },
      "outputs": [],
      "source": [
        "tpu_ip=service_addr\n",
        "MODEL_DIR='gs://train_with_tpu'\n",
        "%tensorboard --logdir={MODEL_DIR} \\\n",
        "    --host=10.10.64.114 --port=8466"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYo5RhWqRuFE"
      },
      "outputs": [],
      "source": [
        "!kill 742"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj_UW7ppgonE"
      },
      "outputs": [],
      "source": [
        "print(service_addr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27g8bzqw6oNA"
      },
      "outputs": [],
      "source": [
        "# Launch TensorBoard.\n",
        "%tensorboard --logdir=gs://train_with_tpu  # Replace the bucket-name variable with your own gcs bucket\n",
        "#%tensorboard --logdir=gs://train_with_tpu --load_fast=false # Replace the bucket-name variable with your own gcs bucket\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWsQ3bIWFhXg"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=gs://train_with_tpu/sg2  # Replace the bucket-name variable with your own gcs bucket\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDEgvPYT6oFT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47L-E9Uf6n_X"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXgDIQyftd6N"
      },
      "outputs": [],
      "source": [
        "sess = tf.get_default_session()\n",
        "import tflex\n",
        "cores = tflex.get_cores(sess)\n",
        "print(cores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1VT_Abl8W6L"
      },
      "outputs": [],
      "source": [
        "                images = Gs.run(_z1, None, **Gs_kwargs) # [minibatch, height, width, channel]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVXE33wS-bzu"
      },
      "outputs": [],
      "source": [
        "print(_z1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGIrhuv6kMX6"
      },
      "outputs": [],
      "source": [
        "duration=5*1/8\n",
        "overlap=0\n",
        "#overlap=duration-0.1\n",
        "video_fps=6\n",
        "xsize=512\n",
        "ysize=512\n",
        "#xsize=int(512/video_fps)\n",
        "#ysize=int(512/video_fps)\n",
        "hz=44100\n",
        "#fps=hz/(32768)\n",
        "\n",
        "#if generate_stylegan2:\n",
        "fps_sg2=1\n",
        "#if generate_wavegan:\n",
        "fps_wg=((hz/(32768*2))/1)*1/1\n",
        "fps_sg2=fps_wg\n",
        "#fps_sg2=fps_wg/3\n",
        "#fps_sg2=fps_wg*1\n",
        "fps_hm=fps_wg\n",
        "\n",
        "fps2_sg2=((fps_sg2*24/8)/3)*video_fps\n",
        "\n",
        "#if 1/fps_wg-0.2>duration:\n",
        "#  duration=1/fps_wg-0.2\n",
        "#  overlap=duration-0.1\n",
        "\n",
        "if 2*1/fps_wg>duration:\n",
        "  duration=2*1/fps_wg\n",
        "#  overlap=0\n",
        "  overlap=(duration/2)-(duration/2)/(fps2_sg2/fps_sg2)\n",
        "\n",
        "debug=True\n",
        "#debug=False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIK-1nDPuy3h"
      },
      "outputs": [],
      "source": [
        "            ji=0\n",
        "            for method in range(len(methods)):\n",
        "             for band in range(len(bands)):\n",
        "              fmin=bands[band][0]\n",
        "              fmax=bands[band][1]\n",
        "              time0071=perf_counter()\n",
        "              #print (f'0071: {(time0071-time000):.1f}s')\n",
        "              #if band == 0:\n",
        "              #print(epochs[band])\n",
        "              con, freqs, times, n_epochs, n_tapers = spectral_connectivity(\n",
        "                #epochs[band], method=methods[method], mode='fourier', sfreq=sfreq, fmin=fmin,\n",
        "                epochs[band][ji:ji+1], method=methods[method], mode='multitaper', sfreq=sfreq, fmin=fmin,\n",
        "#                epochs[band], method=methods[method], mode='multitaper', sfreq=sfreq, fmin=fmin,\n",
        "                #epochs[band][ji:ji+1], method=methods[method], mode='multitaper', sfreq=sfreq, fmin=fmin,\n",
        "                #fmax=fmax, faverage=True, mt_adaptive=False, n_jobs=1, verbose=50)\n",
        "#                fmax=fmax, faverage=True, mt_adaptive=True, n_jobs=1, verbose=50)\n",
        "                fmax=fmax, faverage=True, mt_adaptive=True, n_jobs='cuda', verbose=50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NCIyNWL24tt"
      },
      "outputs": [],
      "source": [
        "debug=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzndN3ApwnmP"
      },
      "outputs": [],
      "source": [
        "#@title debug { run: \"auto\" }\n",
        "\n",
        "debug=\"0\" #@param [0, 1]\n",
        "print('You selected', debug)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fDfFoj9fZ9Y"
      },
      "outputs": [],
      "source": [
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_nvlabs_ada | gen_anime_protraits\n",
        "generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_nvlabs_ada | gen_abctract_art\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_shawwn | gen_tadne\n",
        "#generate = gen_gpu | gen_pytorch | gen_sg2_nagolinc_pt | gen_tf1 | gen_stylegan2 | gen_sg2_shawwn | gen_tadne\n",
        "#generate = gen_gpu | gen_tf1 | gen_wavegan | gen_drums\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_nvlabs_ada | gen_anime_protraits | gen_wavegan | gen_drums\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_nvlabs_ada | gen_abctract_art | gen_wavegan | gen_drums\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_shawwn | gen_tadne | gen_wavegan | gen_drums\n",
        "#generate = gen_gpu | gen_pytorch | gen_sg2_nagolinc_pt | gen_tf1 | gen_stylegan2 | gen_sg2_shawwn | gen_tadne | gen_wavegan | gen_drums\n",
        "\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_nvlabs | gen_anime_protraits | gen_tf2_npy\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_nvlabs_ada | gen_anime_protraits | gen_tf2_npy\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_nvlabs_ada | gen_abctract_art | gen_tf2_npy\n",
        "\n",
        "#generate = gen_gpu | gen_tf2 | gen_stylegan2 | gen_sg2_rosasalberto_tf2 | gen_anime_protraits_tf2_npy\n",
        "#generate = gen_gpu | gen_tf2 | gen_stylegan2 | gen_sg2_rosasalberto_tf2 | gen_abctract_art_tf2_npy\n",
        "#generate = gen_tpu | gen_tf2 | gen_stylegan2 | gen_sg2_rosasalberto_tf2 | gen_anime_protraits_tf2_npy\n",
        "#generate = gen_tpu | gen_tf2 | gen_stylegan2 | gen_sg2_rosasalberto_tf2 | gen_abctract_art_tf2_npy\n",
        "\n",
        "#generate = gen_tpu | gen_tf1 | gen_wavegan | gen_drums\n",
        "\n",
        "#generate = gen_tf2 | gen_stylegan2 | gen_sg2_rosasalberto_tf2 | gen_anime_protraits_tf2_npy\n",
        "#generate = gen_tf2 | gen_stylegan2 | gen_sg2_rosasalberto_tf2 | gen_abctract_art_tf2_npy\n",
        "#generate = gen_tf1 | gen_wavegan | gen_drums\n",
        "\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_shawwn | gen_thdne\n",
        "#generate = gen_gpu | gen_pytorch | gen_stylegan2 | gen_sg2_nvlabs_ada_pt | gen_thdne\n",
        "\n",
        "#generate = gen_gpu | gen_tf1 | gen_wavegan | gen_wg_stereo | gen_drums\n",
        "#generate = gen_tf1 | gen_wavegan | gen_wg_stereo | gen_drums\n",
        "#generate = gen_tf1 | gen_wavegan | gen_wg_stereo | gen_wg_st_swap | gen_drums\n",
        "\n",
        "generate = generate | gen_wg_stereo | gen_wg_st_swap\n",
        "\n",
        "#generate = generate | gen_png | gen_wav\n",
        "generate = generate | gen_jpeg | gen_mp3\n",
        "#generate = generate | gen_mp4\n",
        "#generate = generate | gen_webm\n",
        "\n",
        "#generate = generate | gen_mp4_pyav\n",
        "#generate = generate | gen_mp4_imageio\n",
        "#generate = generate | gen_mp4_moviepy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KubmOemcqU_4"
      },
      "outputs": [],
      "source": [
        "from IPython import display as ipythondisplay\n",
        "import io\n",
        "import os\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "def show_video(vid):\n",
        "  ext = os.path.splitext(vid)[-1][1:]\n",
        "  video = io.open(vid, 'r+b').read()\n",
        "  ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "              loop controls style=\"height: 400px;\">\n",
        "              <source src=\"data:video/{1}';base64,{0}\" type=\"video/{1}\" />\n",
        "              </video>'''.format(base64.b64encode(video).decode('ascii'), ext)))\n",
        "\n",
        "show_video('buffer.mp4')\n",
        "show_video('test.mp4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "061mt49wzs-u"
      },
      "outputs": [],
      "source": [
        "print(fps_sg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fv5ogY3QGr_8"
      },
      "outputs": [],
      "source": [
        "print(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B6d2TUfLSIS3"
      },
      "outputs": [],
      "source": [
        "#@title tf1_wavegan_drums { run: \"auto\" }\n",
        "\n",
        "mono_stereo_swap = \"stereo with swap\" #@param ['mono', 'stereo without swap', 'stereo with swap']\n",
        "print('You selected', mono_stereo_swap)\n",
        "generate = gen_tf1 | gen_wavegan | gen_drums\n",
        "if mono_stereo_swap == \"mono\":\n",
        "  generate = generate\n",
        "if mono_stereo_swap == \"stereo without swap\":\n",
        "  generate = generate | gen_wg_stereo\n",
        "if mono_stereo_swap == \"stereo with swap\":\n",
        "  generate = generate | gen_wg_stereo | gen_wg_st_swap\n",
        "  \n",
        "generate = generate | gen_jpeg |gen_mp3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cS7s4r8BcrB"
      },
      "outputs": [],
      "source": [
        "methods = ['coh']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q_2zf1JBcjE"
      },
      "outputs": [],
      "source": [
        "methods = ['plv']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbmu62mGBccC"
      },
      "outputs": [],
      "source": [
        "methods = ['ciplv']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn0h9pgXBcRp"
      },
      "outputs": [],
      "source": [
        "methods = ['ppc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjpxs4q2BcIZ"
      },
      "outputs": [],
      "source": [
        "methods = ['pli']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXYkvvyjBb_y"
      },
      "outputs": [],
      "source": [
        "methods = ['wpli']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OBOTEr1N38s"
      },
      "outputs": [],
      "source": [
        "methods = ['coh']\n",
        "#methods = ['plv']\n",
        "#methods = ['ciplv']\n",
        "#methods = ['ppc']\n",
        "#methods = ['pli']\n",
        "#methods = ['wpli']\n",
        "#methods = ['coh', 'plv', 'ciplv', 'ppc', 'pli', 'wpli']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzOvkuFuCtCh"
      },
      "outputs": [],
      "source": [
        "bands = [[4.,7.]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBKZSj8YCs-Y"
      },
      "outputs": [],
      "source": [
        "bands = [[8.,12.]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFFiaT_5Cs66"
      },
      "outputs": [],
      "source": [
        "bands = [[13.,29.]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHpj38JXCwY_"
      },
      "outputs": [],
      "source": [
        "bands = [[30.,45.]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2yUC8TD1nrT"
      },
      "outputs": [],
      "source": [
        "#bands = [[4.,7.]]\n",
        "#bands = [[8.,12.]]\n",
        "bands = [[4.,7.],[8.,12.]]\n",
        "#bands = [[13.,29.]]\n",
        "#bands = [[8.,12.],[13.,29.]]\n",
        "#bands = [[4.,7.],[13.,29.]]\n",
        "#bands = [[4.,7.],[8.,12.],[13.,29.]]\n",
        "#bands = [[30.,45.]]\n",
        "#bands = [[4.,7.],[30.,45.]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1bMmefN_y0x"
      },
      "outputs": [],
      "source": [
        "generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_nvlabs_ada | gen_anime_protraits\n",
        "#generate = gen_gpu | gen_tf1 | gen_wavegan | gen_drums\n",
        "#generate = gen_gpu | gen_tf1 | gen_stylegan2 | gen_sg2_ada | gen_anime_protraits | gen_wavegan | gen_drums\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGADYii2xXyR"
      },
      "outputs": [],
      "source": [
        "#generate = gen_gpu | gen_tf1 | gen_wavegan | gen_wavegan_stereo | gen_drums\n",
        "generate = gen_tf1 | gen_wavegan | gen_wg_stereo | gen_drums\n",
        "generate = generate | gen_jpeg |gen_mp3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaVoVmCc8grr"
      },
      "outputs": [],
      "source": [
        "#generate = gen_gpu | gen_tf1 | gen_wavegan | gen_wavegan_stereo | gen_wavegan_stereo_swap | gen_drums\n",
        "generate = gen_tf1 | gen_wavegan | gen_wg_stereo | gen_wg_st_swap | gen_drums\n",
        "generate = generate | gen_jpeg |gen_mp3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVigZ0vLVBMj"
      },
      "outputs": [],
      "source": [
        "fps_sg2=fps_wg/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1cXinGXEkTR"
      },
      "outputs": [],
      "source": [
        "fps_sg2=fps_wg*1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dozWgxSrvC5"
      },
      "outputs": [],
      "source": [
        "fps_sg2=fps_wg*2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iBp7jy5zD5X"
      },
      "outputs": [],
      "source": [
        "fps_sg2=fps_wg*3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJd_aYyNrs8J"
      },
      "outputs": [],
      "source": [
        "fps_sg2=fps_wg*4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZUBDU8frsnJ"
      },
      "outputs": [],
      "source": [
        "fps_sg2=fps_wg*5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKO70vv4rr8J"
      },
      "outputs": [],
      "source": [
        "fps_sg2=fps_wg*6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeU21L1eEk_x"
      },
      "outputs": [],
      "source": [
        "xsize=int(512/1)\n",
        "ysize=int(512/1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnRG1eZwu3f7"
      },
      "outputs": [],
      "source": [
        "xsize=int(512/2)\n",
        "ysize=int(512/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nRQTQbGvffi"
      },
      "outputs": [],
      "source": [
        "xsize=int(512/3)\n",
        "ysize=int(512/3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvcL8hjFvoOh"
      },
      "outputs": [],
      "source": [
        "xsize=int(512/4)\n",
        "ysize=int(512/4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GizuQ5TNvpZZ"
      },
      "outputs": [],
      "source": [
        "xsize=int(512/5)\n",
        "ysize=int(512/5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpoZlAo8vqMJ"
      },
      "outputs": [],
      "source": [
        "xsize=int(512/6)\n",
        "ysize=int(512/6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M3q-mfvIbjA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#if generate_stylegan2:\n",
        "fps_sg2=1\n",
        "#if generate_wavegan:\n",
        "fps_wg=hz/(32768*2)\n",
        "fps_sg2=fps_wg\n",
        "\n",
        "duration=2*1/fps_wg#-0.2\n",
        "#overlap=duration/32#-0.1\n",
        "#overlap=0.1\n",
        "#overlap=0\n",
        "#duration=5*1/8\n",
        "overlap=0\n",
        "#overlap=duration-0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxafHdQ6rP5y"
      },
      "outputs": [],
      "source": [
        "fps_sg2=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHm9yDes4i-V"
      },
      "outputs": [],
      "source": [
        "stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NBWWs1HAzf7"
      },
      "outputs": [],
      "source": [
        "video_out.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRaKPKotYCq4"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/out/output.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6JXcX4OYAsw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cp -r -v \"/content/out\" \"/content/gdrive/MyDrive/EEG-GAN-audio-video\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XJjnDvhJ9Y-"
      },
      "outputs": [],
      "source": [
        "images = Gs.run(_z1, None, **Gs_kwargs) # [minibatch, height, width, channel]\n",
        "if True:\n",
        "                for image in images:\n",
        "                  time1110=perf_counter()\n",
        "                  #print (f'1110: {(time1110-time000):.1f}s')\n",
        "                  #print(image)\n",
        "                  image_pil=PIL.Image.fromarray(image, 'RGB')\n",
        "                  #if generate&gen_sg2_shawwn:\n",
        "                  #  display(image_pil)\n",
        "                  #print(image_pil)\n",
        "                  image_asarray=np.asarray(image_pil)\n",
        "                  #print(image_asarray)\n",
        "                  time1111=perf_counter()\n",
        "                  #print (f'1111: {(time1111-time000):.1f}s')\n",
        "                  #global video_out\n",
        "                  #video_out.append_data(image_asarray)\n",
        "                  time1112=perf_counter()\n",
        "                  #print (f'1112: {(time1112-time000):.1f}s')\n",
        "                  img=image_pil.resize((xsize,ysize),PIL.Image.ANTIALIAS)\n",
        "                  #print(img)\n",
        "                  time1113=perf_counter()\n",
        "                  #print (f'1113: {(time1113-time000):.1f}s')\n",
        "                  buffer = BytesIO()\n",
        "                  if generate&gen_jpeg:\n",
        "                    img.save(buffer,format=\"JPEG\")                  #Enregistre l'image dans le buffer\n",
        "                  if generate&gen_png:\n",
        "                    img.save(buffer,format=\"PNG\")                  #Enregistre l'image dans le buffer\n",
        "                  #img.save('/content/gdrive/MyDrive/EEG-GAN-audio-video/out/'+\n",
        "                  #          f'{(time100*1000):9.0f}'+'/'+f'{(time000*1000):9.0f}'+'.png',format=\"PNG\")\n",
        "                  if False:\n",
        "                    #buffer_wav = BytesIO()\n",
        "                    buffer_x264 = BytesIO()\n",
        "                    buffer_x264.name='output.mp4'  \n",
        "                    video_out = imageio.get_writer(buffer_x264, format='mp4', mode='I', fps=fps_sg2, codec='libx264', bitrate='16M')\n",
        "                    video_out.append_data(image_asarray)\n",
        "                    #print(image_asarray)\n",
        "                    print(video_out)\n",
        "                    video_out.close()\n",
        "                    buffer_x264.seek(0)\n",
        "                    print(buffer_x264.getvalue())\n",
        "\n",
        "                    #  scipy.io.wavfile.write(buffer_wav, hz, _G_z_full) # change rate for different tempo\n",
        "                      #binary = b64decode(data.split(',')[1])\n",
        "                    #  binary = buffer_wav  \n",
        "                    process = (ffmpeg\n",
        "                      .input('pipe:0', format='mp4')\n",
        "                      .output('pipe:1', format='webm')\n",
        "                      .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "                    )\n",
        "                    #print(buffer_x264)\n",
        "                    #print(BytesIO(buffer_x264).read())\n",
        "                    output, err = process.communicate(input=buffer_x264.getvalue())\n",
        "                    buffer.write(output)\n",
        "                    #with buffer as f:\n",
        "                    #  f.write(output)\n",
        "                    print(output)\n",
        "                  if generate&gen_webm:\n",
        "\n",
        "import io\n",
        "  \n",
        "import av\n",
        "\n",
        "song_path = '/tmp/some.mp3'\n",
        "\n",
        "# input\n",
        "container = av.open(song_path)\n",
        "audio_stream = None\n",
        "for stream in container.streams:\n",
        "    if stream.type == 'audio':\n",
        "        audio_stream = stream\n",
        "        break\n",
        "if not audio_stream:\n",
        "    raise Exception('could not find audio stream')\n",
        "\n",
        "out_file = io.BytesIO()\n",
        "out_container = av.open(out_file, 'w', format='wav')\n",
        "out_stream = out_container.add_stream(codec_name='pcm_s16le', rate=44100)\n",
        "\n",
        "for i, frame in enumerate(container.decode(audio_stream)):\n",
        "    for packet in out_stream.encode(frame):\n",
        "        out_container.mux(packet)\n",
        "\n",
        "    if i > 500:\n",
        "        break\n",
        "\n",
        "# flush\n",
        "for packet in out_stream.encode(None):\n",
        "    out_container.mux(packet)\n",
        "out_container.close()\n",
        "print(len(out_file.getvalue()))                    \n",
        " \n",
        "                  buffer.seek(0)\n",
        "                  time1114=perf_counter()\n",
        "                  #print (f'1114: {(time1114-time000):.1f}s')\n",
        "                  myimage = buffer.getvalue()   \n",
        "                  #encoded=myimage\n",
        "                  if generate&gen_jpeg:\n",
        "                    encoded= \"data:image/jpeg;base64,\"+base64.b64encode(myimage).decode()\n",
        "                  if generate&gen_png:\n",
        "                    encoded= \"data:image/png;base64,\"+base64.b64encode(myimage).decode()\n",
        "                  if generate&gen_webm:\n",
        "                    encoded= \"data:video/webm;base64,\"+base64.b64encode(myimage).decode()\n",
        "                  print(encoded)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "EMG_Silent_Speech_with_WaveNet&DeepSpeech_via_BrainFlow.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}